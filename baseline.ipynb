{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da85bfe7",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "variables with prefix ``doc_`` contain data from the dataset\n",
    "variables with prefix ``o_`` contain data from own computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd7e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from petreader.RelationsExtraction import RelationsExtraction\n",
    "from petreader.TokenClassification import TokenClassification\n",
    "from petreader.labels import FLOW, SAME_GATEWAY, AND_GATEWAY, XOR_GATEWAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c49670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\relations-extraction\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.12516093254089355,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0d85b5a5c64f1ba0698862bff069fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\token-classification\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04445481300354004,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d3cbfb715a4a7ba117a845fc27c0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relations_dataset = RelationsExtraction()\n",
    "token_dataset = TokenClassification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d905d3",
   "metadata": {},
   "source": [
    "## 1 Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc1372",
   "metadata": {},
   "source": [
    "### 1.1 Read Example Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe36154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  doc-1.1  ********************\n",
      "A small company manufactures customized bicycles . Whenever the sales department receives an order , a new process instance is created . A member of the sales department can then reject or accept the order for a customized bike . In the former case , the process instance is finished . In the latter case , the storehouse and the engineering department are informed . The storehouse immediately processes the part list of the order and checks the required quantity of each part . If the part is available in-house , it is reserved . If it is not available , it is back-ordered . This procedure is repeated for each item on the part list . In the meantime , the engineering department prepares everything for the assembling of the ordered bicycle . If the storehouse has successfully reserved or back-ordered every item of the part list and the preparation activity has finished , the engineering department assembles the bicycle . Afterwards , the sales department ships the bicycle to the customer and finishes the process instance .\n",
      "*** activities and NER labels (per sentences) ****\n",
      "[[], [['receives']], [['reject'], ['accept']], [], [['informed']], [['processes'], ['checks']], [['reserved']], [['back-ordered']], [], [['prepares']], [['assembles']], [['ships']]]\n",
      "[[('A', 0, 'O'), ('small', 1, 'O'), ('company', 2, 'O'), ('manufactures', 3, 'O'), ('customized', 4, 'O'), ('bicycles', 5, 'O'), ('.', 6, 'O')], [('Whenever', 0, 'O'), ('the', 1, 'B-Actor'), ('sales', 2, 'I-Actor'), ('department', 3, 'I-Actor'), ('receives', 4, 'B-Activity'), ('an', 5, 'B-Activity Data'), ('order', 6, 'I-Activity Data'), (',', 7, 'O'), ('a', 8, 'O'), ('new', 9, 'O'), ('process', 10, 'O'), ('instance', 11, 'O'), ('is', 12, 'O'), ('created', 13, 'O'), ('.', 14, 'O')], [('A', 0, 'O'), ('member', 1, 'O'), ('of', 2, 'O'), ('the', 3, 'O'), ('sales', 4, 'O'), ('department', 5, 'O'), ('can', 6, 'O'), ('then', 7, 'O'), ('reject', 8, 'B-Activity'), ('or', 9, 'B-XOR Gateway'), ('accept', 10, 'B-Activity'), ('the', 11, 'B-Activity Data'), ('order', 12, 'I-Activity Data'), ('for', 13, 'O'), ('a', 14, 'O'), ('customized', 15, 'O'), ('bike', 16, 'O'), ('.', 17, 'O')], [('In', 0, 'O'), ('the', 1, 'O'), ('former', 2, 'O'), ('case', 3, 'O'), (',', 4, 'O'), ('the', 5, 'O'), ('process', 6, 'O'), ('instance', 7, 'O'), ('is', 8, 'O'), ('finished', 9, 'O'), ('.', 10, 'O')]]\n",
      "************* same gateway relations *************\n",
      "source-head-sentence-ID: 6\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 7\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id = 0\n",
    "doc_name = token_dataset.GetDocumentName(doc_id)\n",
    "print(f\"  {doc_name}  \".center(50, '*'))\n",
    "doc_text = relations_dataset.GetDocument(doc_id)\n",
    "print(doc_text)\n",
    "doc_activities = token_dataset.GetDocumentActivities(doc_name)\n",
    "\n",
    "print(\" activities and NER labels (per sentences) \".center(50, '*'))\n",
    "print(doc_activities)\n",
    "doc_sentence_ner_labels = relations_dataset.GetSentencesWithIdsAndNerTagLabels(doc_id)\n",
    "print(doc_sentence_ner_labels[:4])\n",
    "doc_relations = relations_dataset.GetRelations(doc_id)\n",
    "doc_flow_relations, doc_same_gateway_relations = doc_relations[FLOW], doc_relations[SAME_GATEWAY]\n",
    "\n",
    "print(\" same gateway relations \".center(50, '*'))\n",
    "for same_gateway_relation in doc_same_gateway_relations:\n",
    "    for key, value in same_gateway_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b84532",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8a529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "A small company manufactures customized bicycles\n",
      "Whenever the sales department receives an order , a new process instance is created\n",
      "A member of the sales department can then reject or accept the order for a customized bike\n",
      "In the former case , the process instance is finished\n",
      "In the latter case , the storehouse and the engineering department are informed\n",
      "The storehouse immediately processes the part list of the order and checks the required quantity of each part\n",
      "If the part is available in-house , it is reserved\n",
      "If it is not available , it is back-ordered\n",
      "This procedure is repeated for each item on the part list\n",
      "In the meantime , the engineering department prepares everything for the assembling of the ordered bicycle\n",
      "If the storehouse has successfully reserved or back-ordered every item of the part list and the preparation activity has finished , the engineering department assembles the bicycle\n",
      "Afterwards , the sales department ships the bicycle to the customer and finishes the process instance\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(doc_activities) # activities is 2 dim list (one per sentence)\n",
    "print(num_sentences)\n",
    "doc_sentences_raw = [sentence.strip() for sentence in doc_text.split(\".\") if sentence.strip() != \"\"]\n",
    "for s in doc_sentences_raw:\n",
    "    print(s)\n",
    "assert num_sentences == len(doc_sentences_raw)  # check if number of extracted sentences == from dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a98c16",
   "metadata": {},
   "source": [
    "### 1.3 Filter Tokens for Gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b7a2435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [('or', 9, 'B-XOR Gateway')], [], [], [], [('If', 0, 'B-XOR Gateway')], [('If', 0, 'B-XOR Gateway')], [], [], [('If', 0, 'B-XOR Gateway')], []]\n",
      "[[], [], [], [], [], [], [], [], [], [('In', 0, 'B-AND Gateway'), ('the', 1, 'I-AND Gateway'), ('meantime', 2, 'I-AND Gateway')], [], []]\n"
     ]
    }
   ],
   "source": [
    "def filter_ner_labels(sentence_ner_labels, target_label):\n",
    "    return [[token for token in s_list if target_label in token[2]]\n",
    "                        for s_list in sentence_ner_labels]\n",
    "doc_xor_gateway = filter_ner_labels(doc_sentence_ner_labels, XOR_GATEWAY)\n",
    "doc_and_gateway = filter_ner_labels(doc_sentence_ner_labels, AND_GATEWAY)\n",
    "print(doc_xor_gateway)\n",
    "print(doc_and_gateway)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a1ea2",
   "metadata": {},
   "source": [
    "### 1.X Filter Flow Relations? TODO\n",
    "see question doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d4d41",
   "metadata": {},
   "source": [
    "### 1.4 Key Word List\n",
    "#### A) take words from all existing gateways as gold list for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0cb69a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR gold (15) ['either', 'for', 'for each patient for which', 'for the case', 'if', 'in case', 'in case of', 'in the case of', 'it can also happen that', 'or', 'otherwise', 'should', 'sometimes', 'under certain circumstances', 'whereas']\n",
      "AND gold (6) ['at the same time', 'in the meantime', 'meantime', 'two concurrent activities are triggered', 'whereas', 'while']\n"
     ]
    }
   ],
   "source": [
    "def get_gateway_key_words(dataset_gateway_list):\n",
    "    flattened = list(itertools.chain(*dataset_gateway_list))\n",
    "    phrases = [\" \".join(g).lower() for g in flattened]  # join phrases together if multiple words\n",
    "    unique = list(set(phrases))\n",
    "    unique.sort()\n",
    "    return unique\n",
    "\n",
    "xor_key_words_gold = get_gateway_key_words(token_dataset.GetXORGateways())\n",
    "and_key_words_gold = get_gateway_key_words(token_dataset.GetANDGateways())\n",
    "\n",
    "print(f\"XOR gold ({len(xor_key_words_gold)})\", xor_key_words_gold)\n",
    "print(f\"AND gold ({len(and_key_words_gold)})\", and_key_words_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efcd79",
   "metadata": {},
   "source": [
    "#### B) Curated List from Literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45bdd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR literature (14) ['either', 'if', 'if not', 'in case', 'in case of', 'only', 'only if', 'or', 'otherwise', 'till', 'unless', 'until', 'when', 'whether']\n",
      "AND literature (11) ['at the same time', 'concurrently', 'in addition to', 'in parallel', 'in parallel with this', 'in the meantime', 'meantime', 'meanwhile', 'simultaneously', 'whereas', 'while']\n"
     ]
    }
   ],
   "source": [
    "# Ferreira et al. 2017\n",
    "xor_key_words_literature = ['if', 'whether', 'if not', 'or', 'in case', 'in case of', 'otherwise', 'either', 'only', 'till', 'until', 'unless', 'when', 'only if']\n",
    "xor_key_words_literature.sort()\n",
    "and_key_words_literature = ['while', 'meanwhile', 'in parallel', 'concurrently', 'meantime', 'in the meantime', 'in parallel with this', 'in addition to', 'simultaneously', 'at the same time', 'whereas']\n",
    "and_key_words_literature.sort()\n",
    "print(f\"XOR literature ({len(xor_key_words_literature)})\", xor_key_words_literature)\n",
    "print(f\"AND literature ({len(and_key_words_literature)})\", and_key_words_literature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4e627",
   "metadata": {},
   "source": [
    "## 2 Extract Gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20306e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small company manufactures customized bicycles\n",
      "Whenever the sales department receives an order , a new process instance is created\n",
      "A member of the sales department can then reject or accept the order for a customized bike\n",
      "In the former case , the process instance is finished\n",
      "In the latter case , the storehouse and the engineering department are informed\n",
      "The storehouse immediately processes the part list of the order and checks the required quantity of each part\n",
      "If the part is available in-house , it is reserved\n",
      "If it is not available , it is back-ordered\n",
      "This procedure is repeated for each item on the part list\n",
      "In the meantime , the engineering department prepares everything for the assembling of the ordered bicycle\n",
      "If the storehouse has successfully reserved or back-ordered every item of the part list and the preparation activity has finished , the engineering department assembles the bicycle\n",
      "Afterwards , the sales department ships the bicycle to the customer and finishes the process instance\n"
     ]
    }
   ],
   "source": [
    "for s in doc_sentences_raw:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a421ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gateways(sentence_list, key_words, target_gateway_label):\n",
    "    \"\"\"\n",
    "    extracts gateways in a key-word-based manner given a document structured in a list of sentences\n",
    "    if two phrases would match to a token (e.g. 'in the meantime' and 'meantime'), the longer phrase is extracted\n",
    "    target_gateway_label: str, must be 'XOR Gateway' or 'AND Gateway'\n",
    "    \n",
    "    result list is two dimensional -> list of tuples (word, position in sentence, tag) for each sentence\n",
    "    this produces the same structure as sentences and their NER labels are annotated in PET dataset\n",
    "    \"\"\"\n",
    "    gateways = []\n",
    "    # sort key words descending by length of words in phrase\n",
    "    key_words.sort(key=lambda key_word_phrase: len(key_word_phrase.split(\" \")), reverse=True)\n",
    "\n",
    "    # 1) extract gateways\n",
    "    for s_idx, sentence in enumerate(sentence_list):\n",
    "        # print(f\" SENTENCE {s_idx} \".center(50, '-'))\n",
    "        # print(sentence_list[s_idx])\n",
    "        sentence_gateways = []\n",
    "        sentence_to_search = f\" {sentence.lower()} \"  # lowercase and wrap with spaces for search of key words\n",
    "        tokens = sentence.split(\" \")\n",
    "        tokens_lower = sentence.lower().split(\" \")\n",
    "        tokens_already_matched_with_key_phrase = []\n",
    "\n",
    "        # iterate over key phrases\n",
    "        for key_phrase in key_words:\n",
    "            key_phrase_to_search = f\" {key_phrase} \"\n",
    "\n",
    "            # if key phrase is in sentence, search index and extract\n",
    "            if key_phrase_to_search in sentence_to_search:\n",
    "                key_phrase_tokens = key_phrase.split(\" \")\n",
    "                \n",
    "                # check key phrase for every token\n",
    "                for t_idx, token in enumerate(tokens_lower):\n",
    "                    candidate = True\n",
    "                    # iterate over key phrase tokens in case of multiple world phrase\n",
    "                    for key_phrase_token_idx, key_phrase_token in enumerate(key_phrase_tokens):\n",
    "                        # check if token is not part of key phrase or token is already matched with another phrase\n",
    "                        # if yes, stop processing candidate\n",
    "                        if not tokens_lower[t_idx + key_phrase_token_idx] == key_phrase_token or \\\n",
    "                            t_idx + key_phrase_token_idx in tokens_already_matched_with_key_phrase:\n",
    "                            candidate = False\n",
    "                            break\n",
    "                    \n",
    "                    # add tokens to result only if all tokens are matched and not already part of a longer phrase\n",
    "                    if candidate:\n",
    "                        for i, key_phrase_token in enumerate(key_phrase_tokens):\n",
    "                            prefix = \"B\" if i == 0 else \"I\"\n",
    "                            # append tuples with extract information as in PET and process information about gateways to filter later\n",
    "                            sentence_gateways.append((tokens[t_idx + i], t_idx + i, f\"{prefix}-{XOR_GATEWAY}\"))\n",
    "                            tokens_already_matched_with_key_phrase.append(t_idx + i)\n",
    "\n",
    "        sentence_gateways.sort(key=lambda gateway_triple: gateway_triple[1])\n",
    "        gateways.append(sentence_gateways)\n",
    "\n",
    "    return gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52345283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR GATEWAYS\n",
      "0 []\n",
      "1 []\n",
      "2 [('or', 9, 'B-XOR Gateway'), ('for', 13, 'B-XOR Gateway')]\n",
      "3 []\n",
      "4 []\n",
      "5 []\n",
      "6 [('If', 0, 'B-XOR Gateway')]\n",
      "7 [('If', 0, 'B-XOR Gateway')]\n",
      "8 [('for', 4, 'B-XOR Gateway')]\n",
      "9 [('for', 9, 'B-XOR Gateway')]\n",
      "10 [('If', 0, 'B-XOR Gateway'), ('or', 6, 'B-XOR Gateway')]\n",
      "11 []\n",
      "\n",
      "AND GATEWAYS\n",
      "0 []\n",
      "1 []\n",
      "2 []\n",
      "3 []\n",
      "4 []\n",
      "5 []\n",
      "6 []\n",
      "7 []\n",
      "8 []\n",
      "9 [('In', 0, 'B-XOR Gateway'), ('the', 1, 'I-XOR Gateway'), ('meantime', 2, 'I-XOR Gateway')]\n",
      "10 []\n",
      "11 []\n"
     ]
    }
   ],
   "source": [
    "# available key word lists: xor_key_words_gold, and_key_words_gold, xor_key_words_literature, and_key_words_literature\n",
    "o_xor_gateways = extract_gateways(doc_sentences_raw, xor_key_words_gold, XOR_GATEWAY)\n",
    "o_and_gateways = extract_gateways(doc_sentences_raw, and_key_words_gold, AND_GATEWAY)\n",
    "\n",
    "print(\"XOR GATEWAYS\")\n",
    "for idx, sentence_gateways in enumerate(o_xor_gateways):\n",
    "    print(idx, sentence_gateways)\n",
    "print(\"\\nAND GATEWAYS\")\n",
    "for idx, sentence_gateways in enumerate(o_and_gateways):\n",
    "    print(idx, sentence_gateways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1df07",
   "metadata": {},
   "source": [
    "## 3 Evaluate Extraction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
