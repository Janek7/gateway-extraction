{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9c0bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from petreader.labels import *\n",
    "import pandas as pd\n",
    "\n",
    "from PetReader import pet_reader\n",
    "from labels import *\n",
    "from utils import read_json_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc68d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a926cc7",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "021cb907",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cls_goldstandard = read_json_to_dict(\"data/other/token_goldstandard.json\")\n",
    "\n",
    "keywords_gold_token_cls_results = read_json_to_dict(\"data/results/key_words_gold/results-token-classification.json\")\n",
    "keywords_gold_token_cls = read_json_to_dict(\"data/results/key_words_gold/token-classification.json\")\n",
    "\n",
    "keywords_literature_token_cls_results = read_json_to_dict(\"data/results/key_words_literature/results-token-classification.json\")\n",
    "keywords_literature_token_cls = read_json_to_dict(\"data/results/key_words_literature/token-classification.json\")\n",
    "\n",
    "keywords_literature_filtered_token_cls_results = read_json_to_dict(\"data/results/key_words_literature_filtered/results-token-classification.json\")\n",
    "keywords_literature_filtered_token_cls = read_json_to_dict(\"data/results/key_words_literature_filtered/token-classification.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11650ed9",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7fac9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'otherwise']\n"
     ]
    }
   ],
   "source": [
    "from petreader.TokenClassification import TokenClassification\n",
    "from petbenchmarks.tokenclassification import TokenClassificationBenchmark\n",
    "benchmark = TokenClassificationBenchmark(pet_reader.token_dataset)\n",
    "\n",
    "gold = benchmark.GetGoldStandard()\n",
    "print(gold['doc-3.2'][XOR_GATEWAY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7c8f886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [['If'], ['otherwise']], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(pet_reader.token_dataset.GetXORGateways('doc-3.2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8620fb6",
   "metadata": {},
   "source": [
    "## 1) Analyze Keyword Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3323411a",
   "metadata": {},
   "source": [
    "### a) check for documents with recall != 1 (GOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e08188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_name, doc_dict in keywords_gold_token_cls_results.items():\n",
    "    if doc_name.startswith('doc'):\n",
    "        if doc_dict[XOR_GATEWAY][RECALL] != 1 and doc_dict[XOR_GATEWAY][SUPPORT] != 0:\n",
    "            print(doc_name.center(100, '-'))\n",
    "            print(\"--Text--\")\n",
    "            for i, line in enumerate(pet_reader.get_doc_sentences(doc_name)):\n",
    "                print(i, ' '.join(line))\n",
    "            print()\n",
    "            print(\"--Results--:\", doc_dict[XOR_GATEWAY])\n",
    "            print()\n",
    "            print(\"--Extracted--:\", keywords_gold_token_cls[doc_name][XOR_GATEWAY])\n",
    "            print()\n",
    "            print(\"--Gold standard--:\", token_cls_goldstandard[doc_name][XOR_GATEWAY])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d03e3",
   "metadata": {},
   "source": [
    "### b) In how many different documents appear the Gateways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73dffba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldstandard_object_path = \"data/other/token_goldstandard.pkl\"\n",
    "with open(os.path.abspath(goldstandard_object_path), 'rb') as file:\n",
    "    goldstandard = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d336bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gateway_stats(classifications, gateway_type):\n",
    "    gateway_stats = {}\n",
    "    for doc_name, doc_dict in classifications.items():\n",
    "        for gateway_token in doc_dict[gateway_type]:\n",
    "            if gateway_token not in gateway_stats:\n",
    "                gateway_stats[gateway_token] = {\"count\": 1, \"docs\": 1, \"doc_list\": set([doc_name])}\n",
    "            else:\n",
    "                gateway_stats[gateway_token][\"count\"] += 1\n",
    "                gateway_stats[gateway_token][\"doc_list\"].add(doc_name)\n",
    "                gateway_stats[gateway_token][\"docs\"] = len(gateway_stats[gateway_token][\"doc_list\"])\n",
    "            \n",
    "    gateway_stats = collections.OrderedDict(sorted(gateway_stats.items(), \n",
    "                                                   key=lambda kv_pair: (kv_pair[1][\"count\"], kv_pair[1][\"docs\"]),\n",
    "                                                   reverse=True))\n",
    "    return gateway_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2ac0f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If              {'count': 52, 'docs': 23, 'doc_list': {'doc-9.1', 'doc-5.3', 'doc-1.1', 'doc-5.1', 'doc-3.8', 'doc-9\n",
      "or              {'count': 20, 'docs': 15, 'doc_list': {'doc-10.12', 'doc-10.4', 'doc-10.10', 'doc-2.2', 'doc-10.7', \n",
      "case            {'count': 12, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.1', 'doc-2.2'}}\n",
      "if              {'count': 9, 'docs': 7, 'doc_list': {'doc-9.5', 'doc-4.1', 'doc-10.2', 'doc-9.1', 'doc-6.1', 'doc-1.\n",
      "In              {'count': 9, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.1', 'doc-2.2'}}\n",
      "Otherwise       {'count': 6, 'docs': 5, 'doc_list': {'doc-9.5', 'doc-3.5', 'doc-7.1', 'doc-3.6', 'doc-3.8'}}\n",
      "otherwise       {'count': 6, 'docs': 5, 'doc_list': {'doc-8.3', 'doc-9.1', 'doc-2.1', 'doc-3.2', 'doc-1.2'}}\n",
      "of              {'count': 5, 'docs': 2, 'doc_list': {'doc-9.5', 'doc-2.2'}}\n",
      "For             {'count': 5, 'docs': 2, 'doc_list': {'doc-2.1', 'doc-4.1'}}\n",
      "the             {'count': 4, 'docs': 2, 'doc_list': {'doc-2.1', 'doc-2.2'}}\n",
      "Sometimes       {'count': 2, 'docs': 2, 'doc_list': {'doc-6.4', 'doc-8.2'}}\n",
      "Should          {'count': 2, 'docs': 1, 'doc_list': {'doc-6.1'}}\n",
      "sometimes       {'count': 2, 'docs': 1, 'doc_list': {'doc-6.4'}}\n",
      "either          {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "whereas         {'count': 1, 'docs': 1, 'doc_list': {'doc-9.5'}}\n",
      "each            {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "patient         {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "for             {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "which           {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "it              {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "can             {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "also            {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "happen          {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "that            {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "Under           {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "certain         {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "circumstances   {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n"
     ]
    }
   ],
   "source": [
    "# XOR GOLDSTANDARD\n",
    "goldstandard_xor_gateway_stats = get_gateway_stats(goldstandard, XOR_GATEWAY)\n",
    "for gateway, stats in goldstandard_xor_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "065b60f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or              {'count': 64, 'docs': 29, 'doc_list': {'doc-10.12', 'doc-10.4', 'doc-10.10', 'doc-5.3', 'doc-10.9', \n",
      "for             {'count': 59, 'docs': 23, 'doc_list': {'doc-5.3', 'doc-5.1', 'doc-1.1', 'doc-9.3', 'doc-9.5', 'doc-9\n",
      "If              {'count': 55, 'docs': 24, 'doc_list': {'doc-10.10', 'doc-9.1', 'doc-5.3', 'doc-5.1', 'doc-1.1', 'doc\n",
      "case            {'count': 15, 'docs': 4, 'doc_list': {'doc-9.5', 'doc-6.4', 'doc-2.1', 'doc-2.2'}}\n",
      "if              {'count': 12, 'docs': 10, 'doc_list': {'doc-9.5', 'doc-4.1', 'doc-10.2', 'doc-9.1', 'doc-5.4', 'doc-\n",
      "For             {'count': 10, 'docs': 4, 'doc_list': {'doc-5.4', 'doc-4.1', 'doc-2.1', 'doc-2.2'}}\n",
      "In              {'count': 9, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.1', 'doc-2.2'}}\n",
      "of              {'count': 8, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-6.4', 'doc-2.2'}}\n",
      "otherwise       {'count': 7, 'docs': 5, 'doc_list': {'doc-8.3', 'doc-9.1', 'doc-2.1', 'doc-3.2', 'doc-1.2'}}\n",
      "Otherwise       {'count': 6, 'docs': 5, 'doc_list': {'doc-9.5', 'doc-3.5', 'doc-7.1', 'doc-3.6', 'doc-3.8'}}\n",
      "should          {'count': 6, 'docs': 3, 'doc_list': {'doc-2.1', 'doc-9.1', 'doc-2.2'}}\n",
      "either          {'count': 4, 'docs': 3, 'doc_list': {'doc-6.4', 'doc-2.1', 'doc-2.2'}}\n",
      "the             {'count': 4, 'docs': 2, 'doc_list': {'doc-2.1', 'doc-2.2'}}\n",
      "Sometimes       {'count': 3, 'docs': 3, 'doc_list': {'doc-6.4', 'doc-8.2', 'doc-8.3'}}\n",
      "in              {'count': 3, 'docs': 2, 'doc_list': {'doc-6.4', 'doc-2.2'}}\n",
      "whereas         {'count': 2, 'docs': 2, 'doc_list': {'doc-9.5', 'doc-1.2'}}\n",
      "Should          {'count': 2, 'docs': 1, 'doc_list': {'doc-6.1'}}\n",
      "sometimes       {'count': 2, 'docs': 1, 'doc_list': {'doc-6.4'}}\n",
      "it              {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "can             {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "also            {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "happen          {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "that            {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "Under           {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "certain         {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "circumstances   {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "Either          {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "each            {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "patient         {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "which           {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n"
     ]
    }
   ],
   "source": [
    "# XOR GOLD\n",
    "gold_xor_gateway_stats = get_gateway_stats(keywords_gold_token_cls, XOR_GATEWAY)\n",
    "for gateway, stats in gold_xor_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "599f97dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or              {'count': 64, 'docs': 29, 'doc_list': {'doc-10.12', 'doc-10.4', 'doc-10.10', 'doc-5.3', 'doc-10.9', \n",
      "If              {'count': 55, 'docs': 24, 'doc_list': {'doc-10.10', 'doc-9.1', 'doc-5.3', 'doc-1.1', 'doc-5.1', 'doc\n",
      "whether         {'count': 13, 'docs': 9, 'doc_list': {'doc-10.12', 'doc-4.1', 'doc-2.2', 'doc-9.1', 'doc-3.6', 'doc-\n",
      "if              {'count': 12, 'docs': 10, 'doc_list': {'doc-9.5', 'doc-4.1', 'doc-10.2', 'doc-9.1', 'doc-5.4', 'doc-\n",
      "case            {'count': 11, 'docs': 4, 'doc_list': {'doc-9.5', 'doc-6.4', 'doc-2.1', 'doc-2.2'}}\n",
      "when            {'count': 8, 'docs': 7, 'doc_list': {'doc-1.4', 'doc-5.2', 'doc-4.1', 'doc-5.3', 'doc-6.2', 'doc-8.1\n",
      "In              {'count': 8, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.1', 'doc-2.2'}}\n",
      "When            {'count': 7, 'docs': 7, 'doc_list': {'doc-8.3', 'doc-3.5', 'doc-3.1', 'doc-6.1', 'doc-3.6', 'doc-8.2\n",
      "otherwise       {'count': 7, 'docs': 5, 'doc_list': {'doc-8.3', 'doc-9.1', 'doc-2.1', 'doc-3.2', 'doc-1.2'}}\n",
      "of              {'count': 7, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-6.4', 'doc-2.2'}}\n",
      "Otherwise       {'count': 6, 'docs': 5, 'doc_list': {'doc-9.5', 'doc-3.5', 'doc-7.1', 'doc-3.6', 'doc-3.8'}}\n",
      "either          {'count': 4, 'docs': 3, 'doc_list': {'doc-6.4', 'doc-2.1', 'doc-2.2'}}\n",
      "in              {'count': 3, 'docs': 2, 'doc_list': {'doc-6.4', 'doc-2.2'}}\n",
      "only            {'count': 2, 'docs': 2, 'doc_list': {'doc-2.2', 'doc-4.1'}}\n",
      "until           {'count': 1, 'docs': 1, 'doc_list': {'doc-6.1'}}\n",
      "not             {'count': 1, 'docs': 1, 'doc_list': {'doc-3.6'}}\n",
      "unless          {'count': 1, 'docs': 1, 'doc_list': {'doc-1.4'}}\n",
      "Either          {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n"
     ]
    }
   ],
   "source": [
    "# XOR LITERATURE\n",
    "literature_xor_gateway_stats = get_gateway_stats(keywords_literature_token_cls, XOR_GATEWAY)\n",
    "for gateway, stats in literature_xor_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02fd4706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-2.2'}}\n",
      "meantime        {'count': 2, 'docs': 2, 'doc_list': {'doc-3.2', 'doc-1.1'}}\n",
      "While           {'count': 2, 'docs': 2, 'doc_list': {'doc-1.4', 'doc-1.3'}}\n",
      "At              {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "same            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "time            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "In              {'count': 1, 'docs': 1, 'doc_list': {'doc-1.1'}}\n",
      "two             {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "concurrent      {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "activities      {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "are             {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "triggered       {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "Meantime        {'count': 1, 'docs': 1, 'doc_list': {'doc-3.5'}}\n",
      "whereas         {'count': 1, 'docs': 1, 'doc_list': {'doc-1.2'}}\n"
     ]
    }
   ],
   "source": [
    "# AND GOLDSTANDARD\n",
    "goldstandard_and_gateway_stats = get_gateway_stats(goldstandard, AND_GATEWAY)\n",
    "for gateway, stats in goldstandard_and_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4cc7a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereas         {'count': 2, 'docs': 2, 'doc_list': {'doc-9.5', 'doc-1.2'}}\n",
      "While           {'count': 2, 'docs': 2, 'doc_list': {'doc-1.4', 'doc-1.3'}}\n",
      "the             {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-2.2'}}\n",
      "meantime        {'count': 2, 'docs': 2, 'doc_list': {'doc-3.2', 'doc-1.1'}}\n",
      "Meantime        {'count': 1, 'docs': 1, 'doc_list': {'doc-3.5'}}\n",
      "At              {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "same            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "time            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "In              {'count': 1, 'docs': 1, 'doc_list': {'doc-1.1'}}\n",
      "two             {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "concurrent      {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "activities      {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "are             {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "triggered       {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n"
     ]
    }
   ],
   "source": [
    "# AND GOLD\n",
    "gold_and_gateway_stats = get_gateway_stats(keywords_gold_token_cls, AND_GATEWAY)\n",
    "for gateway, stats in gold_and_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fb0f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereas         {'count': 2, 'docs': 2, 'doc_list': {'doc-9.5', 'doc-1.2'}}\n",
      "While           {'count': 2, 'docs': 2, 'doc_list': {'doc-1.4', 'doc-1.3'}}\n",
      "In              {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-1.4'}}\n",
      "the             {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-2.2'}}\n",
      "meantime        {'count': 2, 'docs': 2, 'doc_list': {'doc-3.2', 'doc-1.1'}}\n",
      "addition        {'count': 2, 'docs': 1, 'doc_list': {'doc-1.4'}}\n",
      "to              {'count': 2, 'docs': 1, 'doc_list': {'doc-1.4'}}\n",
      "Meantime        {'count': 1, 'docs': 1, 'doc_list': {'doc-3.5'}}\n",
      "in              {'count': 1, 'docs': 1, 'doc_list': {'doc-1.4'}}\n",
      "At              {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "same            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "time            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n"
     ]
    }
   ],
   "source": [
    "# AND LITERATURE\n",
    "literature_and_gateway_stats = get_gateway_stats(keywords_literature_token_cls, AND_GATEWAY)\n",
    "for gateway, stats in literature_and_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610712ab",
   "metadata": {},
   "source": [
    "### c) Analyze False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a55c49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_keyword_metrics(gold_standard, extracted_tokens, gateway_type: str, doc_names=[]):\n",
    "    keyword_metrics = {}\n",
    "    empty_stats_dict = {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TPs\": [], \"FPs\": [], \"FNs\": []}\n",
    "\n",
    "    for doc_name, doc_dict in keywords_gold_token_cls_results.items():\n",
    "        \n",
    "        if doc_name.startswith('doc'):  # result dict contains as well keys for statistics            \n",
    "            if not doc_names or doc_name in doc_names:  # filter for (optionally) passed list\n",
    "\n",
    "                # remove gateways step by step if they were identified; left overs are false positives\n",
    "                not_discovered_golds = [g.lower() for g in gold_standard[doc_name][gateway_type].copy()]\n",
    "\n",
    "                for extracted in extracted_tokens[doc_name][gateway_type]:\n",
    "                    extracted = extracted.lower()\n",
    "\n",
    "                    # setup keyword dict in case it was not observed yet\n",
    "                    if extracted not in keyword_metrics:\n",
    "                        keyword_metrics[extracted] = copy.deepcopy(empty_stats_dict)\n",
    "\n",
    "                    # 1) CHECK FOR FPs\n",
    "                    if extracted not in not_discovered_golds:\n",
    "                        keyword_metrics[extracted][\"FP\"] += 1\n",
    "                        keyword_metrics[extracted][\"FPs\"].append(doc_name)\n",
    "\n",
    "                    # 2) CHECK FOR TPs\n",
    "                    else:\n",
    "                        keyword_metrics[extracted][\"TP\"] += 1\n",
    "                        keyword_metrics[extracted][\"TPs\"].append(doc_name)\n",
    "                        not_discovered_golds.remove(extracted)\n",
    "\n",
    "                # 3) FILL FNs (FPs from list not_discovered_xor_golds got removed during previous loop)\n",
    "                for not_extracted in not_discovered_golds:\n",
    "\n",
    "                    # setup keyword dict in case it was not observed yet\n",
    "                    if not_extracted not in keyword_metrics:\n",
    "                        keyword_metrics[not_extracted] = copy.deepcopy(empty_stats_dict)\n",
    "\n",
    "                    keyword_metrics[not_extracted][\"FN\"] += 1\n",
    "                    keyword_metrics[not_extracted][\"FNs\"].append(doc_name)\n",
    "    return keyword_metrics\n",
    "\n",
    "\n",
    "def print_keyword_dict(keyword_metrics, first_x=None, order_by=None):\n",
    "    \n",
    "    if order_by:\n",
    "        keyword_metrics = collections.OrderedDict(sorted(keyword_metrics.items(), key=lambda kv_pair: kv_pair[1][order_by],\n",
    "                                                         reverse=True))\n",
    "    for keyword, metrics in keyword_metrics.items():\n",
    "        print(f\"{keyword} \".ljust(15), end='')\n",
    "        for k, v in metrics.items():\n",
    "            if (len(k) == 3 and k.endswith(\"s\")) or k == 'doc_list':\n",
    "                if first_x:\n",
    "                    print(f\"{k}: \", end='')\n",
    "                    print(list(v)[:first_x], \"...\", end='')\n",
    "                    print(\", \", end='')\n",
    "            else:\n",
    "                print(f\"{k}: \", end='')\n",
    "                print(v, end='')\n",
    "                print(\", \", end='')\n",
    "        print()\n",
    "\n",
    "\n",
    "gold_xor_keyword_metrics = analyze_keyword_metrics(token_cls_goldstandard, keywords_gold_token_cls, XOR_GATEWAY)\n",
    "gold_and_keyword_metrics = analyze_keyword_metrics(token_cls_goldstandard, keywords_gold_token_cls, AND_GATEWAY)\n",
    "literature_xor_keyword_metrics = analyze_keyword_metrics(token_cls_goldstandard, keywords_literature_token_cls, XOR_GATEWAY)\n",
    "literature_and_keyword_metrics = analyze_keyword_metrics(token_cls_goldstandard, keywords_literature_token_cls, AND_GATEWAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830b1d2",
   "metadata": {},
   "source": [
    "#### EXCLUSIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5f0c8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for            TP: 6, FP: 63, FN: 0, TPs: ['doc-2.1', 'doc-2.1'] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "or             TP: 20, FP: 44, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "if             TP: 61, FP: 6, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1', 'doc-10.10'] ..., FNs: [] ..., \n",
      "should         TP: 2, FP: 6, FN: 0, TPs: ['doc-6.1', 'doc-6.1'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "either         TP: 1, FP: 4, FN: 0, TPs: ['doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.1'] ..., FNs: [] ..., \n",
      "in             TP: 9, FP: 3, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "case           TP: 12, FP: 3, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "of             TP: 5, FP: 3, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "otherwise      TP: 12, FP: 1, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1'] ..., FNs: [] ..., \n",
      "whereas        TP: 1, FP: 1, FN: 0, TPs: ['doc-9.5'] ..., FPs: ['doc-1.2'] ..., FNs: [] ..., \n",
      "sometimes      TP: 4, FP: 1, FN: 0, TPs: ['doc-6.4', 'doc-6.4'] ..., FPs: ['doc-8.3'] ..., FNs: [] ..., \n",
      "the            TP: 4, FP: 0, FN: 0, TPs: ['doc-2.2', 'doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "each           TP: 1, FP: 0, FN: 0, TPs: ['doc-4.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "patient        TP: 1, FP: 0, FN: 0, TPs: ['doc-4.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "which          TP: 1, FP: 0, FN: 0, TPs: ['doc-4.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "it             TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "can            TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "also           TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "happen         TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "that           TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "under          TP: 1, FP: 0, FN: 0, TPs: ['doc-8.3'] ..., FPs: [] ..., FNs: [] ..., \n",
      "certain        TP: 1, FP: 0, FN: 0, TPs: ['doc-8.3'] ..., FPs: [] ..., FNs: [] ..., \n",
      "circumstances  TP: 1, FP: 0, FN: 0, TPs: ['doc-8.3'] ..., FPs: [] ..., FNs: [] ..., \n"
     ]
    }
   ],
   "source": [
    "# GOLD keywords\n",
    "print_keyword_dict(gold_xor_keyword_metrics, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "753298c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or             TP: 20, FP: 44, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "when           TP: 0, FP: 15, FN: 0, TPs: [] ..., FPs: ['doc-3.6', 'doc-6.2'] ..., FNs: [] ..., \n",
      "whether        TP: 0, FP: 13, FN: 0, TPs: [] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "if             TP: 61, FP: 6, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1', 'doc-10.10'] ..., FNs: [] ..., \n",
      "either         TP: 1, FP: 4, FN: 0, TPs: ['doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.1'] ..., FNs: [] ..., \n",
      "in             TP: 9, FP: 2, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-6.4'] ..., FNs: [] ..., \n",
      "case           TP: 9, FP: 2, FN: 3, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-6.4'] ..., FNs: ['doc-2.1', 'doc-2.1'] ..., \n",
      "of             TP: 5, FP: 2, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-6.4'] ..., FNs: [] ..., \n",
      "only           TP: 0, FP: 2, FN: 0, TPs: [] ..., FPs: ['doc-2.2', 'doc-4.1'] ..., FNs: [] ..., \n",
      "not            TP: 0, FP: 1, FN: 0, TPs: [] ..., FPs: ['doc-3.6'] ..., FNs: [] ..., \n",
      "otherwise      TP: 12, FP: 1, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1'] ..., FNs: [] ..., \n",
      "unless         TP: 0, FP: 1, FN: 0, TPs: [] ..., FPs: ['doc-1.4'] ..., FNs: [] ..., \n",
      "until          TP: 0, FP: 1, FN: 0, TPs: [] ..., FPs: ['doc-6.1'] ..., FNs: [] ..., \n",
      "the            TP: 0, FP: 0, FN: 4, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.2', 'doc-2.1'] ..., \n",
      "for            TP: 0, FP: 0, FN: 6, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1', 'doc-2.1'] ..., \n",
      "whereas        TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-9.5'] ..., \n",
      "should         TP: 0, FP: 0, FN: 2, TPs: [] ..., FPs: [] ..., FNs: ['doc-6.1', 'doc-6.1'] ..., \n",
      "each           TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-4.1'] ..., \n",
      "patient        TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-4.1'] ..., \n",
      "which          TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-4.1'] ..., \n",
      "sometimes      TP: 0, FP: 0, FN: 4, TPs: [] ..., FPs: [] ..., FNs: ['doc-6.4', 'doc-6.4'] ..., \n",
      "it             TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "can            TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "also           TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "happen         TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "that           TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "under          TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.3'] ..., \n",
      "certain        TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.3'] ..., \n",
      "circumstances  TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.3'] ..., \n"
     ]
    }
   ],
   "source": [
    "# LITERATURE keywords\n",
    "print_keyword_dict(literature_xor_keyword_metrics, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a7a3b",
   "metadata": {},
   "source": [
    "#### PARALLEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08194d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereas        TP: 1, FP: 1, FN: 0, TPs: ['doc-1.2'] ..., FPs: ['doc-9.5'] ..., FNs: [] ..., \n",
      "at             TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "the            TP: 2, FP: 0, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "same           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "time           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "in             TP: 1, FP: 0, FN: 0, TPs: ['doc-1.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "meantime       TP: 3, FP: 0, FN: 0, TPs: ['doc-1.1', 'doc-3.5'] ..., FPs: [] ..., FNs: [] ..., \n",
      "two            TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "concurrent     TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "activities     TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "are            TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "triggered      TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "while          TP: 2, FP: 0, FN: 0, TPs: ['doc-1.4', 'doc-1.3'] ..., FPs: [] ..., FNs: [] ..., \n"
     ]
    }
   ],
   "source": [
    "# GOLD keywords\n",
    "print_keyword_dict(gold_and_keyword_metrics, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "046d3acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in             TP: 1, FP: 2, FN: 0, TPs: ['doc-1.1'] ..., FPs: ['doc-1.4', 'doc-1.4'] ..., FNs: [] ..., \n",
      "addition       TP: 0, FP: 2, FN: 0, TPs: [] ..., FPs: ['doc-1.4', 'doc-1.4'] ..., FNs: [] ..., \n",
      "to             TP: 0, FP: 2, FN: 0, TPs: [] ..., FPs: ['doc-1.4', 'doc-1.4'] ..., FNs: [] ..., \n",
      "whereas        TP: 1, FP: 1, FN: 0, TPs: ['doc-1.2'] ..., FPs: ['doc-9.5'] ..., FNs: [] ..., \n",
      "at             TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "the            TP: 2, FP: 0, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "same           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "time           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "meantime       TP: 3, FP: 0, FN: 0, TPs: ['doc-1.1', 'doc-3.5'] ..., FPs: [] ..., FNs: [] ..., \n",
      "two            TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "concurrent     TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "activities     TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "are            TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "triggered      TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "while          TP: 2, FP: 0, FN: 0, TPs: ['doc-1.4', 'doc-1.3'] ..., FPs: [] ..., FNs: [] ..., \n"
     ]
    }
   ],
   "source": [
    "# LITERATURE keywords\n",
    "print_keyword_dict(literature_and_keyword_metrics, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0079aa",
   "metadata": {},
   "source": [
    "### d) Combined analysis (counts & FPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a811c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# available stats data:\n",
    "goldstandard_xor_gateway_stats\n",
    "goldstandard_and_gateway_stats\n",
    "gold_xor_gateway_stats\n",
    "gold_and_gateway_stats\n",
    "literature_xor_gateway_stats\n",
    "literature_and_gateway_stats\n",
    "\n",
    "# available metrics data\n",
    "gold_xor_keyword_metrics\n",
    "gold_and_keyword_metrics\n",
    "literature_xor_keyword_metrics\n",
    "literature_and_keyword_metrics\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42e619a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_stats_metrics_dict(metrics, stats):\n",
    "    merged = {}\n",
    "    for gateway_token, gateway_token_stats in stats.items():\n",
    "        gateway_token = gateway_token.lower()\n",
    "        if gateway_token in metrics:\n",
    "            merged[gateway_token] = {**gateway_token_stats, **metrics[gateway_token]}\n",
    "        else:\n",
    "            print(gateway_token)\n",
    "            merged[gateway_token] = gateway_token_stats\n",
    "    return merged\n",
    "\n",
    "gold_xor_merged = merge_stats_metrics_dict(gold_xor_keyword_metrics, gold_xor_gateway_stats)\n",
    "gold_and_merged = merge_stats_metrics_dict(gold_and_keyword_metrics, gold_and_gateway_stats)\n",
    "literature_xor_merged = merge_stats_metrics_dict(literature_xor_keyword_metrics, literature_xor_gateway_stats)\n",
    "literature_and_merged = merge_stats_metrics_dict(literature_and_keyword_metrics, literature_and_gateway_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c99437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keyword_dict(keyword_dictionary, path, reference_literature_set=None):\n",
    "    keyword_dictionary = copy.deepcopy(keyword_dictionary)\n",
    "    # prepare list of dicts (one for each row) for convert to df\n",
    "    output_list = []\n",
    "    for keyword, d in keyword_dictionary.items():\n",
    "        del d[\"doc_list\"]\n",
    "#         del d[\"TPs\"]\n",
    "#         del d[\"FPs\"]\n",
    "#         del d[\"FNs\"]\n",
    "        d[\"keyword\"] = keyword\n",
    "        if reference_literature_set:\n",
    "            d[\"in_literature\"] = keyword in reference_literature_set\n",
    "        output_list.append(d)\n",
    "    df = pd.DataFrame(output_list)\n",
    "    # reorder columns\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    # save as excel\n",
    "    df.to_excel(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df61618",
   "metadata": {},
   "source": [
    "#### 1 - Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6984448",
   "metadata": {},
   "source": [
    "##### xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7f1d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or             count: 64, docs: 29, TP: 20, FP: 44, FN: 0, \n",
      "when           count: 7, docs: 7, TP: 0, FP: 15, FN: 0, \n",
      "whether        count: 13, docs: 9, TP: 0, FP: 13, FN: 0, \n",
      "if             count: 12, docs: 10, TP: 61, FP: 6, FN: 0, \n",
      "either         count: 1, docs: 1, TP: 1, FP: 4, FN: 0, \n",
      "case           count: 11, docs: 4, TP: 9, FP: 2, FN: 3, \n",
      "in             count: 3, docs: 2, TP: 9, FP: 2, FN: 0, \n",
      "of             count: 7, docs: 3, TP: 5, FP: 2, FN: 0, \n",
      "only           count: 2, docs: 2, TP: 0, FP: 2, FN: 0, \n",
      "otherwise      count: 6, docs: 5, TP: 12, FP: 1, FN: 0, \n",
      "until          count: 1, docs: 1, TP: 0, FP: 1, FN: 0, \n",
      "not            count: 1, docs: 1, TP: 0, FP: 1, FN: 0, \n",
      "unless         count: 1, docs: 1, TP: 0, FP: 1, FN: 0, \n"
     ]
    }
   ],
   "source": [
    "print_keyword_dict(literature_xor_merged, first_x=0, order_by='FP')\n",
    "save_keyword_dict(literature_xor_merged, \"data/keywords/analysis/literature_xor_merged.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef8912",
   "metadata": {},
   "source": [
    "##### and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "defeca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in             count: 1, docs: 1, TP: 1, FP: 2, FN: 0, \n",
      "addition       count: 2, docs: 1, TP: 0, FP: 2, FN: 0, \n",
      "to             count: 2, docs: 1, TP: 0, FP: 2, FN: 0, \n",
      "whereas        count: 2, docs: 2, TP: 1, FP: 1, FN: 0, \n",
      "while          count: 2, docs: 2, TP: 2, FP: 0, FN: 0, \n",
      "the            count: 2, docs: 2, TP: 2, FP: 0, FN: 0, \n",
      "meantime       count: 1, docs: 1, TP: 3, FP: 0, FN: 0, \n",
      "at             count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "same           count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "time           count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n"
     ]
    }
   ],
   "source": [
    "print_keyword_dict(literature_and_merged, first_x=0, order_by='FP')\n",
    "save_keyword_dict(literature_and_merged, \"data/keywords/analysis/literature_and_merged.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19623207",
   "metadata": {},
   "source": [
    "#### 2 - GOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "607c4871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utilities:Load keywords 'literature' ...\n",
      "INFO:utilities:Loaded 15 XOR and 11 AND keywords (literature)\n",
      "INFO:utilities:Used XOR keywords: ['either', 'else', 'if', 'if not', 'in case', 'in case of', 'only', 'only if', 'or', 'otherwise', 'till', 'unless', 'until', 'when', 'whether']\n",
      "INFO:utilities:Used AND keywords: ['at the same time', 'concurrently', 'in addition to', 'in parallel', 'in parallel with this', 'in the meantime', 'meantime', 'meanwhile', 'simultaneously', 'whereas', 'while']\n"
     ]
    }
   ],
   "source": [
    "literature_xor_keywords, literature_and_keywords = read_keywords(LITERATURE)\n",
    "import itertools\n",
    "literature_xor_keywords_tokens = list(set(itertools.chain(*[keyword.split(\" \") for keyword in literature_xor_keywords])))\n",
    "literature_and_keywords_tokens = list(set(itertools.chain(*[keyword.split(\" \") for keyword in literature_and_keywords])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a07ddc",
   "metadata": {},
   "source": [
    "##### xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93216d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for            count: 10, docs: 4, TP: 6, FP: 63, FN: 0, \n",
      "or             count: 64, docs: 29, TP: 20, FP: 44, FN: 0, \n",
      "if             count: 12, docs: 10, TP: 61, FP: 6, FN: 0, \n",
      "should         count: 2, docs: 1, TP: 2, FP: 6, FN: 0, \n",
      "either         count: 1, docs: 1, TP: 1, FP: 4, FN: 0, \n",
      "case           count: 15, docs: 4, TP: 12, FP: 3, FN: 0, \n",
      "in             count: 3, docs: 2, TP: 9, FP: 3, FN: 0, \n",
      "of             count: 8, docs: 3, TP: 5, FP: 3, FN: 0, \n",
      "otherwise      count: 6, docs: 5, TP: 12, FP: 1, FN: 0, \n",
      "sometimes      count: 2, docs: 1, TP: 4, FP: 1, FN: 0, \n",
      "whereas        count: 2, docs: 2, TP: 1, FP: 1, FN: 0, \n",
      "the            count: 4, docs: 2, TP: 4, FP: 0, FN: 0, \n",
      "it             count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "can            count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "also           count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "happen         count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "that           count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "under          count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "certain        count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "circumstances  count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "each           count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "patient        count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "which          count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n"
     ]
    }
   ],
   "source": [
    "print_keyword_dict(gold_xor_merged, first_x=0, order_by='FP')\n",
    "save_keyword_dict(gold_xor_merged, \"data/keywords/analysis/gold_xor_merged.xlsx\", reference_literature_set=literature_xor_keywords_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070813e",
   "metadata": {},
   "source": [
    "##### and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43c750bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereas        count: 2, docs: 2, TP: 1, FP: 1, FN: 0, \n",
      "while          count: 2, docs: 2, TP: 2, FP: 0, FN: 0, \n",
      "the            count: 2, docs: 2, TP: 2, FP: 0, FN: 0, \n",
      "meantime       count: 1, docs: 1, TP: 3, FP: 0, FN: 0, \n",
      "at             count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "same           count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "time           count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "in             count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "two            count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "concurrent     count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "activities     count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "are            count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n",
      "triggered      count: 1, docs: 1, TP: 1, FP: 0, FN: 0, \n"
     ]
    }
   ],
   "source": [
    "print_keyword_dict(gold_and_merged, first_x=0, order_by='FP')\n",
    "save_keyword_dict(gold_and_merged, \"data/keywords/analysis/gold_and_merged.xlsx\", reference_literature_set=literature_and_keywords_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
