{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42ca6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from labels import *\n",
    "from utils import read_json_to_dict, read_and_set_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0cc34e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PetReader:Load RelationsExtraction dataset ...\n",
      "WARNING:datasets.builder:Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\relations-extraction\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02296757698059082,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5e08d27ef84e3e997cd83bda2a1a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PetReader:Load TokenClassification dataset ...\n",
      "WARNING:datasets.builder:Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\token-classification\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017894268035888672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5516d8be55074504a96e6e1c214d6b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import collections\n",
    "\n",
    "from PetReader import pet_reader\n",
    "from petreader.labels import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24e1a2",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "142400cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cls_goldstandard = read_json_to_dict(\"data/other/token_goldstandard.json\")\n",
    "\n",
    "keywords_gold_token_cls_results = read_json_to_dict(\"data/results/key_words_gold/results-token-classification.json\")\n",
    "keywords_gold_token_cls = read_json_to_dict(\"data/results/key_words_gold/token-classification.json\")\n",
    "\n",
    "keywords_literature_token_cls_results = read_json_to_dict(\"data/results/key_words_literature/results-token-classification.json\")\n",
    "keywords_literature_token_cls = read_json_to_dict(\"data/results/key_words_literature/token-classification.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22713b54",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad7ec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'otherwise']\n"
     ]
    }
   ],
   "source": [
    "from petreader.TokenClassification import TokenClassification\n",
    "from petbenchmarks.tokenclassification import TokenClassificationBenchmark\n",
    "benchmark = TokenClassificationBenchmark(pet_reader.token_dataset)\n",
    "\n",
    "gold = benchmark.GetGoldStandard()\n",
    "print(gold['doc-3.2'][XOR_GATEWAY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff11de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [['If'], ['otherwise']], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(pet_reader.token_dataset.GetXORGateways('doc-3.2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4821a9",
   "metadata": {},
   "source": [
    "## 1) Analyze Keyword Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb775c8",
   "metadata": {},
   "source": [
    "### a) check for documents with recall != 1 (GOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9688a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_name, doc_dict in keywords_gold_token_cls_results.items():\n",
    "    if doc_name.startswith('doc'):\n",
    "        if doc_dict[XOR_GATEWAY][RECALL] != 1 and doc_dict[XOR_GATEWAY][SUPPORT] != 0:\n",
    "            print(doc_name.center(100, '-'))\n",
    "            print(\"--Text--\")\n",
    "            for i, line in enumerate(pet_reader.get_doc_sentences(doc_name)):\n",
    "                print(i, ' '.join(line))\n",
    "            print()\n",
    "            print(\"--Results--:\", doc_dict[XOR_GATEWAY])\n",
    "            print()\n",
    "            print(\"--Extracted--:\", keywords_gold_token_cls[doc_name][XOR_GATEWAY])\n",
    "            print()\n",
    "            print(\"--Gold standard--:\", token_cls_goldstandard[doc_name][XOR_GATEWAY])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4163db",
   "metadata": {},
   "source": [
    "### b) In how many different documents appear the Gateways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "620d4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldstandard_object_path = \"data/other/token_goldstandard.pkl\"\n",
    "with open(os.path.abspath(goldstandard_object_path), 'rb') as file:\n",
    "    goldstandard = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1ab322d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gateway_stats(classifications, gateway_type):\n",
    "    gateway_stats = {}\n",
    "    for doc_name, doc_dict in classifications.items():\n",
    "        for gateway_token in doc_dict[gateway_type]:\n",
    "            if gateway_token not in gateway_stats:\n",
    "                gateway_stats[gateway_token] = {\"count\": 1, \"docs\": 1, \"doc_list\": set([doc_name])}\n",
    "            else:\n",
    "                gateway_stats[gateway_token][\"count\"] += 1\n",
    "                gateway_stats[gateway_token][\"doc_list\"].add(doc_name)\n",
    "                gateway_stats[gateway_token][\"docs\"] = len(gateway_stats[gateway_token][\"doc_list\"])\n",
    "            \n",
    "    gateway_stats = collections.OrderedDict(sorted(gateway_stats.items(), \n",
    "                                                   key=lambda kv_pair: (kv_pair[1][\"count\"], kv_pair[1][\"docs\"]),\n",
    "                                                   reverse=True))\n",
    "    return gateway_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ae864c7f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or              {'count': 64, 'docs': 29, 'doc_list': {'doc-10.14', 'doc-10.2', 'doc-6.1', 'doc-5.2', 'doc-10.12', '\n",
      "for             {'count': 59, 'docs': 23, 'doc_list': {'doc-8.2', 'doc-10.14', 'doc-8.3', 'doc-1.4', 'doc-6.1', 'doc\n",
      "If              {'count': 55, 'docs': 24, 'doc_list': {'doc-8.2', 'doc-10.14', 'doc-10.2', 'doc-6.1', 'doc-5.2', 'do\n",
      "case            {'count': 15, 'docs': 4, 'doc_list': {'doc-2.1', 'doc-9.5', 'doc-2.2', 'doc-6.4'}}\n",
      "if              {'count': 12, 'docs': 10, 'doc_list': {'doc-1.3', 'doc-9.5', 'doc-10.2', 'doc-6.3', 'doc-6.4', 'doc-\n",
      "For             {'count': 10, 'docs': 4, 'doc_list': {'doc-5.4', 'doc-4.1', 'doc-2.2', 'doc-2.1'}}\n",
      "In              {'count': 9, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.2', 'doc-2.1'}}\n",
      "of              {'count': 8, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.2', 'doc-6.4'}}\n",
      "otherwise       {'count': 7, 'docs': 5, 'doc_list': {'doc-3.2', 'doc-1.2', 'doc-8.3', 'doc-2.1', 'doc-9.1'}}\n",
      "Otherwise       {'count': 6, 'docs': 5, 'doc_list': {'doc-7.1', 'doc-3.6', 'doc-9.5', 'doc-3.8', 'doc-3.5'}}\n",
      "should          {'count': 6, 'docs': 3, 'doc_list': {'doc-9.1', 'doc-2.2', 'doc-2.1'}}\n",
      "either          {'count': 4, 'docs': 3, 'doc_list': {'doc-2.1', 'doc-2.2', 'doc-6.4'}}\n",
      "the             {'count': 4, 'docs': 2, 'doc_list': {'doc-2.2', 'doc-2.1'}}\n",
      "Sometimes       {'count': 3, 'docs': 3, 'doc_list': {'doc-8.2', 'doc-8.3', 'doc-6.4'}}\n",
      "in              {'count': 3, 'docs': 2, 'doc_list': {'doc-2.2', 'doc-6.4'}}\n",
      "whereas         {'count': 2, 'docs': 2, 'doc_list': {'doc-9.5', 'doc-1.2'}}\n",
      "Should          {'count': 2, 'docs': 1, 'doc_list': {'doc-6.1'}}\n",
      "sometimes       {'count': 2, 'docs': 1, 'doc_list': {'doc-6.4'}}\n",
      "it              {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "can             {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "also            {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "happen          {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "that            {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "Under           {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "certain         {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "circumstances   {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "Either          {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "each            {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "patient         {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "which           {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n"
     ]
    }
   ],
   "source": [
    "# XOR GOLDSTANDARD\n",
    "goldstandard_xor_gateway_stats = get_gateway_stats(goldstandard, XOR_GATEWAY)\n",
    "for gateway, stats in gold_xor_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ac57d3f3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or              {'count': 64, 'docs': 29, 'doc_list': {'doc-10.14', 'doc-10.2', 'doc-6.1', 'doc-5.2', 'doc-10.12', '\n",
      "for             {'count': 59, 'docs': 23, 'doc_list': {'doc-8.2', 'doc-10.14', 'doc-8.3', 'doc-1.4', 'doc-6.1', 'doc\n",
      "If              {'count': 55, 'docs': 24, 'doc_list': {'doc-8.2', 'doc-10.14', 'doc-10.2', 'doc-6.1', 'doc-5.2', 'do\n",
      "case            {'count': 15, 'docs': 4, 'doc_list': {'doc-2.1', 'doc-9.5', 'doc-2.2', 'doc-6.4'}}\n",
      "if              {'count': 12, 'docs': 10, 'doc_list': {'doc-1.3', 'doc-9.5', 'doc-10.2', 'doc-6.3', 'doc-6.4', 'doc-\n",
      "For             {'count': 10, 'docs': 4, 'doc_list': {'doc-5.4', 'doc-4.1', 'doc-2.2', 'doc-2.1'}}\n",
      "In              {'count': 9, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.2', 'doc-2.1'}}\n",
      "of              {'count': 8, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.2', 'doc-6.4'}}\n",
      "otherwise       {'count': 7, 'docs': 5, 'doc_list': {'doc-3.2', 'doc-1.2', 'doc-8.3', 'doc-2.1', 'doc-9.1'}}\n",
      "Otherwise       {'count': 6, 'docs': 5, 'doc_list': {'doc-7.1', 'doc-3.6', 'doc-9.5', 'doc-3.8', 'doc-3.5'}}\n",
      "should          {'count': 6, 'docs': 3, 'doc_list': {'doc-9.1', 'doc-2.2', 'doc-2.1'}}\n",
      "either          {'count': 4, 'docs': 3, 'doc_list': {'doc-2.1', 'doc-2.2', 'doc-6.4'}}\n",
      "the             {'count': 4, 'docs': 2, 'doc_list': {'doc-2.2', 'doc-2.1'}}\n",
      "Sometimes       {'count': 3, 'docs': 3, 'doc_list': {'doc-8.2', 'doc-8.3', 'doc-6.4'}}\n",
      "in              {'count': 3, 'docs': 2, 'doc_list': {'doc-2.2', 'doc-6.4'}}\n",
      "whereas         {'count': 2, 'docs': 2, 'doc_list': {'doc-9.5', 'doc-1.2'}}\n",
      "Should          {'count': 2, 'docs': 1, 'doc_list': {'doc-6.1'}}\n",
      "sometimes       {'count': 2, 'docs': 1, 'doc_list': {'doc-6.4'}}\n",
      "it              {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "can             {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "also            {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "happen          {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "that            {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "Under           {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "certain         {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "circumstances   {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "Either          {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "each            {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "patient         {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "which           {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n"
     ]
    }
   ],
   "source": [
    "# XOR GOLD\n",
    "gold_xor_gateway_stats = get_gateway_stats(keywords_gold_token_cls, XOR_GATEWAY)\n",
    "for gateway, stats in gold_xor_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "486a40c6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or              {'count': 64, 'docs': 29, 'doc_list': {'doc-10.14', 'doc-10.2', 'doc-6.1', 'doc-5.2', 'doc-10.12', '\n",
      "If              {'count': 55, 'docs': 24, 'doc_list': {'doc-8.2', 'doc-10.14', 'doc-10.2', 'doc-6.1', 'doc-5.2', 'do\n",
      "whether         {'count': 13, 'docs': 9, 'doc_list': {'doc-10.12', 'doc-3.6', 'doc-3.8', 'doc-6.3', 'doc-2.2', 'doc-\n",
      "if              {'count': 12, 'docs': 10, 'doc_list': {'doc-1.3', 'doc-9.5', 'doc-10.2', 'doc-6.3', 'doc-2.1', 'doc-\n",
      "case            {'count': 11, 'docs': 4, 'doc_list': {'doc-6.4', 'doc-9.5', 'doc-2.2', 'doc-2.1'}}\n",
      "when            {'count': 8, 'docs': 7, 'doc_list': {'doc-5.3', 'doc-6.2', 'doc-3.8', 'doc-1.4', 'doc-8.1', 'doc-5.2\n",
      "In              {'count': 8, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.2', 'doc-2.1'}}\n",
      "When            {'count': 7, 'docs': 7, 'doc_list': {'doc-8.2', 'doc-1.3', 'doc-3.6', 'doc-8.3', 'doc-3.5', 'doc-6.1\n",
      "otherwise       {'count': 7, 'docs': 5, 'doc_list': {'doc-3.2', 'doc-1.2', 'doc-8.3', 'doc-2.1', 'doc-9.1'}}\n",
      "of              {'count': 7, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.2', 'doc-6.4'}}\n",
      "Otherwise       {'count': 6, 'docs': 5, 'doc_list': {'doc-7.1', 'doc-3.6', 'doc-9.5', 'doc-3.8', 'doc-3.5'}}\n",
      "either          {'count': 4, 'docs': 3, 'doc_list': {'doc-6.4', 'doc-2.2', 'doc-2.1'}}\n",
      "in              {'count': 3, 'docs': 2, 'doc_list': {'doc-2.2', 'doc-6.4'}}\n",
      "only            {'count': 2, 'docs': 2, 'doc_list': {'doc-2.2', 'doc-4.1'}}\n",
      "until           {'count': 1, 'docs': 1, 'doc_list': {'doc-6.1'}}\n",
      "not             {'count': 1, 'docs': 1, 'doc_list': {'doc-3.6'}}\n",
      "unless          {'count': 1, 'docs': 1, 'doc_list': {'doc-1.4'}}\n",
      "Either          {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n"
     ]
    }
   ],
   "source": [
    "# XOR LITERATURE\n",
    "literature_xor_gateway_stats = get_gateway_stats(keywords_literature_token_cls, XOR_GATEWAY)\n",
    "for gateway, stats in literature_xor_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "44540ca0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-2.2'}}\n",
      "meantime        {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-3.2'}}\n",
      "While           {'count': 2, 'docs': 2, 'doc_list': {'doc-1.3', 'doc-1.4'}}\n",
      "At              {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "same            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "time            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "In              {'count': 1, 'docs': 1, 'doc_list': {'doc-1.1'}}\n",
      "two             {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "concurrent      {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "activities      {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "are             {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "triggered       {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "Meantime        {'count': 1, 'docs': 1, 'doc_list': {'doc-3.5'}}\n",
      "whereas         {'count': 1, 'docs': 1, 'doc_list': {'doc-1.2'}}\n"
     ]
    }
   ],
   "source": [
    "# AND GOLDSTANDARD\n",
    "goldstandard_and_gateway_stats = get_gateway_stats(goldstandard, AND_GATEWAY)\n",
    "for gateway, stats in goldstandard_and_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "106619e4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereas         {'count': 2, 'docs': 2, 'doc_list': {'doc-9.5', 'doc-1.2'}}\n",
      "While           {'count': 2, 'docs': 2, 'doc_list': {'doc-1.3', 'doc-1.4'}}\n",
      "the             {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-2.2'}}\n",
      "meantime        {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-3.2'}}\n",
      "Meantime        {'count': 1, 'docs': 1, 'doc_list': {'doc-3.5'}}\n",
      "At              {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "same            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "time            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "In              {'count': 1, 'docs': 1, 'doc_list': {'doc-1.1'}}\n",
      "two             {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "concurrent      {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "activities      {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "are             {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "triggered       {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n"
     ]
    }
   ],
   "source": [
    "# AND GOLD\n",
    "gold_and_gateway_stats = get_gateway_stats(keywords_gold_token_cls, AND_GATEWAY)\n",
    "for gateway, stats in gold_and_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "04c98243",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereas         {'count': 2, 'docs': 2, 'doc_list': {'doc-9.5', 'doc-1.2'}}\n",
      "While           {'count': 2, 'docs': 2, 'doc_list': {'doc-1.3', 'doc-1.4'}}\n",
      "In              {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-1.4'}}\n",
      "the             {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-2.2'}}\n",
      "meantime        {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-3.2'}}\n",
      "addition        {'count': 2, 'docs': 1, 'doc_list': {'doc-1.4'}}\n",
      "to              {'count': 2, 'docs': 1, 'doc_list': {'doc-1.4'}}\n",
      "Meantime        {'count': 1, 'docs': 1, 'doc_list': {'doc-3.5'}}\n",
      "in              {'count': 1, 'docs': 1, 'doc_list': {'doc-1.4'}}\n",
      "At              {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "same            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "time            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n"
     ]
    }
   ],
   "source": [
    "# AND LITERATURE\n",
    "literature_and_gateway_stats = get_gateway_stats(keywords_literature_token_cls, AND_GATEWAY)\n",
    "for gateway, stats in literature_and_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53519d27",
   "metadata": {},
   "source": [
    "### c) Analyze False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f6878364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_keyword_metrics(gold_standard, extracted_tokens, gateway_type: str, doc_names=[]):\n",
    "    keyword_metrics = {}\n",
    "    empty_stats_dict = {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TPs\": [], \"FPs\": [], \"FNs\": []}\n",
    "\n",
    "    for doc_name, doc_dict in keywords_gold_token_cls_results.items():\n",
    "        \n",
    "        if doc_name.startswith('doc'):  # result dict contains as well keys for statistics            \n",
    "            if not doc_names or doc_name in doc_names:  # filter for (optionally) passed list\n",
    "\n",
    "                # remove gateways step by step if they were identified; left overs are false positives\n",
    "                not_discovered_golds = [g.lower() for g in gold_standard[doc_name][gateway_type].copy()]\n",
    "\n",
    "                for extracted in extracted_tokens[doc_name][gateway_type]:\n",
    "                    extracted = extracted.lower()\n",
    "\n",
    "                    # setup keyword dict in case it was not observed yet\n",
    "                    if extracted not in keyword_metrics:\n",
    "                        keyword_metrics[extracted] = copy.deepcopy(empty_stats_dict)\n",
    "\n",
    "                    # 1) CHECK FOR FPs\n",
    "                    if extracted not in not_discovered_golds:\n",
    "                        keyword_metrics[extracted][\"FP\"] += 1\n",
    "                        keyword_metrics[extracted][\"FPs\"].append(doc_name)\n",
    "\n",
    "                    # 2) CHECK FOR TPs\n",
    "                    else:\n",
    "                        keyword_metrics[extracted][\"TP\"] += 1\n",
    "                        keyword_metrics[extracted][\"TPs\"].append(doc_name)\n",
    "                        not_discovered_golds.remove(extracted)\n",
    "\n",
    "                # 3) FILL FNs (FPs from list not_discovered_xor_golds got removed during previous loop)\n",
    "                for not_extracted in not_discovered_golds:\n",
    "\n",
    "                    # setup keyword dict in case it was not observed yet\n",
    "                    if not_extracted not in keyword_metrics:\n",
    "                        keyword_metrics[not_extracted] = copy.deepcopy(empty_stats_dict)\n",
    "\n",
    "                    keyword_metrics[not_extracted][\"FN\"] += 1\n",
    "                    keyword_metrics[not_extracted][\"FNs\"].append(doc_name)\n",
    "    return keyword_metrics\n",
    "\n",
    "\n",
    "def print_keyword_dict(keyword_metrics, first_x=None, order_by=None):\n",
    "    \n",
    "    if order_by:\n",
    "        keyword_metrics = collections.OrderedDict(sorted(keyword_metrics.items(), key=lambda kv_pair: kv_pair[1][order_by],\n",
    "                                                         reverse=True))\n",
    "    for keyword, metrics in keyword_metrics.items():\n",
    "        print(f\"{keyword} \".ljust(15), end='')\n",
    "        for k, v in metrics.items():\n",
    "            if len(k) == 3 and k.endswith(\"s\"):\n",
    "                if first_x:\n",
    "                    print(f\"{k}: \", end='')\n",
    "                    print(v[:first_x], \"...\", end='')\n",
    "                    print(\", \", end='')\n",
    "            else:\n",
    "                print(f\"{k}: \", end='')\n",
    "                print(v, end='')\n",
    "                print(\", \", end='')\n",
    "        print()\n",
    "\n",
    "\n",
    "gold_xor_keyword_metrics = analyze_keyword_stats(token_cls_goldstandard, keywords_gold_token_cls, XOR_GATEWAY)\n",
    "gold_and_keyword_metrics = analyze_keyword_stats(token_cls_goldstandard, keywords_gold_token_cls, AND_GATEWAY)\n",
    "literature_xor_keyword_metrics = analyze_keyword_stats(token_cls_goldstandard, keywords_literature_token_cls, XOR_GATEWAY)\n",
    "literature_and_keyword_metrics = analyze_keyword_stats(token_cls_goldstandard, keywords_literature_token_cls, AND_GATEWAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4a042",
   "metadata": {},
   "source": [
    "#### EXCLUSIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "17cc7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for            TP: 6, FP: 63, FN: 0, TPs: ['doc-2.1', 'doc-2.1'] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "or             TP: 20, FP: 44, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "if             TP: 61, FP: 6, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1', 'doc-10.10'] ..., FNs: [] ..., \n",
      "should         TP: 2, FP: 6, FN: 0, TPs: ['doc-6.1', 'doc-6.1'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "either         TP: 1, FP: 4, FN: 0, TPs: ['doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.1'] ..., FNs: [] ..., \n",
      "in             TP: 9, FP: 3, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "case           TP: 12, FP: 3, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "of             TP: 5, FP: 3, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "otherwise      TP: 12, FP: 1, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1'] ..., FNs: [] ..., \n",
      "whereas        TP: 1, FP: 1, FN: 0, TPs: ['doc-9.5'] ..., FPs: ['doc-1.2'] ..., FNs: [] ..., \n",
      "sometimes      TP: 4, FP: 1, FN: 0, TPs: ['doc-6.4', 'doc-6.4'] ..., FPs: ['doc-8.3'] ..., FNs: [] ..., \n",
      "the            TP: 4, FP: 0, FN: 0, TPs: ['doc-2.2', 'doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "each           TP: 1, FP: 0, FN: 0, TPs: ['doc-4.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "patient        TP: 1, FP: 0, FN: 0, TPs: ['doc-4.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "which          TP: 1, FP: 0, FN: 0, TPs: ['doc-4.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "it             TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "can            TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "also           TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "happen         TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "that           TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "under          TP: 1, FP: 0, FN: 0, TPs: ['doc-8.3'] ..., FPs: [] ..., FNs: [] ..., \n",
      "certain        TP: 1, FP: 0, FN: 0, TPs: ['doc-8.3'] ..., FPs: [] ..., FNs: [] ..., \n",
      "circumstances  TP: 1, FP: 0, FN: 0, TPs: ['doc-8.3'] ..., FPs: [] ..., FNs: [] ..., \n"
     ]
    }
   ],
   "source": [
    "# GOLD keywords\n",
    "print_keyword_dict(gold_xor_keyword_metrics, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c5490941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or             TP: 20, FP: 44, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "when           TP: 0, FP: 15, FN: 0, TPs: [] ..., FPs: ['doc-3.6', 'doc-6.2'] ..., FNs: [] ..., \n",
      "whether        TP: 0, FP: 13, FN: 0, TPs: [] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "if             TP: 61, FP: 6, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1', 'doc-10.10'] ..., FNs: [] ..., \n",
      "either         TP: 1, FP: 4, FN: 0, TPs: ['doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.1'] ..., FNs: [] ..., \n",
      "in             TP: 9, FP: 2, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-6.4'] ..., FNs: [] ..., \n",
      "case           TP: 9, FP: 2, FN: 3, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-6.4'] ..., FNs: ['doc-2.1', 'doc-2.1'] ..., \n",
      "of             TP: 5, FP: 2, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-6.4'] ..., FNs: [] ..., \n",
      "only           TP: 0, FP: 2, FN: 0, TPs: [] ..., FPs: ['doc-2.2', 'doc-4.1'] ..., FNs: [] ..., \n",
      "not            TP: 0, FP: 1, FN: 0, TPs: [] ..., FPs: ['doc-3.6'] ..., FNs: [] ..., \n",
      "otherwise      TP: 12, FP: 1, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1'] ..., FNs: [] ..., \n",
      "unless         TP: 0, FP: 1, FN: 0, TPs: [] ..., FPs: ['doc-1.4'] ..., FNs: [] ..., \n",
      "until          TP: 0, FP: 1, FN: 0, TPs: [] ..., FPs: ['doc-6.1'] ..., FNs: [] ..., \n",
      "the            TP: 0, FP: 0, FN: 4, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.2', 'doc-2.1'] ..., \n",
      "for            TP: 0, FP: 0, FN: 6, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1', 'doc-2.1'] ..., \n",
      "whereas        TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-9.5'] ..., \n",
      "should         TP: 0, FP: 0, FN: 2, TPs: [] ..., FPs: [] ..., FNs: ['doc-6.1', 'doc-6.1'] ..., \n",
      "each           TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-4.1'] ..., \n",
      "patient        TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-4.1'] ..., \n",
      "which          TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-4.1'] ..., \n",
      "sometimes      TP: 0, FP: 0, FN: 4, TPs: [] ..., FPs: [] ..., FNs: ['doc-6.4', 'doc-6.4'] ..., \n",
      "it             TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "can            TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "also           TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "happen         TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "that           TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "under          TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.3'] ..., \n",
      "certain        TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.3'] ..., \n",
      "circumstances  TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.3'] ..., \n"
     ]
    }
   ],
   "source": [
    "# LITERATURE keywords\n",
    "print_keyword_dict(literature_xor_keyword_metrics, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd02af",
   "metadata": {},
   "source": [
    "#### PARALLEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "765ba40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereas        TP: 1, FP: 1, FN: 0, TPs: ['doc-1.2'] ..., FPs: ['doc-9.5'] ..., FNs: [] ..., \n",
      "at             TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "the            TP: 2, FP: 0, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "same           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "time           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "in             TP: 1, FP: 0, FN: 0, TPs: ['doc-1.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "meantime       TP: 3, FP: 0, FN: 0, TPs: ['doc-1.1', 'doc-3.5'] ..., FPs: [] ..., FNs: [] ..., \n",
      "two            TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "concurrent     TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "activities     TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "are            TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "triggered      TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "while          TP: 2, FP: 0, FN: 0, TPs: ['doc-1.4', 'doc-1.3'] ..., FPs: [] ..., FNs: [] ..., \n"
     ]
    }
   ],
   "source": [
    "# GOLD keywords\n",
    "print_keyword_dict(gold_and_keyword_metrics, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d529bf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in             TP: 1, FP: 2, FN: 0, TPs: ['doc-1.1'] ..., FPs: ['doc-1.4', 'doc-1.4'] ..., FNs: [] ..., \n",
      "addition       TP: 0, FP: 2, FN: 0, TPs: [] ..., FPs: ['doc-1.4', 'doc-1.4'] ..., FNs: [] ..., \n",
      "to             TP: 0, FP: 2, FN: 0, TPs: [] ..., FPs: ['doc-1.4', 'doc-1.4'] ..., FNs: [] ..., \n",
      "whereas        TP: 1, FP: 1, FN: 0, TPs: ['doc-1.2'] ..., FPs: ['doc-9.5'] ..., FNs: [] ..., \n",
      "at             TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "the            TP: 2, FP: 0, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "same           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "time           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "meantime       TP: 3, FP: 0, FN: 0, TPs: ['doc-1.1', 'doc-3.5'] ..., FPs: [] ..., FNs: [] ..., \n",
      "two            TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "concurrent     TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "activities     TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "are            TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "triggered      TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "while          TP: 2, FP: 0, FN: 0, TPs: ['doc-1.4', 'doc-1.3'] ..., FPs: [] ..., FNs: [] ..., \n"
     ]
    }
   ],
   "source": [
    "# LITERATURE keywords\n",
    "print_keyword_dict(literature_and_keyword_metrics, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c725705",
   "metadata": {},
   "source": [
    "### d) Combined analysis (counts & FPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "12e18e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# available stats data:\n",
    "goldstandard_xor_gateway_stats\n",
    "goldstandard_and_gateway_stats\n",
    "gold_xor_gateway_stats\n",
    "gold_and_gateway_stats\n",
    "literature_xor_gateway_stats\n",
    "literature_and_gateway_stats\n",
    "\n",
    "# available metrics data\n",
    "gold_xor_keyword_metrics\n",
    "gold_and_keyword_metrics\n",
    "literature_xor_keyword_metrics\n",
    "literature_and_keyword_metrics\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d4c811f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_stats_metrics_dict(metrics, stats):\n",
    "    merged = {}\n",
    "    for gateway_token, gateway_token_stats in stats.items():\n",
    "        gateway_token = gateway_token.lower()\n",
    "        if gateway_token in metrics:\n",
    "            merged[gateway_token] = {**gateway_token_stats, **metrics[gateway_token]}\n",
    "        else:\n",
    "            merged[gateway_token] = gateway_token_stats\n",
    "    return merged\n",
    "\n",
    "gold_xor_merged = merge_stats_metrics_dict(gold_xor_gateway_stats, gold_xor_keyword_metrics)\n",
    "gold_and_merged = merge_stats_metrics_dict(gold_and_gateway_stats, gold_and_keyword_metrics)\n",
    "literature_xor_merged = merge_stats_metrics_dict(literature_xor_gateway_stats, literature_xor_keyword_metrics)\n",
    "literature_and_merged = merge_stats_metrics_dict(literature_and_gateway_stats, literature_and_keyword_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5f155410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for            TP: 6, FP: 63, FN: 0, count: 59, docs: 23, doc_list: {'doc-8.2', 'doc-10.14', 'doc-8.3', 'doc-1.4', 'doc-6.1', 'doc-3.6', 'doc-9.3', 'doc-6.3', 'doc-2.2', 'doc-6.4', 'doc-9.4', 'doc-5.3', 'doc-10.5', 'doc-5.1', 'doc-1.3', 'doc-9.5', 'doc-3.2', 'doc-9.2', 'doc-2.1', 'doc-1.1', 'doc-3.5', 'doc-5.4', 'doc-4.1'}, \n",
      "or             TP: 20, FP: 44, FN: 0, count: 64, docs: 29, doc_list: {'doc-10.14', 'doc-10.2', 'doc-6.1', 'doc-5.2', 'doc-10.12', 'doc-3.6', 'doc-10.3', 'doc-3.8', 'doc-6.3', 'doc-10.6', 'doc-10.11', 'doc-2.2', 'doc-6.4', 'doc-10.8', 'doc-10.7', 'doc-5.3', 'doc-5.1', 'doc-10.10', 'doc-10.1', 'doc-10.4', 'doc-1.3', 'doc-9.5', 'doc-10.13', 'doc-9.2', 'doc-3.3', 'doc-2.1', 'doc-1.1', 'doc-5.4', 'doc-10.9'}, \n",
      "if             TP: 61, FP: 6, FN: 0, count: 12, docs: 10, doc_list: {'doc-1.3', 'doc-9.5', 'doc-10.2', 'doc-6.3', 'doc-6.4', 'doc-2.1', 'doc-5.4', 'doc-6.1', 'doc-9.1', 'doc-4.1'}, \n",
      "should         TP: 2, FP: 6, FN: 0, count: 6, docs: 3, doc_list: {'doc-9.1', 'doc-2.2', 'doc-2.1'}, \n",
      "either         TP: 1, FP: 4, FN: 0, count: 4, docs: 3, doc_list: {'doc-2.1', 'doc-2.2', 'doc-6.4'}, \n",
      "in             TP: 9, FP: 3, FN: 0, count: 3, docs: 2, doc_list: {'doc-2.2', 'doc-6.4'}, \n",
      "case           TP: 12, FP: 3, FN: 0, count: 15, docs: 4, doc_list: {'doc-2.1', 'doc-9.5', 'doc-2.2', 'doc-6.4'}, \n",
      "of             TP: 5, FP: 3, FN: 0, count: 8, docs: 3, doc_list: {'doc-9.5', 'doc-2.2', 'doc-6.4'}, \n",
      "otherwise      TP: 12, FP: 1, FN: 0, count: 7, docs: 5, doc_list: {'doc-3.2', 'doc-1.2', 'doc-8.3', 'doc-2.1', 'doc-9.1'}, \n",
      "whereas        TP: 1, FP: 1, FN: 0, count: 2, docs: 2, doc_list: {'doc-9.5', 'doc-1.2'}, \n",
      "sometimes      TP: 4, FP: 1, FN: 0, count: 2, docs: 1, doc_list: {'doc-6.4'}, \n",
      "the            TP: 4, FP: 0, FN: 0, count: 4, docs: 2, doc_list: {'doc-2.2', 'doc-2.1'}, \n",
      "each           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, doc_list: {'doc-4.1'}, \n",
      "patient        TP: 1, FP: 0, FN: 0, count: 1, docs: 1, doc_list: {'doc-4.1'}, \n",
      "which          TP: 1, FP: 0, FN: 0, count: 1, docs: 1, doc_list: {'doc-4.1'}, \n",
      "it             TP: 1, FP: 0, FN: 0, count: 1, docs: 1, doc_list: {'doc-8.2'}, \n",
      "can            TP: 1, FP: 0, FN: 0, count: 1, docs: 1, doc_list: {'doc-8.2'}, \n",
      "also           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, doc_list: {'doc-8.2'}, \n",
      "happen         TP: 1, FP: 0, FN: 0, count: 1, docs: 1, doc_list: {'doc-8.2'}, \n",
      "that           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, doc_list: {'doc-8.2'}, \n",
      "under          TP: 1, FP: 0, FN: 0, \n",
      "certain        TP: 1, FP: 0, FN: 0, count: 1, docs: 1, doc_list: {'doc-8.3'}, \n",
      "circumstances  TP: 1, FP: 0, FN: 0, count: 1, docs: 1, doc_list: {'doc-8.3'}, \n"
     ]
    }
   ],
   "source": [
    "print_keyword_dict(gold_xor_merged, first_x=0, order_by='FP')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
