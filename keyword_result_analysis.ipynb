{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa3f18d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PetReader:Load RelationsExtraction dataset ...\n",
      "WARNING:datasets.builder:Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\relations-extraction\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02296757698059082,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5e08d27ef84e3e997cd83bda2a1a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PetReader:Load TokenClassification dataset ...\n",
      "WARNING:datasets.builder:Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\token-classification\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017894268035888672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5516d8be55074504a96e6e1c214d6b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import collections\n",
    "\n",
    "from utils import read_json_to_dict\n",
    "from PetReader import pet_reader\n",
    "from petreader.labels import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe531b99",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bc5da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cls_goldstandard = read_json_to_dict(\"data/other/token_goldstandard.json\")\n",
    "\n",
    "keywords_gold_token_cls_results = read_json_to_dict(\"data/results/key_words_gold/results-token-classification.json\")\n",
    "keywords_gold_token_cls = read_json_to_dict(\"data/results/key_words_gold/token-classification.json\")\n",
    "\n",
    "keywords_literature_token_cls_results = read_json_to_dict(\"data/results/key_words_literature/results-token-classification.json\")\n",
    "keywords_literature_token_cls = read_json_to_dict(\"data/results/key_words_literature/token-classification.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adab351",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b8ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'otherwise']\n"
     ]
    }
   ],
   "source": [
    "from petreader.TokenClassification import TokenClassification\n",
    "from petbenchmarks.tokenclassification import TokenClassificationBenchmark\n",
    "benchmark = TokenClassificationBenchmark(pet_reader.token_dataset)\n",
    "\n",
    "gold = benchmark.GetGoldStandard()\n",
    "print(gold['doc-3.2'][XOR_GATEWAY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6acf8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [['If'], ['otherwise']], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(pet_reader.token_dataset.GetXORGateways('doc-3.2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151505fd",
   "metadata": {},
   "source": [
    "## 1) Analyze Gold Keyword Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88743a4",
   "metadata": {},
   "source": [
    "### a) check for documents with recall != 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c907397",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_name, doc_dict in keywords_gold_token_cls_results.items():\n",
    "    if doc_name.startswith('doc'):\n",
    "        if doc_dict[XOR_GATEWAY][RECALL] != 1 and doc_dict[XOR_GATEWAY][SUPPORT] != 0:\n",
    "            print(doc_name.center(100, '-'))\n",
    "            print(\"--Text--\")\n",
    "            for i, line in enumerate(pet_reader.get_doc_sentences(doc_name)):\n",
    "                print(i, ' '.join(line))\n",
    "            print()\n",
    "            print(\"--Results--:\", doc_dict[XOR_GATEWAY])\n",
    "            print()\n",
    "            print(\"--Extracted--:\", keywords_gold_token_cls[doc_name][XOR_GATEWAY])\n",
    "            print()\n",
    "            print(\"--Gold standard--:\", token_cls_goldstandard[doc_name][XOR_GATEWAY])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c233e6b",
   "metadata": {},
   "source": [
    "### b) Analyze False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7d802d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_keyword_stats(gold_standard, extracted_tokens, gateway_type: str, doc_names=[]):\n",
    "    keyword_stats = {}\n",
    "    empty_stats_dict = {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TPs\": [], \"FPs\": [], \"FNs\": []}\n",
    "\n",
    "    for doc_name, doc_dict in keywords_gold_token_cls_results.items():\n",
    "        \n",
    "        if doc_name.startswith('doc'):  # result dict contains as well keys for statistics            \n",
    "            if not doc_names or doc_name in doc_names:  # filter for (optionally) passed list\n",
    "\n",
    "                # remove gateways step by step if they were identified; left overs are false positives\n",
    "                not_discovered_golds = [g.lower() for g in gold_standard[doc_name][gateway_type].copy()]\n",
    "\n",
    "                for extracted in extracted_tokens[doc_name][gateway_type]:\n",
    "                    extracted = extracted.lower()\n",
    "\n",
    "                    # setup keyword dict in case it was not observed yet\n",
    "                    if extracted not in keyword_stats:\n",
    "                        keyword_stats[extracted] = copy.deepcopy(empty_stats_dict)\n",
    "\n",
    "                    # 1) CHECK FOR FPs\n",
    "                    if extracted not in not_discovered_golds:\n",
    "                        keyword_stats[extracted][\"FP\"] += 1\n",
    "                        keyword_stats[extracted][\"FPs\"].append(doc_name)\n",
    "\n",
    "                    # 2) CHECK FOR TPs\n",
    "                    else:\n",
    "                        keyword_stats[extracted][\"TP\"] += 1\n",
    "                        keyword_stats[extracted][\"TPs\"].append(doc_name)\n",
    "                        not_discovered_golds.remove(extracted)\n",
    "\n",
    "                # 3) FILL FNs (FPs from list not_discovered_xor_golds got removed during previous loop)\n",
    "                for not_extracted in not_discovered_golds:\n",
    "\n",
    "                    # setup keyword dict in case it was not observed yet\n",
    "                    if not_extracted not in keyword_stats:\n",
    "                        keyword_stats[not_extracted] = copy.deepcopy(empty_stats_dict)\n",
    "\n",
    "                    keyword_stats[not_extracted][\"FN\"] += 1\n",
    "                    keyword_stats[not_extracted][\"FNs\"].append(doc_name)\n",
    "    return keyword_stats\n",
    "\n",
    "\n",
    "def print_stats(keyword_stats, first_x=None, order_by=None):\n",
    "    \n",
    "    if order_by:\n",
    "        keyword_stats = collections.OrderedDict(sorted(keyword_stats.items(), key=lambda kv_pair: kv_pair[1][order_by],\n",
    "                                                      reverse=True))\n",
    "    for keyword, stats in keyword_stats.items():\n",
    "        print(f\"{keyword} \".ljust(15), end='')\n",
    "        for k, v in stats.items():\n",
    "            print(f\"{k}: \", end='')\n",
    "            if k.endswith(\"s\") and first_x:\n",
    "                print(v[:first_x], \"...\", end='')\n",
    "            else:\n",
    "                print(v, end='')\n",
    "            print(\", \", end='')\n",
    "        print()\n",
    "\n",
    "\n",
    "gold_xor_keyword_stats = analyze_keyword_stats(token_cls_goldstandard, keywords_gold_token_cls, XOR_GATEWAY)\n",
    "gold_and_keyword_stats = analyze_keyword_stats(token_cls_goldstandard, keywords_gold_token_cls, AND_GATEWAY)\n",
    "literature_xor_keyword_stats = analyze_keyword_stats(token_cls_goldstandard, keywords_literature_token_cls, XOR_GATEWAY)\n",
    "literature_and_keyword_stats = analyze_keyword_stats(token_cls_goldstandard, keywords_literature_token_cls, AND_GATEWAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000325",
   "metadata": {},
   "source": [
    "#### EXCLUSIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebcb5e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for            TP: 6, FP: 63, FN: 0, TPs: ['doc-2.1', 'doc-2.1'] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "or             TP: 20, FP: 44, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "if             TP: 61, FP: 6, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1', 'doc-10.10'] ..., FNs: [] ..., \n",
      "should         TP: 2, FP: 6, FN: 0, TPs: ['doc-6.1', 'doc-6.1'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "either         TP: 1, FP: 4, FN: 0, TPs: ['doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.1'] ..., FNs: [] ..., \n",
      "in             TP: 9, FP: 3, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "case           TP: 12, FP: 3, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "of             TP: 5, FP: 3, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.2'] ..., FNs: [] ..., \n",
      "otherwise      TP: 12, FP: 1, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1'] ..., FNs: [] ..., \n",
      "whereas        TP: 1, FP: 1, FN: 0, TPs: ['doc-9.5'] ..., FPs: ['doc-1.2'] ..., FNs: [] ..., \n",
      "sometimes      TP: 4, FP: 1, FN: 0, TPs: ['doc-6.4', 'doc-6.4'] ..., FPs: ['doc-8.3'] ..., FNs: [] ..., \n",
      "the            TP: 4, FP: 0, FN: 0, TPs: ['doc-2.2', 'doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "each           TP: 1, FP: 0, FN: 0, TPs: ['doc-4.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "patient        TP: 1, FP: 0, FN: 0, TPs: ['doc-4.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "which          TP: 1, FP: 0, FN: 0, TPs: ['doc-4.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "it             TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "can            TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "also           TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "happen         TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "that           TP: 1, FP: 0, FN: 0, TPs: ['doc-8.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "under          TP: 1, FP: 0, FN: 0, TPs: ['doc-8.3'] ..., FPs: [] ..., FNs: [] ..., \n",
      "certain        TP: 1, FP: 0, FN: 0, TPs: ['doc-8.3'] ..., FPs: [] ..., FNs: [] ..., \n",
      "circumstances  TP: 1, FP: 0, FN: 0, TPs: ['doc-8.3'] ..., FPs: [] ..., FNs: [] ..., \n"
     ]
    }
   ],
   "source": [
    "# GOLD keywords\n",
    "print_stats(gold_xor_keyword_stats, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5667b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or             TP: 20, FP: 44, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "when           TP: 0, FP: 15, FN: 0, TPs: [] ..., FPs: ['doc-3.6', 'doc-6.2'] ..., FNs: [] ..., \n",
      "whether        TP: 0, FP: 13, FN: 0, TPs: [] ..., FPs: ['doc-3.6', 'doc-2.2'] ..., FNs: [] ..., \n",
      "if             TP: 61, FP: 6, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1', 'doc-10.10'] ..., FNs: [] ..., \n",
      "either         TP: 1, FP: 4, FN: 0, TPs: ['doc-2.2'] ..., FPs: ['doc-2.2', 'doc-2.1'] ..., FNs: [] ..., \n",
      "in             TP: 9, FP: 2, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-6.4'] ..., FNs: [] ..., \n",
      "case           TP: 9, FP: 2, FN: 3, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-6.4'] ..., FNs: ['doc-2.1', 'doc-2.1'] ..., \n",
      "of             TP: 5, FP: 2, FN: 0, TPs: ['doc-2.2', 'doc-2.2'] ..., FPs: ['doc-2.2', 'doc-6.4'] ..., FNs: [] ..., \n",
      "only           TP: 0, FP: 2, FN: 0, TPs: [] ..., FPs: ['doc-2.2', 'doc-4.1'] ..., FNs: [] ..., \n",
      "not            TP: 0, FP: 1, FN: 0, TPs: [] ..., FPs: ['doc-3.6'] ..., FNs: [] ..., \n",
      "otherwise      TP: 12, FP: 1, FN: 0, TPs: ['doc-3.6', 'doc-3.6'] ..., FPs: ['doc-2.1'] ..., FNs: [] ..., \n",
      "unless         TP: 0, FP: 1, FN: 0, TPs: [] ..., FPs: ['doc-1.4'] ..., FNs: [] ..., \n",
      "until          TP: 0, FP: 1, FN: 0, TPs: [] ..., FPs: ['doc-6.1'] ..., FNs: [] ..., \n",
      "the            TP: 0, FP: 0, FN: 4, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.2', 'doc-2.1'] ..., \n",
      "for            TP: 0, FP: 0, FN: 6, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1', 'doc-2.1'] ..., \n",
      "whereas        TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-9.5'] ..., \n",
      "should         TP: 0, FP: 0, FN: 2, TPs: [] ..., FPs: [] ..., FNs: ['doc-6.1', 'doc-6.1'] ..., \n",
      "each           TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-4.1'] ..., \n",
      "patient        TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-4.1'] ..., \n",
      "which          TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-4.1'] ..., \n",
      "sometimes      TP: 0, FP: 0, FN: 4, TPs: [] ..., FPs: [] ..., FNs: ['doc-6.4', 'doc-6.4'] ..., \n",
      "it             TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "can            TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "also           TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "happen         TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "that           TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.2'] ..., \n",
      "under          TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.3'] ..., \n",
      "certain        TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.3'] ..., \n",
      "circumstances  TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-8.3'] ..., \n"
     ]
    }
   ],
   "source": [
    "# LITERATURE keywords\n",
    "print_stats(literature_xor_keyword_stats, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c20bf1f",
   "metadata": {},
   "source": [
    "#### PARALLEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fecd3cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereas        TP: 1, FP: 1, FN: 0, TPs: ['doc-1.2'] ..., FPs: ['doc-9.5'] ..., FNs: [] ..., \n",
      "at             TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "the            TP: 2, FP: 0, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "same           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "time           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "in             TP: 1, FP: 0, FN: 0, TPs: ['doc-1.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "meantime       TP: 3, FP: 0, FN: 0, TPs: ['doc-1.1', 'doc-3.5'] ..., FPs: [] ..., FNs: [] ..., \n",
      "two            TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "concurrent     TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "activities     TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "are            TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "triggered      TP: 1, FP: 0, FN: 0, TPs: ['doc-2.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "while          TP: 2, FP: 0, FN: 0, TPs: ['doc-1.4', 'doc-1.3'] ..., FPs: [] ..., FNs: [] ..., \n"
     ]
    }
   ],
   "source": [
    "# GOLD keywords\n",
    "print_stats(gold_and_keyword_stats, first_x=2, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9c78164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in             TP: 1, FP: 2, FN: 0, TPs: ['doc-1.1'] ..., FPs: ['doc-1.4', 'doc-1.4'] ..., FNs: [] ..., \n",
      "addition       TP: 0, FP: 2, FN: 0, TPs: [] ..., FPs: ['doc-1.4', 'doc-1.4'] ..., FNs: [] ..., \n",
      "to             TP: 0, FP: 2, FN: 0, TPs: [] ..., FPs: ['doc-1.4', 'doc-1.4'] ..., FNs: [] ..., \n",
      "whereas        TP: 1, FP: 1, FN: 0, TPs: ['doc-1.2'] ..., FPs: ['doc-9.5'] ..., FNs: [] ..., \n",
      "at             TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "the            TP: 2, FP: 0, FN: 0, TPs: ['doc-2.2', 'doc-1.1'] ..., FPs: [] ..., FNs: [] ..., \n",
      "same           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "time           TP: 1, FP: 0, FN: 0, TPs: ['doc-2.2'] ..., FPs: [] ..., FNs: [] ..., \n",
      "meantime       TP: 3, FP: 0, FN: 0, TPs: ['doc-1.1', 'doc-3.5'] ..., FPs: [] ..., FNs: [] ..., \n",
      "two            TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "concurrent     TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "activities     TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "are            TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "triggered      TP: 0, FP: 0, FN: 1, TPs: [] ..., FPs: [] ..., FNs: ['doc-2.1'] ..., \n",
      "while          TP: 2, FP: 0, FN: 0, TPs: ['doc-1.4', 'doc-1.3'] ..., FPs: [] ..., FNs: [] ..., \n"
     ]
    }
   ],
   "source": [
    "# LITERATURE keywords\n",
    "print_stats(literature_and_keyword_stats, first_x=2, order_by=\"FP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
