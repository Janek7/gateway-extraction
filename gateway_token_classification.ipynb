{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a455458b",
   "metadata": {},
   "source": [
    "# TF Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6314563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\relations-extraction\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.08182048797607422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96932b00f4fb410ba7f943d2dab296ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\token-classification\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.049878597259521484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42702f479e814b1b843c64609936ea3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PetReader import pet_reader\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons  as tfa\n",
    "import transformers\n",
    "from petreader.labels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e71018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2510505",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e139a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d13193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417, 64)\n",
      "(417, 64)\n"
     ]
    }
   ],
   "source": [
    "sample_numbers = pet_reader.token_dataset.GetRandomizedSampleNumbers()\n",
    "sample_dicts = [pet_reader.token_dataset.GetSampleDictWithNerLabels(sample_number) for sample_number in sample_numbers]\n",
    "sample_sentences = [sample_dict['tokens'] for sample_dict in sample_dicts]\n",
    "\n",
    "# 1) transform tokens tags into IDs classification\n",
    "all_tokens = tokenizer(sample_sentences, is_split_into_words=True, padding=True, return_tensors='tf')\n",
    "max_sentence_length = all_tokens['input_ids'].shape[1]\n",
    "\n",
    "# 2) transform NER token tags into labels for classification\n",
    "all_sample_labels = []\n",
    "for i, sample_number in enumerate(sample_numbers):\n",
    "    sample_dict = pet_reader.token_dataset.GetSampleDictWithNerLabels(sample_number)\n",
    "    # transformer_tokens = tokenizer.convert_ids_to_tokens(tokens['input_ids'][i])\n",
    "    # tokenize again every single sample to get access to .word_ids()\n",
    "    tokenization = tokenizer(sample_dict['tokens'], is_split_into_words=True, \n",
    "                             padding='max_length', max_length=max_sentence_length, return_tensors='tf')\n",
    "    sample_tokens = tokenizer.convert_ids_to_tokens(tokenization['input_ids'][0])\n",
    "    \n",
    "    sample_labels = []\n",
    "    # word index necessary, because one token in PET could be splitted into multiple tokens with tokenizer\n",
    "    # multiple tokens have all the same word_id -> allows retrieval of the same one NER label from PET tokens\n",
    "    for token, word_index in zip(sample_tokens, tokenization.word_ids()):\n",
    "        # set special class for special tokens\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            sample_labels.append(0)\n",
    "        else:\n",
    "            token_tag = sample_dict['ner-tags'][word_index]\n",
    "            if token_tag.endswith(XOR_GATEWAY):\n",
    "                sample_labels.append(2)\n",
    "            elif token_tag.endswith(AND_GATEWAY):\n",
    "                sample_labels.append(3)\n",
    "            else:\n",
    "                sample_labels.append(1)\n",
    "                \n",
    "    all_sample_labels.append(sample_labels)\n",
    "\n",
    "# print(all_sample_labels)\n",
    "labels = tf.constant(all_sample_labels)\n",
    "tokens = all_tokens\n",
    "print(labels.shape)\n",
    "print(tokens['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63596ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417, 64)\n",
      "(417, 64)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(tokens['input_ids'].shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokens['input_ids'], 'attention_mask': tokens['input_ids']}, \n",
    "                                              labels))\n",
    "# dataset = dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeef604e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 / 42\n",
      "Batched: 47 / 6\n"
     ]
    }
   ],
   "source": [
    "# split up\n",
    "val_share = 0.1\n",
    "val_instances = round(tokens['input_ids'].shape[0] * val_share)\n",
    "dev_dataset = dataset.take(val_instances) \n",
    "train_dataset = dataset.skip(val_instances) \n",
    "print(f\"{len(train_dataset)} / {len(dev_dataset)}\")\n",
    "\n",
    "# batch both\n",
    "dev_dataset = dev_dataset.batch(8)\n",
    "train_dataset = train_dataset.batch(8)\n",
    "print(f\"Batched: {len(train_dataset)} / {len(dev_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbc0db",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e507b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForTokenClassification: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "token_cls_model = transformers.TFAutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c34c2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForTokenClassification"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token_cls_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a90cd",
   "metadata": {},
   "source": [
    "### a) including Model in native Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff25a06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_token_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3076      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,365,956\n",
      "Trainable params: 66,365,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1156, in train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 459, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\metrics.py\", line 178, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\metrics.py\", line 1403, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 619, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 64, 4) and (None, 64) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m token_cls_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtoken_cls_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1156, in train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 459, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\metrics.py\", line 178, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\metrics.py\", line 1403, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 619, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 64, 4) and (None, 64) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# setting up training parameters\n",
    "batch_size = 8\n",
    "num_train_epochs = 3\n",
    "num_train_steps = (len(train_dataset) // 8) * num_train_epochs\n",
    "optimizer, lr_schedule = transformers.create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")\n",
    "\n",
    "# compile model\n",
    "token_cls_model.compile(optimizer=optimizer,\n",
    "                        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                        metrics=[tf.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "#                                  tf.metrics.Precision(name=\"precision\"),\n",
    "#                                  tf.metrics.Recall(name=\"recall\"),\n",
    "#                                  tfa.metrics.F1Score(4, name=\"f1\")\n",
    "                                ])\n",
    "token_cls_model.summary()\n",
    "\n",
    "# train the model\n",
    "token_cls_model.fit(train_dataset, \n",
    "                      validation_data=dev_dataset, \n",
    "                      epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38957962",
   "metadata": {},
   "source": [
    "### b) with transformers Trainer (NOT WORKING WITH TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a8a14d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\trainer_tf.py:115: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Method `n_gpu` requires PyTorch.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 42\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     32\u001b[0m     }\n\u001b[0;32m     34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTFTrainer(\n\u001b[0;32m     35\u001b[0m     model,\n\u001b[0;32m     36\u001b[0m     args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     40\u001b[0m )\n\u001b[1;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\trainer_tf.py:477\u001b[0m, in \u001b[0;36mTFTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m    Train method to train the model.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m     train_ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_tfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m    480\u001b[0m         tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mtrace_on(graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, profiler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\trainer_tf.py:155\u001b[0m, in \u001b[0;36mTFTrainer.get_train_tfdataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: training requires a train_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39mcardinality()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_examples \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\training_args.py:1234\u001b[0m, in \u001b[0;36mTrainingArguments.train_batch_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing deprecated `--per_gpu_train_batch_size` argument which will be removed in a future \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion. Using `--per_device_train_batch_size` is preferred.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1232\u001b[0m     )\n\u001b[0;32m   1233\u001b[0m per_device_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mper_gpu_train_batch_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mper_device_train_batch_size\n\u001b[1;32m-> 1234\u001b[0m train_batch_size \u001b[38;5;241m=\u001b[39m per_device_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_gpu\u001b[49m)\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_batch_size\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\transformers\\utils\\import_utils.py:831\u001b[0m, in \u001b[0;36mtorch_required.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 831\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` requires PyTorch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Method `n_gpu` requires PyTorch."
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "args = transformers.TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for (p, l) in zip(prediction, label) if l != -100] \n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "trainer = transformers.TFTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
