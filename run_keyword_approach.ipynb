{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63bb02fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\relations-extraction\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01993846893310547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68f5e356656403c9ffcaea097920c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\token-classification\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018474578857421875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e85e66e3cf345f0bf4602e10383f31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from petreader.labels import *\n",
    "from PetReader import pet_reader\n",
    "from petbenchmarks.benchmarks import BenchmarkApproach\n",
    "from petbenchmarks.tokenclassification import TokenClassificationBenchmark\n",
    "from petbenchmarks.relationsextraction import RelationsExtractionBenchmark\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from typing import List, Tuple, Optional, Dict, Union\n",
    "from labels import *\n",
    "from utils import format_json_file\n",
    "\n",
    "logger = logging.getLogger('keyword approach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973d9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KeywordApproach:\n",
    "\n",
    "    def __init__(self, approach_name: str = None, keywords: str = LITERATURE, same_xor_gateway_threshold: int = 1,\n",
    "                 output_format: str = BENCHMARK):\n",
    "        \"\"\"\n",
    "        creates new instance of the basic keyword approach\n",
    "        :param approach_name: description of approach to use in result folder name; if not set use key word variant\n",
    "        :param keywords: flag/variant which keywords to use; available: literature, gold, own\n",
    "        :param same_xor_gateway_threshold: threshold to recognize subsequent (contradictory xor) gateways as same\n",
    "        :param output_format: output format of extracted element and flows; available: benchmark, pet\n",
    "        \"\"\"\n",
    "        self.approach_name = approach_name\n",
    "        if not self.approach_name:\n",
    "            self.approach_name = f\"keywords_{keywords}\"\n",
    "        self.keywords = keywords\n",
    "        self._same_xor_gateway_threshold = same_xor_gateway_threshold\n",
    "        self.output_format = output_format\n",
    "        self._xor_keywords = None\n",
    "        self._and_keywords = None\n",
    "        self._contradictory_gateways = None\n",
    "        self._read_and_set_keywords()\n",
    "        self._read_contradictory_gateways()\n",
    "        self._processed_doc_gateway_frames = []\n",
    "        # flag if details of extractions should be logged for each document\n",
    "        # default = True; temporarily False during evaluating all documents\n",
    "        self._log_document_level_details = True\n",
    "\n",
    "        # check string parameters for valid values\n",
    "        if self.keywords not in [LITERATURE, GOLD, OWN]:\n",
    "            raise ValueError(f\"Key words must be {LITERATURE} or {GOLD}\")\n",
    "        if self.output_format not in [PET, BENCHMARK]:\n",
    "            raise ValueError(f\"Output format must be {PET} or {BENCHMARK}\")\n",
    "\n",
    "    def evaluate_documents(self, doc_names: List[str] = None, log_document_details: bool = False,\n",
    "                           evaluate_token_cls: bool = True, evaluate_relation_extraction: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        run extraction and evaluation with petbenchmarks\n",
    "        :param doc_names: list of document names to evaluate, use all as default value\n",
    "        :param log_document_details: flag if document level results of extractions should be logged\n",
    "        :param evaluate_token_cls: flag to run evaluation of token classification or not\n",
    "        :param evaluate_relation_extraction: flag to run evaluation of relation extraction or not\n",
    "        :return: nothing, results are written to .json file\n",
    "        \"\"\"\n",
    "        self._log_document_level_details = log_document_details\n",
    "        if not doc_names:\n",
    "            doc_names = pet_reader.document_names\n",
    "\n",
    "        # prepare evaluation structures to fill\n",
    "        logger.info(\"Create TokenClassificationBenchmark ...\")\n",
    "        tcb = TokenClassificationBenchmark(pet_dataset=pet_reader.token_dataset)\n",
    "        process_elements = tcb.GetEmptyPredictionsDict()\n",
    "        logger.info(\"Create RelationsExtractionBenchmark ...\")\n",
    "        reb = RelationsExtractionBenchmark(pet_dataset=pet_reader.relations_dataset)\n",
    "        relations = reb.GetEmptyPredictionsDict()\n",
    "\n",
    "        # process all documents\n",
    "        logger.info(f\"Start processing of {len(doc_names)} documents ...\")\n",
    "        for i, doc_name in enumerate(doc_names):\n",
    "            if i % 5 == 0 and i != 0:\n",
    "                logger.info(f\"Finished processing of {i} documents.\")\n",
    "            xor_gateways, and_gateways, doc_flows, same_gateway_relations = self.process_document(doc_name)\n",
    "            process_elements[doc_name][XOR_GATEWAY].extend(xor_gateways)\n",
    "            process_elements[doc_name][AND_GATEWAY].extend(and_gateways)\n",
    "            relations[doc_name][FLOW].extend(doc_flows)\n",
    "            relations[doc_name][SAME_GATEWAY].extend(same_gateway_relations)\n",
    "        logger.info(f\"Finished processing of documents\")\n",
    "\n",
    "        # save results as json\n",
    "        folder = f'data/results/{self.approach_name}/'\n",
    "        # clear directory first and then create new\n",
    "        shutil.rmtree(folder)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        process_elements_filename = os.path.join(folder, 'process_elements.json')\n",
    "        with open(process_elements_filename, 'w') as file:\n",
    "            json.dump(process_elements, file, indent=4)\n",
    "        relations_filename = os.path.join(folder, 'relations.json')\n",
    "        with open(relations_filename, 'w') as file:\n",
    "            json.dump(relations, file, indent=4)\n",
    "        logger.info(f\"Saved results to {folder}\")\n",
    "\n",
    "        # run evaluation\n",
    "        if evaluate_token_cls:\n",
    "            logger.info(f\"Run process element / token classification evaluation\")\n",
    "            BenchmarkApproach(approach_name=self.approach_name, predictions_file_or_folder=process_elements_filename,\n",
    "                              pet_dataset=pet_reader.token_dataset)\n",
    "            format_json_file(os.path.join(folder, f\"results-{self.approach_name}.json\"))\n",
    "        if evaluate_relation_extraction:\n",
    "            logger.info(f\"Run relation extraction evaluation\")\n",
    "            BenchmarkApproach(approach_name=self.approach_name, predictions_file_or_folder=relations_filename,\n",
    "                              pet_dataset=pet_reader.relations_dataset)\n",
    "            format_json_file(os.path.join(folder, f\"results-{self.approach_name}.json\"))\n",
    "\n",
    "        \"\"\"\n",
    "        Params of BenchmarkApproach\n",
    "        approach_name:  (str) the name of the approach benchmarked\n",
    "        predictions_file_or_folder: (str) path to predictions.\n",
    "                                    it can be either a file or a folder containing results files\n",
    "        output_results: (str) the path to the results file to store\n",
    "        relax_window: (int) the number of words to consider when relaxing comparison\n",
    "        strategy: (str) the strategy adopted to perform comparisong.\n",
    "                        'lr': left-right\n",
    "                        'l' : left\n",
    "                        'r' : right\n",
    "        \"\"\"\n",
    "\n",
    "        # reset to standard value\n",
    "        self._log_document_level_details = True\n",
    "\n",
    "    def process_document(self, doc_name: str) -> Tuple[List, List, List, List]:\n",
    "        \"\"\"\n",
    "        extracts and returns gateways and related flow relations for given document\n",
    "        :param doc_name: document name\n",
    "        :return: xor_gateways, and_gateways, doc_flows, same_gateway_relations\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare document\n",
    "        doc_sentences = pet_reader.get_doc_sentences(doc_name)\n",
    "        doc_activities_enriched = pet_reader.get_index_enriched_activities(doc_name)\n",
    "\n",
    "        # extract concurrent gateways and related flow relations\n",
    "        and_gateways = self._extract_gateways(doc_sentences, AND_GATEWAY)\n",
    "        and_flows = self._extract_concurrent_flows(doc_activities_enriched, and_gateways)\n",
    "\n",
    "        # extract exclusive gateways and related flow relations\n",
    "        xor_gateways = self._extract_gateways(doc_sentences, XOR_GATEWAY)\n",
    "        xor_flows, same_gateway_relations = self._extract_exclusive_flows(doc_activities_enriched, xor_gateways)\n",
    "\n",
    "        # extract flow relations of gold activities and remove the ones involved in gateway flows\n",
    "        gold_activity_flows = self._extract_gold_activity_flows(doc_activities_enriched)\n",
    "        doc_flows = self._merge_flows(gold_activity_flows, xor_flows, and_flows)\n",
    "\n",
    "        # change format of outputs to BENCHMARK if necessary\n",
    "        if self.output_format == BENCHMARK:\n",
    "            # transform gateway entities to simpler benchmark format\n",
    "            def gateways_to_benchmark(gateways):\n",
    "                results = []\n",
    "                for sentence_gateways in gateways:\n",
    "                    i = 0\n",
    "                    while i < len(sentence_gateways):\n",
    "                        gateway_tokens = [sentence_gateways[i][0]]\n",
    "                        i += 1\n",
    "                        while i < len(sentence_gateways) and sentence_gateways[i][2].startswith('I-'):\n",
    "                            gateway_tokens.append(sentence_gateways[i][0])\n",
    "                            i += 1\n",
    "                        results.append(gateway_tokens)\n",
    "                return results\n",
    "            xor_gateways = gateways_to_benchmark(xor_gateways)\n",
    "            and_gateways = gateways_to_benchmark(and_gateways)\n",
    "\n",
    "            # transform relation dictionaries to simpler benchmark format\n",
    "            relations_to_benchmark = lambda relations: [{SOURCE_ENTITY: r[SOURCE_ENTITY],\n",
    "                                                         TARGET_ENTITY: r[TARGET_ENTITY]} for r in relations]\n",
    "            doc_flows = relations_to_benchmark(doc_flows)\n",
    "            same_gateway_relations = relations_to_benchmark(same_gateway_relations)\n",
    "\n",
    "        return xor_gateways, and_gateways, doc_flows, same_gateway_relations\n",
    "\n",
    "    def _extract_gateways(self, sentence_list: List[List[str]], gateway_type: str) -> List[List[Tuple[str, int, str]]]:\n",
    "        \"\"\"\n",
    "        extracts gateways in a key-word-based manner given a document structured in a list of sentences\n",
    "        if two phrases would match to a token (e.g. 'in the meantime' and 'meantime'), the longer phrase is extracted\n",
    "        :param sentence_list: document represented as list of sentences (each sentence is a list of tokens)\n",
    "        :param gateway_type: gateway type to extract ('XOR Gateway' or 'AND Gateway')\n",
    "\n",
    "        :return: a two dimensional list -> list of tuples (word, position in sentence, tag) for each sentence this\n",
    "                 produces the same structure as sentences and their tokens and NER labels are annotated in PET dataset\n",
    "        \"\"\"\n",
    "        if gateway_type == XOR_GATEWAY:\n",
    "            key_words = self._xor_keywords\n",
    "        elif gateway_type == AND_GATEWAY:\n",
    "            key_words = self._and_keywords\n",
    "        else:\n",
    "            raise ValueError(f\"gateway_type must be {XOR_GATEWAY} or {AND_GATEWAY}\")\n",
    "        # sort key words descending by length of words in phrase\n",
    "        key_words.sort(key=lambda key_word_phrase: len(key_word_phrase.split(\" \")), reverse=True)\n",
    "\n",
    "        # 1) extract gateways\n",
    "        gateways = []\n",
    "        for s_idx, tokens in enumerate(sentence_list):\n",
    "            sentence_gateways = []\n",
    "            tokens_lower = [t.lower() for t in tokens]\n",
    "            # create sentence string to search (multi-word) key phrases\n",
    "            sentence_to_search = f\" {' '.join(tokens_lower).lower()} \"\n",
    "            tokens_already_matched_with_key_phrase = []\n",
    "\n",
    "            # iterate over key phrases\n",
    "            for key_phrase in key_words:\n",
    "                key_phrase_to_search = f\" {key_phrase} \"\n",
    "\n",
    "                # if key phrase is in sentence, search index and extract\n",
    "                if key_phrase_to_search in sentence_to_search:\n",
    "                    key_phrase_tokens = key_phrase.split(\" \")\n",
    "\n",
    "                    # check key phrase for every token\n",
    "                    for t_idx, token in enumerate(tokens_lower):\n",
    "                        candidate = True\n",
    "                        # iterate over key phrase tokens in case of multiple world phrase\n",
    "                        for key_phrase_token_idx, key_phrase_token in enumerate(key_phrase_tokens):\n",
    "                            # check if token is not part of key phrase or token is already matched with another phrase\n",
    "                            # if yes, stop processing candidate\n",
    "                            if not tokens_lower[t_idx + key_phrase_token_idx] == key_phrase_token or \\\n",
    "                                    t_idx + key_phrase_token_idx in tokens_already_matched_with_key_phrase:\n",
    "                                candidate = False\n",
    "                                break\n",
    "\n",
    "                        # add tokens to result only if all tokens are matched and not already part of a longer phrase\n",
    "                        if candidate:\n",
    "                            for i, key_phrase_token in enumerate(key_phrase_tokens):\n",
    "                                prefix = \"B\" if i == 0 else \"I\"\n",
    "                                # append tuples with extract information as in PET\n",
    "                                sentence_gateways.append((tokens[t_idx + i], t_idx + i, f\"{prefix}-{gateway_type}\"))\n",
    "                                tokens_already_matched_with_key_phrase.append(t_idx + i)\n",
    "\n",
    "            sentence_gateways.sort(key=lambda gateway_triple: gateway_triple[1])\n",
    "            gateways.append(sentence_gateways)\n",
    "\n",
    "        return gateways\n",
    "\n",
    "    def _preprocess_extracted_gateways(self, extracted_gateways: List[List[Tuple[str, int, str]]], gateway_type: str) \\\n",
    "            -> List[Dict]:\n",
    "        \"\"\"\n",
    "        flatten gateways but keep sentence index; merge multiple gateway tokens into one gateway\n",
    "        :param extracted_gateways: gateways in PET format (token, index, tag)\n",
    "        :param gateway_type: type of gateway\n",
    "        :return: flattened gateway list with entity dictionaries; for format see self._get_entity_dict documentation\n",
    "        \"\"\"\n",
    "        gateways = []\n",
    "        for sentence_idx, sentence_gateways in enumerate(extracted_gateways):\n",
    "            sentence_gateways_already_included = []\n",
    "            for i, gateway in enumerate(sentence_gateways):\n",
    "                if gateway not in sentence_gateways_already_included:\n",
    "                    gateway_tokens = [gateway[0]]\n",
    "                    start_token_idx = gateway[1]\n",
    "                    # append further tokens of same gateway ('I-' marked)\n",
    "                    I_index = i + 1\n",
    "                    while I_index < len(sentence_gateways) and sentence_gateways[I_index][2].startswith('I-'):\n",
    "                        gateway_tokens.append(sentence_gateways[I_index][0])\n",
    "                        sentence_gateways_already_included.append(sentence_gateways[I_index])\n",
    "                        I_index += 1\n",
    "                    gateway_tokens_lower = [t.lower() for t in gateway_tokens]\n",
    "                    gateway_tuple = (sentence_idx, start_token_idx, gateway_tokens, gateway_tokens_lower)\n",
    "                    gateways.append(self._get_entity_dict(gateway_tuple, gateway_type))\n",
    "        return gateways\n",
    "\n",
    "    def _extract_gold_activity_flows(self, doc_activity_tokens: List[List[Tuple[str, int]]]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Creates simple flows by order of activities\n",
    "        :param doc_activity_tokens: list of activity tokens (word, idx) for each sentence\n",
    "        :return: list of flows represented as dicts\n",
    "        \"\"\"\n",
    "        activities_flattened = [(i, activity) for i, sentence_activities in enumerate(doc_activity_tokens)\n",
    "                                for activity in sentence_activities]\n",
    "        flow_relations = []\n",
    "        for i in range(len(activities_flattened) - 1):\n",
    "            s_idx_1, a1 = activities_flattened[i]\n",
    "            s_idx_2, a2 = activities_flattened[i + 1]\n",
    "            a1 = self._get_pet_relation_rep(s_idx_1, a1[1], ACTIVITY, a1[0], source=True)\n",
    "            a2 = self._get_pet_relation_rep(s_idx_2, a2[1], ACTIVITY, a2[0], source=False)\n",
    "            flow_relations.append({**a1, **a2})\n",
    "        return flow_relations\n",
    "\n",
    "    def _extract_exclusive_flows(self, doc_activity_tokens: List[List[Tuple[str, int]]],\n",
    "                                 extracted_gateways: List[List[Tuple[str, int, str]]]) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"\n",
    "        extracts sequence flows surrounding exclusive gateways based on rules TODO describe rules\n",
    "        :param doc_activity_tokens: list of activity tokens (word, idx) for each sentence\n",
    "        :param extracted_gateways: list of own extracted gateway for each sentence\n",
    "        :return: list of flow relations as source/target dicts; list of same gateway relations as source/target dicts\n",
    "        \"\"\"\n",
    "        sequence_flows = []\n",
    "        same_gateway_relations = []\n",
    "\n",
    "        gateways = self._preprocess_extracted_gateways(extracted_gateways, XOR_GATEWAY)\n",
    "        gateways_involved = []  # list for gateways already involved into sequence flows\n",
    "\n",
    "        # RULE 1): check for every pair of following gateways if it fits to a gateway constellation with\n",
    "        # contradictory key words. Gateways must be in range of same_xor_gateway_threshold sentences, otherwise they\n",
    "        # would be seen as separate ones\n",
    "        for i in range(len(gateways) - 1):\n",
    "            g1, g2 = gateways[i], gateways[i + 1]\n",
    "            # if sentence distances is larger than threshold, reject possible pair\n",
    "            if abs(g2[ELEMENT][0] - g1[ELEMENT][0]) > self._same_xor_gateway_threshold:\n",
    "                continue\n",
    "            # check for every pair of following gateways if it fits to a gateway pair of contradictory key words\n",
    "            # and check that first gateway is at the beginning of a sentence\n",
    "            # and check if gateways already matched another pair; possible because of partly same phrase\n",
    "            for pattern_gateway_1, pattern_gateway_2 in self._contradictory_gateways:\n",
    "                if g1[ELEMENT][3] == pattern_gateway_1 and g2[ELEMENT][3] == pattern_gateway_2 and g1[ELEMENT][1] == 0 \\\n",
    "                        and g1[ELEMENT] not in gateways_involved and g2[ELEMENT] not in gateways_involved:\n",
    "                    gateways_involved.append(g1[ELEMENT])\n",
    "                    gateways_involved.append(g2[ELEMENT])\n",
    "\n",
    "                    # A) find related activities\n",
    "                    _, pa_g1, fa_g1, _ = self._get_surrounding_activities(g1, doc_activity_tokens)\n",
    "                    _, _, fa_g2, ffa_g2 = self._get_surrounding_activities(g2, doc_activity_tokens)\n",
    "\n",
    "                    # B.1) connect elements to sequence flows\n",
    "                    # check if fol. activities of g1 and g2 are equal -> if yes, the first branch is without activity\n",
    "                    empty_branch = fa_g1[ELEMENT] == fa_g2[ELEMENT]\n",
    "                    # 1) previous activity to first gateway -> split point (if not None because of document start)\n",
    "                    if pa_g1[ELEMENT]:\n",
    "                        sequence_flows.append(self._merge_flow(pa_g1, g1))\n",
    "                    # 2) gateway 1 to following activity and following activity to activity after gateway (second\n",
    "                    # following of g2) -> merge point\n",
    "                    # if None because of empty branch then directly there\n",
    "                    if not empty_branch and fa_g1[ELEMENT]:  # could be None if at document end\n",
    "                        sequence_flows.append(self._merge_flow(g1, fa_g1))\n",
    "                        if ffa_g2[ELEMENT]:  # could be None if at document end\n",
    "                            sequence_flows.append(self._merge_flow(fa_g1, ffa_g2))\n",
    "                    elif empty_branch and ffa_g2[ELEMENT]:  # could be None if at document end\n",
    "                        sequence_flows.append(self._merge_flow(g1, ffa_g2))\n",
    "                    # 3) gateway 2 to following activity and following activity to activity after gateway (second\n",
    "                    # following of g2) -> merge point\n",
    "                    if fa_g2[ELEMENT]:  # could be None if at document end\n",
    "                        sequence_flows.append(self._merge_flow(g2, fa_g2))\n",
    "                    if ffa_g2[ELEMENT]:  # could be None if at document end\n",
    "                        sequence_flows.append(self._merge_flow(fa_g2, ffa_g2))\n",
    "\n",
    "                    # B.2) same gateway flows\n",
    "                    same_gateway_relations.append(self._merge_flow(g1, g2))\n",
    "\n",
    "                    # log gateway frame for later usage in flow merging of whole document\n",
    "                    closing = fa_g2 if fa_g2[ELEMENT] else g2\n",
    "                    self._log_gateway_frame(g1[ELEMENT][0], g1[ELEMENT][1], g1,\n",
    "                                            closing[ELEMENT][0], closing[ELEMENT][1], closing)\n",
    "\n",
    "        # RULE 2): exclusive actions of common pattern \"... <activity> ... or ... <activity> ...\"\n",
    "        for g in gateways:\n",
    "            if g[ELEMENT] not in gateways_involved and g[ELEMENT][3] == ['or']:\n",
    "                # A) find surrounding activities\n",
    "                ppa, pa, fa, ffa = self._get_surrounding_activities(g, doc_activity_tokens)\n",
    "\n",
    "                if pa[ELEMENT] and fa[ELEMENT]:  # check if exist because of document end/start\n",
    "                    if pa[ELEMENT][0] == g[ELEMENT][0] and fa[ELEMENT][0] == g[ELEMENT][0]:  # check if in same sentence\n",
    "\n",
    "                        if pa[ELEMENT] is None or fa[ELEMENT] is None:\n",
    "                            # if not surrounding activities are given, do not wire anything; TODO: maybe drop gateway\n",
    "                            continue\n",
    "                        gateways_involved.append(g[ELEMENT])\n",
    "\n",
    "                        # B) connect elements to sequence flows\n",
    "                        # 1) second previous activity to gateway -> split point\n",
    "                        if ppa[ELEMENT]:  # (if not None because of document start)\n",
    "                            sequence_flows.append(self._merge_flow(ppa, g))\n",
    "                        # 2) gateway to following activity and previous activity -> exclusive branches\n",
    "                        sequence_flows.append(self._merge_flow(g, pa))\n",
    "                        sequence_flows.append(self._merge_flow(g, fa))\n",
    "                        # 3) exclusive activities to second following activity of gateway -> merge point\n",
    "                        if ffa[ELEMENT]:  # (if not None because of document end)\n",
    "                            sequence_flows.append(self._merge_flow(pa, ffa))\n",
    "                            sequence_flows.append(self._merge_flow(fa, ffa))\n",
    "\n",
    "                        # log gateway frame for later usage in flow merging of whole document\n",
    "                        self._log_gateway_frame(pa[ELEMENT][0], pa[ELEMENT][1], g, fa[ELEMENT][0], fa[ELEMENT][1], fa)\n",
    "\n",
    "        # RULE 3): single-branch gateways: the gateway is related to an activity in the same sentence (order is arbitrary)\n",
    "        # Assumptiosn: multi-branch gateways are already recognized by rule 1 before; only one activity for the gateway\n",
    "        for g in gateways:\n",
    "            if g[ELEMENT] not in gateways_involved and g[ELEMENT][3] != ['or']:\n",
    "                # A) Prepare elements for flow connections\n",
    "                ppa, pa, fa, ffa = self._get_surrounding_activities(g, doc_activity_tokens)\n",
    "                gateways_involved_length_start = len(gateways_involved)\n",
    "\n",
    "                # B) connect elements to sequence flows -> check which activities exist and how are they located\n",
    "                # Assumption: only one in the sentence including the gateway\n",
    "                if fa[ELEMENT] or pa[ELEMENT]:\n",
    "\n",
    "                    # case 1: no activity before but after in same sentence\n",
    "                    if fa[ELEMENT] and not pa[ELEMENT] and fa[ELEMENT][0] == g[ELEMENT][0]:\n",
    "                        sequence_flows.append(self._merge_flow(g, fa))\n",
    "                        if ffa[ELEMENT]:  # could be None if at document end\n",
    "                            sequence_flows.append(self._merge_flow(g, ffa))\n",
    "                            sequence_flows.append(self._merge_flow(fa, ffa))\n",
    "\n",
    "                        # log gateway frame for later usage in flow merging of whole document\n",
    "                        self._log_gateway_frame(g[ELEMENT][0], g[ELEMENT][1], g, fa[ELEMENT][0], fa[ELEMENT][1], fa)\n",
    "                        gateways_involved.append(g[ELEMENT])\n",
    "\n",
    "                    # case 2: no activity after but before in same sentence\n",
    "                    elif pa[ELEMENT] and not fa[ELEMENT] and pa[ELEMENT][0] == g[ELEMENT][0]:\n",
    "                        sequence_flows.append(self._merge_flow(g, pa))\n",
    "                        # no check for ffa link necessary, because fa is already none\n",
    "                        # log gateway frame for later usage in flow merging of whole document\n",
    "                        self._log_gateway_frame(pa[ELEMENT][0], pa[ELEMENT][1], g,\n",
    "                                                g[ELEMENT][0], g[ELEMENT][1], pa)\n",
    "                        gateways_involved.append(g[ELEMENT])\n",
    "\n",
    "                    elif pa[ELEMENT] and fa[ELEMENT]:\n",
    "                        # case 3: previous is not in the same sentence, but following yes -> activity after gateway\n",
    "                        if pa[ELEMENT][0] != g[ELEMENT][0] and fa[ELEMENT][0] == g[ELEMENT][0]:\n",
    "                            # 1) previous activity to gateway -> split point\n",
    "                            sequence_flows.append(self._merge_flow(pa, g))\n",
    "                            # 2) gateway to following activity -> exclusive branch\n",
    "                            sequence_flows.append(self._merge_flow(g, fa))\n",
    "                            # 3) exclusive activity and gateway to second following activity of gateway -> merge point\n",
    "                            if ffa[ELEMENT]:  # could be None if at document end\n",
    "                                sequence_flows.append(self._merge_flow(g, ffa))\n",
    "                                sequence_flows.append(self._merge_flow(fa, ffa))\n",
    "                            # log gateway frame for later usage in flow merging of whole document\n",
    "                            self._log_gateway_frame(g[ELEMENT][0], g[ELEMENT][1], g, fa[ELEMENT][0], fa[ELEMENT][1], fa)\n",
    "                            gateways_involved.append(g[ELEMENT])\n",
    "\n",
    "                        # case 4: previous is in the same sentence, but following not -> activity before gateway\n",
    "                        elif pa[ELEMENT][0] == g[ELEMENT][0] and fa[ELEMENT][0] != g[ELEMENT][0]:\n",
    "                            # 1) second previous activity to gateway -> split point\n",
    "                            if ppa[ELEMENT]:  # could be None if at document start\n",
    "                                sequence_flows.append(self._merge_flow(ppa, g))\n",
    "                            # 2) gateway to previous activity -> exclusive branch\n",
    "                            sequence_flows.append(self._merge_flow(g, pa))\n",
    "                            # 3) exclusive activity and gateway to following activity of gateway -> merge point\n",
    "                            sequence_flows.append(self._merge_flow(g, fa))\n",
    "                            sequence_flows.append(self._merge_flow(pa, fa))\n",
    "                            # log gateway frame for later usage in flow merging of whole document\n",
    "                            self._log_gateway_frame(pa[ELEMENT][0], pa[ELEMENT][1], g,\n",
    "                                                    fa[ELEMENT][0], fa[ELEMENT][1], fa)\n",
    "                            gateways_involved.append(g[ELEMENT])\n",
    "\n",
    "                if len(gateways_involved) == gateways_involved_length_start:\n",
    "                    pass  # TODO: remove gateway if no rule for extracting flows could be applied\n",
    "\n",
    "        return sequence_flows, same_gateway_relations\n",
    "\n",
    "    def _extract_concurrent_flows(self, doc_activity_tokens: List[List[Tuple[str, int]]],\n",
    "                                  extracted_gateways: List[List[Tuple[str, int, str]]]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        extract flow relations for already found AND gateways following the logic:\n",
    "        - for every gateway, to extract parallel branches, add relation to next activity after and before, because\n",
    "          that's the pattern how AND key phrases are usually used (oriented by rules of Ferreira et al. 2017)\n",
    "        - for each case, check over borders if not found in same sentence\n",
    "        - to extract the flow relation that points to the gateway split point, take the second before\n",
    "        - to extract the flow relation that points to the gateway merge point, take the second following\n",
    "\n",
    "        :param doc_activity_tokens: list of activity tokens (word, idx) for each sentence\n",
    "        :param extracted_gateways: list of own extracted gateway for each sentence\n",
    "        :return: list of flow relations in source/target dict representation\n",
    "        \"\"\"\n",
    "        relations = []\n",
    "\n",
    "        for g in self._preprocess_extracted_gateways(extracted_gateways, AND_GATEWAY):\n",
    "            # 1) Find surrounding activities\n",
    "            ppa, pa, fa, ffa = self._get_surrounding_activities(g, doc_activity_tokens)\n",
    "\n",
    "            # 2) Create relations\n",
    "            # a) flow to gateway: second previous -> gateway\n",
    "            if ppa[ELEMENT]:  # could be None if at document start\n",
    "                relations.append(self._merge_flow(ppa, g))\n",
    "            # b) split into concurrent gateway branches: gateway -> previous; gateway -> following\n",
    "            # following two None checks (probably) wont never be False, but for safety included\n",
    "            if pa[ELEMENT]:  # could be None if at document start\n",
    "                relations.append(self._merge_flow(g, pa))\n",
    "            if fa[ELEMENT]:  # could be None if at document end\n",
    "                relations.append(self._merge_flow(g, fa))\n",
    "            # c) merge branches together: previous -> second following; following -> second following\n",
    "            if ffa[ELEMENT]:  # could be None if at document end\n",
    "                relations.append(self._merge_flow(pa, ffa))\n",
    "                relations.append(self._merge_flow(fa, ffa))\n",
    "\n",
    "            # log gateway frame for later usage in flow merging of whole document\n",
    "            self._log_gateway_frame(pa[ELEMENT][0], pa[ELEMENT][1], g,\n",
    "                                    fa[ELEMENT][0], fa[ELEMENT][1], fa)\n",
    "\n",
    "        return relations\n",
    "\n",
    "    def _merge_flows(self, gold_activity_flows: List[Dict], xor_flows: List[Dict], and_flows: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        merge gold activity flows and flows surrounding gateways into a list of flows for the whole document\n",
    "        -> gold activity flows are filtered if a gateway flow is involved into an activity\n",
    "        :param gold_activity_flows: list of flows between gold activities\n",
    "        :param xor_flows: list of flows involved in XOR gateways\n",
    "        :param and_flows: list of flows involved in AND gateways\n",
    "        :return: list of flows describing the whole document\n",
    "        \"\"\"\n",
    "        gateway_flows = xor_flows + and_flows\n",
    "        if self._log_document_level_details:\n",
    "            logger.info(f\"{len(gateway_flows)} gateway flows\")\n",
    "            logger.info(f\"{len(gold_activity_flows)} gold activity flows\")\n",
    "\n",
    "        gateway_flows_source_entities = [flow[SOURCE_ENTITY] for flow in gateway_flows]\n",
    "        # 1) gateway flows as basis\n",
    "        doc_flows = gateway_flows.copy()\n",
    "\n",
    "        # 2) check if flow from this entity to another entity exists already in the gateway flows\n",
    "        # -> prefer flows to gateways than to gold activities\n",
    "        for flow in gold_activity_flows:\n",
    "            if not flow[SOURCE_ENTITY] in gateway_flows_source_entities:\n",
    "                doc_flows.append(flow)\n",
    "        doc_flows.sort(key=lambda flow: (flow[SOURCE_SENTENCE_ID], flow[SOURCE_HEAD_TOKEN_ID]))\n",
    "\n",
    "        def get_gateway_frames(flow):\n",
    "            \"\"\"\n",
    "            return a tuple that contains the gateway frames of source and target entity if they are in one, if not None\n",
    "            :param flow: flow dict in PET format\n",
    "            :return: tuple of gateway frame indices\n",
    "            \"\"\"\n",
    "            source_entity_gateway_frame = None\n",
    "            target_entity_gateway_frame = None\n",
    "\n",
    "            for i, gateway_frame in enumerate(self._processed_doc_gateway_frames):\n",
    "\n",
    "                if (flow[SOURCE_SENTENCE_ID] == gateway_frame[START_SENTENCE_IDX]\n",
    "                    and flow[SOURCE_HEAD_TOKEN_ID] >= gateway_frame[START_TOKEN_ID]) or \\\n",
    "                        (flow[SOURCE_SENTENCE_ID] > gateway_frame[START_SENTENCE_IDX]\n",
    "                         and flow[SOURCE_SENTENCE_ID] < gateway_frame[END_SENTENCE_IDX]) or \\\n",
    "                        (flow[SOURCE_SENTENCE_ID] == gateway_frame[END_SENTENCE_IDX]\n",
    "                         and flow[SOURCE_HEAD_TOKEN_ID] <= gateway_frame[END_TOKEN_ID]):\n",
    "                    if not source_entity_gateway_frame:\n",
    "                        source_entity_gateway_frame = i\n",
    "\n",
    "                if (flow[TARGET_SENTENCE_ID] == gateway_frame[START_SENTENCE_IDX]\n",
    "                    and flow[TARGET_HEAD_TOKEN_ID] >= gateway_frame[START_TOKEN_ID]) or \\\n",
    "                        (flow[TARGET_SENTENCE_ID] > gateway_frame[START_SENTENCE_IDX]\n",
    "                         and flow[TARGET_SENTENCE_ID] < gateway_frame[END_SENTENCE_IDX]) or \\\n",
    "                        (flow[TARGET_SENTENCE_ID] == gateway_frame[END_SENTENCE_IDX]\n",
    "                         and flow[TARGET_HEAD_TOKEN_ID] <= gateway_frame[END_TOKEN_ID]):\n",
    "                    if not target_entity_gateway_frame:\n",
    "                        target_entity_gateway_frame = i\n",
    "\n",
    "            return source_entity_gateway_frame, target_entity_gateway_frame\n",
    "\n",
    "        # 3) check if target of this flows is inside a detected gateway frame\n",
    "        # -> if yes, redirect flow to gateway start / split point\n",
    "        flows_to_remove = []\n",
    "        flows_to_add = []\n",
    "        for flow in doc_flows:\n",
    "            source_entity_gf, target_entity_gf = get_gateway_frames(flow)\n",
    "            # check if source and target are not part of the same gateway and the target entity is part of a gateway\n",
    "            if source_entity_gf != target_entity_gf and target_entity_gf is not None:\n",
    "                # if the flow target is not the start entity of the gateway (i.e. split point), then rewire\n",
    "                if flow[TARGET_ENTITY] !=self._processed_doc_gateway_frames[target_entity_gf][START_ENTITY][ELEMENT][2]:\n",
    "                    flows_to_remove.append(flow)\n",
    "                    # create new flow between source of current flow and start entity of gateway (split point)\n",
    "                    flows_to_add.append({**{k: v for k, v in flow.items() if k.startswith(\"source-\")},\n",
    "                                         **self._processed_doc_gateway_frames[target_entity_gf][START_ENTITY][TARGET]})\n",
    "\n",
    "        # add/remove new/wrong wired flows after gateway frame check\n",
    "        doc_flows.extend(flows_to_add)\n",
    "        for flow in flows_to_remove:\n",
    "            doc_flows.remove(flow)\n",
    "\n",
    "        # filtered doc flows for duplicates\n",
    "        doc_flows_unique = []\n",
    "        for flow in doc_flows:\n",
    "            if flow not in doc_flows_unique:\n",
    "                doc_flows_unique.append(flow)\n",
    "        doc_flows = doc_flows_unique\n",
    "\n",
    "        # sort for easier debugging by order in text\n",
    "        doc_flows.sort(key=lambda flow: (flow[SOURCE_SENTENCE_ID], flow[SOURCE_HEAD_TOKEN_ID]))\n",
    "\n",
    "        # clear gateway frames of processed doc for next processing\n",
    "        self._processed_doc_gateway_frames.clear()\n",
    "        if self._log_document_level_details:\n",
    "            logger.info(f\"{len(doc_flows)} doc flows\")\n",
    "        return doc_flows\n",
    "\n",
    "    def _get_surrounding_activities(self, gateway: 'see _get_entity_dict documentation',\n",
    "                                    doc_activity_tokens: List[List[Tuple[str, int]]]) -> Tuple[Dict, Dict, Dict, Dict]:\n",
    "        \"\"\"\n",
    "        searches for all surrounding activities of a gateway (previous, second previous, following, second following)\n",
    "        :param gateway: gateway as entity dict\n",
    "        :param doc_activity_tokens: list of activity lists (describes whole document)\n",
    "        :return: for activities; each as dictionary\n",
    "        \"\"\"\n",
    "        g = gateway[ELEMENT]\n",
    "        # 1) get surrounding activities (previous, second previous, following, second following)\n",
    "        pa = self._get_previous_activity(g[0], g[1], doc_activity_tokens)\n",
    "        ppa = self._get_previous_activity(g[0], g[1], doc_activity_tokens, skip_first=True)\n",
    "        fa = self._get_following_activity(g[0], g[1], doc_activity_tokens)\n",
    "        ffa = self._get_following_activity(g[0], g[1], doc_activity_tokens, skip_first=True)\n",
    "\n",
    "        return self._get_entity_dict(ppa, ACTIVITY), self._get_entity_dict(pa, ACTIVITY), \\\n",
    "               self._get_entity_dict(fa, ACTIVITY), self._get_entity_dict(ffa, ACTIVITY)\n",
    "\n",
    "    # HINT: the two following methods follow the same logic, just in different search direction\n",
    "\n",
    "    def _get_previous_activity(self, sentence_idx: int, token_idx: int,\n",
    "                               doc_activity_tokens: List[List[Tuple[str, int]]], skip_first: bool = False,\n",
    "                               one_already_found: bool = False) -> Optional[Tuple[int, int, str]]:\n",
    "        \"\"\"\n",
    "        search recursive for the second last previous activity from a start point defined by sentence_idx and token_idx\n",
    "        :param sentence_idx: sentence index where to start the search\n",
    "        :param token_idx: token index where to stat the search\n",
    "        :param doc_activity_tokens: list of activity lists (describes whole document)\n",
    "        :param skip_first: True if searching for the second previous activity, False (default) when searching for the previous activity\n",
    "        :param one_already_found: flag if one activity was already found and skipped for return in course of search for the second previous\n",
    "        :returns: triple of (sentence idx, token_idx, token) if found, else None\n",
    "        \"\"\"\n",
    "        # search for activities left to the token in target sentence if token is given else in the whole\n",
    "        if token_idx is not None:\n",
    "            previous_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx] if a_t[1] < token_idx]\n",
    "        else:\n",
    "            previous_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx]]\n",
    "\n",
    "        # if activities were found, take the last one\n",
    "        if previous_activities_sentence:\n",
    "            # return when just searching the first last activity OR when one was already found before\n",
    "            previous_activity = previous_activities_sentence[-1]\n",
    "            # 1a) base case: activity found\n",
    "            if not skip_first or one_already_found:\n",
    "                return (sentence_idx, previous_activity[1], previous_activity[0])\n",
    "            # 2a) recursive case: continue search for second previous activity at index of previous activity\n",
    "            else:\n",
    "                return self._get_previous_activity(sentence_idx, previous_activity[1], doc_activity_tokens,\n",
    "                                                   one_already_found=True)\n",
    "        else:\n",
    "            next_sentence_idx = sentence_idx - 1\n",
    "            # 1b) base case: no sentences any more to search\n",
    "            if next_sentence_idx == -1:\n",
    "                return None\n",
    "            # 2b) recursive case: continue search for previous activity in previous sentence\n",
    "            else:\n",
    "                return self._get_previous_activity(next_sentence_idx, None, doc_activity_tokens,\n",
    "                                                   skip_first=skip_first, one_already_found=one_already_found)\n",
    "\n",
    "    def _get_following_activity(self, sentence_idx: int, token_idx: int,\n",
    "                                doc_activity_tokens: List[List[Tuple[str, int]]], skip_first: bool = False,\n",
    "                                one_already_found: bool = False) -> Optional[Tuple[int, int, str]]:\n",
    "        \"\"\"\n",
    "        search recursive for the next following activity from a start point defined by sentence_idx and token_idx\n",
    "        :param sentence_idx: sentence index where to start the search\n",
    "        :param token_idx: token index where to stat the search\n",
    "        :param doc_activity_tokens: list of activity lists (describes whole document)\n",
    "        :param skip_first: True if searching for the second following activity, False (default) when searching for the following activity\n",
    "        :param one_already_found: flag if one activity was already found and skipped for return in course of search for the second following\n",
    "        :returns: triple of (sentence idx, token_idx, token) or None if none was found\n",
    "        \"\"\"\n",
    "        # search for activities right to the token in target sentence if token is given else in the whole\n",
    "        if token_idx is not None:\n",
    "            following_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx] if a_t[1] > token_idx]\n",
    "        else:\n",
    "            following_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx]]\n",
    "\n",
    "        # if activities were found, take the first one\n",
    "        if following_activities_sentence:\n",
    "            # return when just searching the first following activity OR when one was already found before\n",
    "            following_activity = following_activities_sentence[0]\n",
    "            # 1a) base case: activity found\n",
    "            if not skip_first or one_already_found:\n",
    "                return (sentence_idx, following_activity[1], following_activity[0])\n",
    "            # 2a) recursive case: continue search for second following activity at index of following activity\n",
    "            else:\n",
    "                return self._get_following_activity(sentence_idx, following_activity[1], doc_activity_tokens,\n",
    "                                                    one_already_found=True)\n",
    "\n",
    "        else:\n",
    "            next_sentence_idx = sentence_idx + 1\n",
    "            # 1b) base case: no sentences any more to search\n",
    "            if next_sentence_idx == len(doc_activity_tokens):\n",
    "                return None\n",
    "            # 2b) recursive case: continue search for following activity in following sentence\n",
    "            else:\n",
    "                return self._get_following_activity(next_sentence_idx, None, doc_activity_tokens,\n",
    "                                                    skip_first=skip_first, one_already_found=one_already_found)\n",
    "\n",
    "    def _get_pet_entity_relation_rep(self, entity: Tuple[int, int, List[str], Optional[List[str]]], entity_type: str,\n",
    "                                     source: bool = True):\n",
    "        \"\"\"\n",
    "        return the dict representation of an entity for usage as part of a relation\n",
    "        dict structure depends on the output format of the baseline (as in PET or simpler for benchmark library)\n",
    "        :param entity: process entity as tuple -> sentence idx, token idx, ['Word', 'List'], optional(['word', 'List'])\n",
    "                       last element of tuple is optional (is passed for gateways, for activities not)\n",
    "        :param entity_type: entity type according to PET labels\n",
    "        :param source: flag if it is source or target entity in the relation\n",
    "        :return: Dictionary in format based on the output format\n",
    "        \"\"\"\n",
    "        return self._get_pet_relation_rep(entity[0], entity[1], entity_type, entity[2], source=source)\n",
    "\n",
    "    def _get_pet_relation_rep(self, sentence_idx: int, token_idx: int, entity_type: str, entity: List[str],\n",
    "                              source: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        return the dict representation of an entity for usage as part of a relation\n",
    "        :param sentence_idx: sentence index\n",
    "        :param token_idx: token/word index\n",
    "        :param entity_type: entity type according to PET labels\n",
    "        :param entity: entity as list of single words\n",
    "        :param source: flag if it is source or target entity in the relation\n",
    "        :return: Dictionary in format based on the output format\n",
    "        \"\"\"\n",
    "        if source:\n",
    "            return {\n",
    "                SOURCE_SENTENCE_ID: sentence_idx,\n",
    "                SOURCE_HEAD_TOKEN_ID: token_idx,\n",
    "                SOURCE_ENTITY_TYPE: entity_type,\n",
    "                SOURCE_ENTITY: entity\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                TARGET_SENTENCE_ID: sentence_idx,\n",
    "                TARGET_HEAD_TOKEN_ID: token_idx,\n",
    "                TARGET_ENTITY_TYPE: entity_type,\n",
    "                TARGET_ENTITY: entity\n",
    "            }\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_flow(source, target):\n",
    "        \"\"\"\n",
    "        merge two entity dictionaries (created with _get_entity_dict) into one\n",
    "        :param source: source entity\n",
    "        :param target: target entity\n",
    "        :return: merged entitiy\n",
    "        \"\"\"\n",
    "        return {**source[SOURCE], **target[TARGET]}\n",
    "\n",
    "    def _get_entity_dict(self, entity: Tuple[int, int, List[str], Optional[List[str]]], entity_type: str) -> Dict:\n",
    "        \"\"\"\n",
    "        create entity dictionary including the entity itself and its source and target repr. dicts for flow connections\n",
    "        :param entity: entity (activity or gateway) in form of tuple\n",
    "        :param entity_type: entity type\n",
    "        :return: dict with structure\n",
    "            element: Tuple[int, int, List[str], Optional[List[str]]]\n",
    "            source/target: dict -> structure based on flow relation dict from PET\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'element': entity,  # tuple (sentence idx, token idx, [word, list])\n",
    "            'source': self._get_pet_entity_relation_rep(entity, entity_type, source=True) if entity else None,\n",
    "            'target': self._get_pet_entity_relation_rep(entity, entity_type, source=False) if entity else None\n",
    "        }\n",
    "\n",
    "    def _log_gateway_frame(self, start_sentence_idx: int, start_token_idx: int, start_entity: Dict,\n",
    "                           end_sentence_idx: int, end_token_idx: int, end_entity: Dict) -> None:\n",
    "        \"\"\"\n",
    "        log frame of a gateway sequence defined by start/end sentence index, token index and entity\n",
    "        \"\"\"\n",
    "        self._processed_doc_gateway_frames.append({\n",
    "            START_SENTENCE_IDX: start_sentence_idx,\n",
    "            START_TOKEN_ID: start_token_idx,\n",
    "            START_ENTITY: start_entity,\n",
    "            END_SENTENCE_IDX: end_sentence_idx,\n",
    "            END_TOKEN_ID: end_token_idx,\n",
    "            END_ENTITY: end_entity,\n",
    "        })\n",
    "\n",
    "    def _read_and_set_keywords(self) -> None:\n",
    "        \"\"\"\n",
    "        load and set key word lists based on passed variant\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logger.info(f\"Load keywords '{self.keywords}' ...\")\n",
    "        if self.keywords == LITERATURE:\n",
    "            # based on key words proposals of Ferreira et al. 2017\n",
    "            with open('data/keywords/literature_xor.txt') as file:\n",
    "                self._xor_keywords = file.read().splitlines()\n",
    "\n",
    "            with open('data/keywords/literature_and.txt') as file:\n",
    "                self._and_keywords = file.read().splitlines()\n",
    "\n",
    "        elif self.keywords == GOLD:\n",
    "            self._xor_keywords = pet_reader.xor_key_words_gold\n",
    "            self._and_keywords = pet_reader.and_key_words_gold\n",
    "\n",
    "        elif self.keywords == OWN:\n",
    "            raise NotImplementedError(\"Own keywords not implemented yet\")\n",
    "\n",
    "        self._xor_keywords.sort()\n",
    "        self._and_keywords.sort()\n",
    "        logger.info(f\"Loaded {len(self._xor_keywords)} XOR and {len(self._and_keywords)} AND keywords\")\n",
    "        logger.info(f\"Used XOR keywords: {self._xor_keywords}\")\n",
    "        logger.info(f\"Used AND keywords: {self._and_keywords}\")\n",
    "\n",
    "    def _read_contradictory_gateways(self):\n",
    "        \"\"\"\n",
    "        read pairs of contradictory exclusive gateway key words from file\n",
    "        sort to prefer longer matching phrases during search\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open('data/keywords/contradictory_gateways_gold.txt') as file:\n",
    "            self._contradictory_gateways = [[x.split(\" \") for x in l.strip().split(\";\")] for l in file.readlines()]\n",
    "            self._contradictory_gateways.sort(key=lambda pair: len(pair[0]) + len(pair[1]), reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ad7132",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:keyword approach:Create TokenClassificationBenchmark ...\n",
      "WARNING:datasets.builder:Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\token-classification\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07737588882446289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da95e0b827c840f989e709def8e7fd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:keyword approach:Create RelationsExtractionBenchmark ...\n",
      "WARNING:datasets.builder:Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\relations-extraction\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03891706466674805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9ff455de57486fa952a50e45d73cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare evaluation structures to fill\n",
    "logger.info(\"Create TokenClassificationBenchmark ...\")\n",
    "tcb = TokenClassificationBenchmark()\n",
    "process_elements = tcb.GetEmptyPredictionsDict()\n",
    "logger.info(\"Create RelationsExtractionBenchmark ...\")\n",
    "reb = RelationsExtractionBenchmark()\n",
    "relations = reb.GetEmptyPredictionsDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94728d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:keyword approach:Load keywords 'literature' ...\n",
      "INFO:keyword approach:Loaded 14 XOR and 11 AND keywords\n",
      "INFO:keyword approach:Used XOR keywords: ['either', 'if', 'if not', 'in case', 'in case of', 'only', 'only if', 'or', 'otherwise', 'till', 'unless', 'until', 'when', 'whether']\n",
      "INFO:keyword approach:Used AND keywords: ['at the same time', 'concurrently', 'in addition to', 'in parallel', 'in parallel with this', 'in the meantime', 'meantime', 'meanwhile', 'simultaneously', 'whereas', 'while']\n",
      "INFO:keyword approach:Create TokenClassificationBenchmark ...\n",
      "INFO:keyword approach:Create RelationsExtractionBenchmark ...\n",
      "C:\\Users\\janek\\anaconda3\\envs\\thesis\\lib\\site-packages\\petbenchmarks\\relationsextraction.py:46: UserWarning: UPDATE THIS FUNCTION\n",
      "  warnings.warn('UPDATE THIS FUNCTION')\n",
      "INFO:keyword approach:Start processing of 1 documents ...\n",
      "INFO:keyword approach:Finished processing of documents\n",
      "INFO:keyword approach:Saved results to data/results/key_words_literature/\n",
      "INFO:keyword approach:Run process element / token classification evaluation\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TokenClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m keyword_approach \u001b[38;5;241m=\u001b[39m KeywordApproach(approach_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_words_literature\u001b[39m\u001b[38;5;124m'\u001b[39m, keywords\u001b[38;5;241m=\u001b[39mLITERATURE,\n\u001b[0;32m      3\u001b[0m                                    same_xor_gateway_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, output_format\u001b[38;5;241m=\u001b[39mBENCHMARK)\n\u001b[0;32m      5\u001b[0m doc_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc-3.2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mkeyword_approach\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_token_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_relation_extraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [2], line 84\u001b[0m, in \u001b[0;36mKeywordApproach.evaluate_documents\u001b[1;34m(self, doc_names, log_document_details, evaluate_token_cls, evaluate_relation_extraction)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_token_cls:\n\u001b[0;32m     83\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun process element / token classification evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mBenchmarkApproach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapproach_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproach_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_file_or_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_elements_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpet_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpet_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     format_json_file(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproach_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_relation_extraction:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\petbenchmarks\\benchmarks.py:219\u001b[0m, in \u001b[0;36mBenchmarkApproach.__init__\u001b[1;34m(self, approach_name, predictions_file_or_folder, output_results, relax_window, strategy, pet_dataset)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions_file_or_folder \u001b[38;5;241m=\u001b[39m predictions_file_or_folder\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadPrediction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions_file_or_folder)\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLunchBenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpet_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\petbenchmarks\\benchmarks.py:149\u001b[0m, in \u001b[0;36mBenchmark.LunchBenchmark\u001b[1;34m(self, pet_dataset, results_filename)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbenchmarking_task \u001b[38;5;241m==\u001b[39m TOKEN_CLASSIFICATION:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;66;03m#  token classification\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pet_dataset:\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pet_dataset, \u001b[43mTokenClassification\u001b[49m)\n\u001b[0;32m    150\u001b[0m     bench \u001b[38;5;241m=\u001b[39m TokenClassificationBenchmark(PETDataset\u001b[38;5;241m=\u001b[39mpet_dataset)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m#  relation extraction\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TokenClassification' is not defined"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "keyword_approach = KeywordApproach(approach_name='key_words_literature', keywords=LITERATURE,\n",
    "                                   same_xor_gateway_threshold=1, output_format=BENCHMARK)\n",
    "\n",
    "doc_names = ['doc-3.2']\n",
    "keyword_approach.evaluate_documents(doc_names, evaluate_token_cls=True, evaluate_relation_extraction=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
