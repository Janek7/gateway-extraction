{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da85bfe7",
   "metadata": {},
   "source": [
    "# Key Word Approach\n",
    "variables with prefix ``doc_`` contain data from the dataset\n",
    "variables with prefix ``o_`` contain data from own computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bdd7e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from petreader.RelationsExtraction import RelationsExtraction\n",
    "from petreader.TokenClassification import TokenClassification\n",
    "from petreader import labels\n",
    "from petreader.labels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c49670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\relations-extraction\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022540807723999023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfa21554bb8474e838235b45f405732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\token-classification\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018558740615844727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354da6037a634599ba49b72259dc7ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relations_dataset = RelationsExtraction()\n",
    "token_dataset = TokenClassification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d905d3",
   "metadata": {},
   "source": [
    "## 1 Prepare Document Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51523aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc-1.3', 'doc-1.4', 'doc-1.2', 'doc-1.1', 'doc-10.11', 'doc-10.2', 'doc-10.8', 'doc-10.10', 'doc-10.7', 'doc-10.6', 'doc-10.12', 'doc-10.4', 'doc-10.1', 'doc-10.5', 'doc-10.3', 'doc-10.13', 'doc-10.14', 'doc-10.9', 'doc-2.2', 'doc-2.1', 'doc-3.6', 'doc-3.7', 'doc-3.2', 'doc-3.5', 'doc-3.3', 'doc-3.1', 'doc-3.8', 'doc-4.1', 'doc-5.4', 'doc-5.2', 'doc-5.1', 'doc-5.3', 'doc-6.3', 'doc-6.4', 'doc-6.1', 'doc-6.2', 'doc-7.1', 'doc-8.3', 'doc-8.1', 'doc-8.2', 'doc-9.3', 'doc-9.4', 'doc-9.2', 'doc-9.5', 'doc-9.1']\n"
     ]
    }
   ],
   "source": [
    "doc_names = token_dataset.GetDocumentNames()\n",
    "doc_names.sort(key=lambda name: (int(name[4]), name[5]))\n",
    "\n",
    "print(doc_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc1372",
   "metadata": {},
   "source": [
    "### 1.1 Read Example Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "79d0ed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  doc-3.2  ********************\n",
      "Each morning , the files which have yet to be processed need to be checked , to make sure they are in order for the court hearing that day .\n",
      "If some files are missing , a search is initiated , otherwise the files can be physically tracked to the intended location .\n",
      "Once all the files are ready , these are handed to the Associate , and meantime the Judgeis Lawlist is distributed to the relevant people .\n",
      "Afterwards , the directions hearings are conducted .\n"
     ]
    }
   ],
   "source": [
    "doc_name = \"doc-3.2\"  # \"doc-1.2\"\n",
    "doc_number = relations_dataset.GetDocumentNumber(doc_name)\n",
    "print(f\"  {doc_name}  \".center(50, '*'))\n",
    "doc_text = token_dataset.GetDocumentText(doc_name)\n",
    "print(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cbe36154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** activities and NER labels (per sentences) ****\n",
      "[[['checked']], [['initiated'], ['tracked']], [['handed'], ['distributed']], [['conducted']]]\n",
      "[[('Each', 0, 'O'), ('morning', 1, 'O'), (',', 2, 'O'), ('the', 3, 'B-Activity Data'), ('files', 4, 'I-Activity Data'), ('which', 5, 'I-Activity Data'), ('have', 6, 'I-Activity Data'), ('yet', 7, 'I-Activity Data'), ('to', 8, 'I-Activity Data'), ('be', 9, 'I-Activity Data'), ('processed', 10, 'I-Activity Data'), ('need', 11, 'O'), ('to', 12, 'O'), ('be', 13, 'O'), ('checked', 14, 'B-Activity'), (',', 15, 'O'), ('to', 16, 'O'), ('make', 17, 'O'), ('sure', 18, 'O'), ('they', 19, 'O'), ('are', 20, 'O'), ('in', 21, 'O'), ('order', 22, 'O'), ('for', 23, 'O'), ('the', 24, 'O'), ('court', 25, 'O'), ('hearing', 26, 'O'), ('that', 27, 'O'), ('day', 28, 'O'), ('.', 29, 'O')], [('If', 0, 'B-XOR Gateway'), ('some', 1, 'B-Condition Specification'), ('files', 2, 'I-Condition Specification'), ('are', 3, 'I-Condition Specification'), ('missing', 4, 'I-Condition Specification'), (',', 5, 'O'), ('a', 6, 'B-Activity Data'), ('search', 7, 'I-Activity Data'), ('is', 8, 'O'), ('initiated', 9, 'B-Activity'), (',', 10, 'O'), ('otherwise', 11, 'B-XOR Gateway'), ('the', 12, 'B-Activity Data'), ('files', 13, 'I-Activity Data'), ('can', 14, 'O'), ('be', 15, 'O'), ('physically', 16, 'O'), ('tracked', 17, 'B-Activity'), ('to', 18, 'B-Further Specification'), ('the', 19, 'I-Further Specification'), ('intended', 20, 'I-Further Specification'), ('location', 21, 'I-Further Specification'), ('.', 22, 'O')], [('Once', 0, 'O'), ('all', 1, 'O'), ('the', 2, 'O'), ('files', 3, 'O'), ('are', 4, 'O'), ('ready', 5, 'O'), (',', 6, 'O'), ('these', 7, 'B-Activity Data'), ('are', 8, 'O'), ('handed', 9, 'B-Activity'), ('to', 10, 'O'), ('the', 11, 'B-Actor'), ('Associate', 12, 'I-Actor'), (',', 13, 'O'), ('and', 14, 'O'), ('meantime', 15, 'B-AND Gateway'), ('the', 16, 'B-Activity Data'), ('Judgeis', 17, 'I-Activity Data'), ('Lawlist', 18, 'I-Activity Data'), ('is', 19, 'O'), ('distributed', 20, 'B-Activity'), ('to', 21, 'O'), ('the', 22, 'B-Actor'), ('relevant', 23, 'I-Actor'), ('people', 24, 'I-Actor'), ('.', 25, 'O')], [('Afterwards', 0, 'O'), (',', 1, 'O'), ('the', 2, 'B-Activity Data'), ('directions', 3, 'I-Activity Data'), ('hearings', 4, 'I-Activity Data'), ('are', 5, 'O'), ('conducted', 6, 'B-Activity'), ('.', 7, 'O')]]\n",
      "************* same gateway relations *************\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 11\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['otherwise']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_activities = token_dataset.GetDocumentActivities(doc_name)\n",
    "print(\" activities and NER labels (per sentences) \".center(50, '*'))\n",
    "print(doc_activities)\n",
    "doc_sentence_ner_labels = relations_dataset.GetSentencesWithIdsAndNerTagLabels(doc_number)\n",
    "print(doc_sentence_ner_labels[:4])\n",
    "doc_relations = relations_dataset.GetRelations(doc_number)\n",
    "doc_flow_relations, doc_same_gateway_relations = doc_relations[labels.FLOW], doc_relations[labels.SAME_GATEWAY]\n",
    "\n",
    "print(\" same gateway relations \".center(50, '*'))\n",
    "for same_gateway_relation in doc_same_gateway_relations:\n",
    "    for key, value in same_gateway_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b84532",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9b8a529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0 Each morning , the files which have yet to be processed need to be checked , to make sure they are in order for the court hearing that day\n",
      "1 If some files are missing , a search is initiated , otherwise the files can be physically tracked to the intended location\n",
      "2 Once all the files are ready , these are handed to the Associate , and meantime the Judgeis Lawlist is distributed to the relevant people\n",
      "3 Afterwards , the directions hearings are conducted\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(doc_activities) # activities is 2 dim list (one per sentence)\n",
    "print(num_sentences)\n",
    "doc_sentences_raw = [sentence.strip() for sentence in doc_text.split(\".\") if sentence.strip() != \"\"]\n",
    "for i, s in enumerate(doc_sentences_raw):\n",
    "    print(i, s)\n",
    "assert num_sentences == len(doc_sentences_raw)  # check if number of extracted sentences == from dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a98c16",
   "metadata": {},
   "source": [
    "### 1.3 Filter Gateways Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5b7a2435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [('If', 0, 'B-XOR Gateway'), ('otherwise', 11, 'B-XOR Gateway')], [], []]\n",
      "[[], [], [('meantime', 15, 'B-AND Gateway')], []]\n"
     ]
    }
   ],
   "source": [
    "def filter_ner_labels(sentence_ner_labels, target_label):\n",
    "    return [[token for token in s_list if target_label in token[2]]\n",
    "                        for s_list in sentence_ner_labels]\n",
    "\n",
    "doc_xor_gateway = filter_ner_labels(doc_sentence_ner_labels, labels.XOR_GATEWAY)\n",
    "doc_and_gateway = filter_ner_labels(doc_sentence_ner_labels, labels.AND_GATEWAY)\n",
    "print(doc_xor_gateway)\n",
    "print(doc_and_gateway)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a1ea2",
   "metadata": {},
   "source": [
    "### 1.4 Filter Sequence Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9f1ecabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow relations involving XOR gateways 4; AND gateways 4; overall gateways 8; overall 10\n"
     ]
    }
   ],
   "source": [
    "def filter_flow_relations(flow_relations, entity_type_list):\n",
    "    \"\"\"\n",
    "    filter list of flow relations (single dictionaries) for source or target entity type = given entity tyoe\n",
    "    \"\"\"\n",
    "    return [flow_relation for flow_relation in flow_relations if flow_relation[labels.SOURCE_ENTITY_TYPE] in entity_type_list\n",
    "                                                               or flow_relation[labels.TARGET_ENTITY_TYPE] in entity_type_list]\n",
    "\n",
    "doc_flow_relations_xor = filter_flow_relations(doc_flow_relations, [labels.XOR_GATEWAY, labels.CONDITION_SPECIFICATION])\n",
    "doc_flow_relations_and = filter_flow_relations(doc_flow_relations, [labels.AND_GATEWAY])\n",
    "doc_flow_relations_gateways = filter_flow_relations(doc_flow_relations, [labels.XOR_GATEWAY, labels.AND_GATEWAY, labels.CONDITION_SPECIFICATION])\n",
    "print(f\"Flow relations involving XOR gateways {len(doc_flow_relations_xor)}; \"\\\n",
    "      f\"AND gateways {len(doc_flow_relations_and)}; overall gateways {len(doc_flow_relations_gateways)}; overall {len(doc_flow_relations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4b20d",
   "metadata": {},
   "source": [
    "### 1.5 Enrich activities with token index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "da4cae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_activity_tokens = []\n",
    "for i, (tokens, activities) in enumerate(zip(doc_sentence_ner_labels, doc_activities)):\n",
    "    sentence_activity_tokens = []\n",
    "    # note: activity is a list because it could consist of more words (neglect here)\n",
    "    for activity in activities:\n",
    "        activity_token_triple = [token_triple for token_triple in tokens if token_triple[0] == activity[0]][0]\n",
    "        sentence_activity_tokens.append((activity, activity_token_triple[1]))\n",
    "    doc_activity_tokens.append(sentence_activity_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4e627",
   "metadata": {},
   "source": [
    "## 2 Extract Gateways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d4d41",
   "metadata": {},
   "source": [
    "### 2.1 Key Word List\n",
    "#### A) take words from all existing gateways in PET dataset as gold list for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d0cb69a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR gold (15) ['either', 'for', 'for each patient for which', 'for the case', 'if', 'in case', 'in case of', 'in the case of', 'it can also happen that', 'or', 'otherwise', 'should', 'sometimes', 'under certain circumstances', 'whereas']\n",
      "AND gold (6) ['at the same time', 'in the meantime', 'meantime', 'two concurrent activities are triggered', 'whereas', 'while']\n"
     ]
    }
   ],
   "source": [
    "def get_gateway_key_words(dataset_gateway_list):\n",
    "    flattened = list(itertools.chain(*dataset_gateway_list))\n",
    "    phrases = [\" \".join(g).lower() for g in flattened]  # join phrases together if multiple words\n",
    "    unique = list(set(phrases))\n",
    "    unique.sort()\n",
    "    return unique\n",
    "\n",
    "xor_key_words_gold = get_gateway_key_words(token_dataset.GetXORGateways())\n",
    "and_key_words_gold = get_gateway_key_words(token_dataset.GetANDGateways())\n",
    "\n",
    "print(f\"XOR gold ({len(xor_key_words_gold)})\", xor_key_words_gold)\n",
    "print(f\"AND gold ({len(and_key_words_gold)})\", and_key_words_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efcd79",
   "metadata": {},
   "source": [
    "#### B) Curated List from Literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "744985cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR literature (14) ['either', 'if', 'if not', 'in case', 'in case of', 'only', 'only if', 'or', 'otherwise', 'till', 'unless', 'until', 'when', 'whether']\n",
      "AND literature (11) ['at the same time', 'concurrently', 'in addition to', 'in parallel', 'in parallel with this', 'in the meantime', 'meantime', 'meanwhile', 'simultaneously', 'whereas', 'while']\n"
     ]
    }
   ],
   "source": [
    "# Ferreira et al. 2017\n",
    "with open('data/keywords/literature_xor.txt') as f:\n",
    "    xor_key_words_literature = f.read().splitlines()\n",
    "    xor_key_words_literature.sort()\n",
    "\n",
    "with open('data/keywords/literature_and.txt') as f:\n",
    "    and_key_words_literature = f.read().splitlines()\n",
    "    and_key_words_literature.sort()\n",
    "\n",
    "print(f\"XOR literature ({len(xor_key_words_literature)})\", xor_key_words_literature)\n",
    "print(f\"AND literature ({len(and_key_words_literature)})\", and_key_words_literature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20306e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small company manufactures customized bicycles\n",
      "Whenever the sales department receives an order , a new process instance is created\n",
      "A member of the sales department can then reject or accept the order for a customized bike\n",
      "In the former case , the process instance is finished\n",
      "In the latter case , the storehouse and the engineering department are informed\n",
      "The storehouse immediately processes the part list of the order and checks the required quantity of each part\n",
      "If the part is available in-house , it is reserved\n",
      "If it is not available , it is back-ordered\n",
      "This procedure is repeated for each item on the part list\n",
      "In the meantime , the engineering department prepares everything for the assembling of the ordered bicycle\n",
      "If the storehouse has successfully reserved or back-ordered every item of the part list and the preparation activity has finished , the engineering department assembles the bicycle\n",
      "Afterwards , the sales department ships the bicycle to the customer and finishes the process instance\n"
     ]
    }
   ],
   "source": [
    "for s in doc_sentences_raw:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8d965",
   "metadata": {},
   "source": [
    "### 2.2 Extraction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a421ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gateways(sentence_list, key_words, target_gateway_label):\n",
    "    \"\"\"\n",
    "    extracts gateways in a key-word-based manner given a document structured in a list of sentences\n",
    "    if two phrases would match to a token (e.g. 'in the meantime' and 'meantime'), the longer phrase is extracted\n",
    "    target_gateway_label: str, must be 'XOR Gateway' or 'AND Gateway'\n",
    "    \n",
    "    result list is two dimensional -> list of tuples (word, position in sentence, tag) for each sentence\n",
    "    this produces the same structure as sentences and their NER labels are annotated in PET dataset\n",
    "    \"\"\"\n",
    "    gateways = []\n",
    "    benchmark_gateways = []\n",
    "    # sort key words descending by length of words in phrase\n",
    "    key_words.sort(key=lambda key_word_phrase: len(key_word_phrase.split(\" \")), reverse=True)\n",
    "\n",
    "    # 1) extract gateways\n",
    "    for s_idx, sentence in enumerate(sentence_list):\n",
    "        # print(f\" SENTENCE {s_idx} \".center(50, '-'))\n",
    "        # print(sentence_list[s_idx])\n",
    "        sentence_gateways = []\n",
    "        sentence_to_search = f\" {sentence.lower()} \"  # lowercase and wrap with spaces for search of key words\n",
    "        tokens = sentence.split(\" \")\n",
    "        tokens_lower = sentence.lower().split(\" \")\n",
    "        tokens_already_matched_with_key_phrase = []\n",
    "\n",
    "        # iterate over key phrases\n",
    "        for key_phrase in key_words:\n",
    "            key_phrase_to_search = f\" {key_phrase} \"\n",
    "\n",
    "            # if key phrase is in sentence, search index and extract\n",
    "            if key_phrase_to_search in sentence_to_search:\n",
    "                key_phrase_tokens = key_phrase.split(\" \")\n",
    "                \n",
    "                # check key phrase for every token\n",
    "                for t_idx, token in enumerate(tokens_lower):\n",
    "                    candidate = True\n",
    "                    # iterate over key phrase tokens in case of multiple world phrase\n",
    "                    for key_phrase_token_idx, key_phrase_token in enumerate(key_phrase_tokens):\n",
    "                        # check if token is not part of key phrase or token is already matched with another phrase\n",
    "                        # if yes, stop processing candidate\n",
    "                        if not tokens_lower[t_idx + key_phrase_token_idx] == key_phrase_token or \\\n",
    "                            t_idx + key_phrase_token_idx in tokens_already_matched_with_key_phrase:\n",
    "                            candidate = False\n",
    "                            break\n",
    "                    \n",
    "                    # add tokens to result only if all tokens are matched and not already part of a longer phrase\n",
    "                    if candidate:\n",
    "                        for i, key_phrase_token in enumerate(key_phrase_tokens):\n",
    "                            prefix = \"B\" if i == 0 else \"I\"\n",
    "                            # append tuples with extract information as in PET and process information about gateways to filter later\n",
    "                            sentence_gateways.append((tokens[t_idx + i], t_idx + i, f\"{prefix}-{labels.XOR_GATEWAY}\"))\n",
    "                            tokens_already_matched_with_key_phrase.append(t_idx + i)\n",
    "                            benchmark_gateways.append([tokens[t_idx + i] for i, x in enumerate(key_phrase_tokens)])\n",
    "\n",
    "        sentence_gateways.sort(key=lambda gateway_triple: gateway_triple[1])\n",
    "        gateways.append(sentence_gateways)\n",
    "\n",
    "    return gateways, benchmark_gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ea280708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR GATEWAYS\n",
      "0 [('for', 23, 'B-XOR Gateway')]\n",
      "1 [('If', 0, 'B-XOR Gateway'), ('otherwise', 11, 'B-XOR Gateway')]\n",
      "2 []\n",
      "3 []\n",
      "\n",
      "AND GATEWAYS\n",
      "0 []\n",
      "1 []\n",
      "2 [('meantime', 15, 'B-XOR Gateway')]\n",
      "3 []\n"
     ]
    }
   ],
   "source": [
    "# available key word lists: xor_key_words_gold, and_key_words_gold, xor_key_words_literature, and_key_words_literature\n",
    "o_xor_gateways, o_xor_gateways_benchmark = extract_gateways(doc_sentences_raw, xor_key_words_gold, labels.XOR_GATEWAY)\n",
    "o_and_gateways, o_and_gateways_benchmark = extract_gateways(doc_sentences_raw, and_key_words_gold, labels.AND_GATEWAY)\n",
    "\n",
    "print(\"XOR GATEWAYS\")\n",
    "for idx, sentence_gateways in enumerate(o_xor_gateways):\n",
    "    print(idx, sentence_gateways)\n",
    "print(\"\\nAND GATEWAYS\")\n",
    "for idx, sentence_gateways in enumerate(o_and_gateways):\n",
    "    print(idx, sentence_gateways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7a5cd",
   "metadata": {},
   "source": [
    "## 3 Extract Control Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc9175",
   "metadata": {},
   "source": [
    "### 3.1 Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd5d83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow_relation_representation(sentence_idx, token_idx, entity_type, entity, source=True):\n",
    "    if source:\n",
    "        return {\n",
    "            labels.SOURCE_SENTENCE_ID: sentence_idx,\n",
    "            labels.SOURCE_HEAD_TOKEN_ID: token_idx,\n",
    "            labels.SOURCE_ENTITY_TYPE: entity_type,\n",
    "            labels.SOURCE_ENTITY: entity\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            labels.TARGET_SENTENCE_ID: sentence_idx,\n",
    "            labels.TARGET_HEAD_TOKEN_ID: token_idx,\n",
    "            labels.TARGET_ENTITY_TYPE: entity_type,\n",
    "            labels.TARGET_ENTITY: entity\n",
    "        }\n",
    "        \n",
    "\n",
    "def merge_source_target_dicts(source_dict, target_dict):\n",
    "    return {**source_dict, **target_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6442043",
   "metadata": {},
   "source": [
    "### 3.2 Involving AND gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "051e3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) METHODS FOR EXTRACTING THE PREVIOUS (INCL. SECOND PREVIOUS) AND NEXT ACTIVITY\n",
    "\n",
    "def get_previous_activity(sentence_idx, token_idx, doc_activity_tokens, skip_first=False, one_already_found=False):\n",
    "    \"\"\"\n",
    "    search recursive for the second last previous activity from a start point defined by sentence_idx and token_idx\n",
    "    sentence_idx: sentence index where to start the search\n",
    "    token_idx: token index where to stat the search\n",
    "    doc_activity_tokens: list of activity lists (describes whole document)\n",
    "    skip_first: True if searching for the second previous activity, False (default) when searching for the previous activity\n",
    "    one_already_found: flag if one activity was already found and skipped for return in course of search for the second previous\n",
    "    \n",
    "    returns: triple of (sentence idx, token_idx, token)\n",
    "    \"\"\"\n",
    "    # search for activities left to the token in target sentence if token is given else in the whole\n",
    "    if token_idx is not None:\n",
    "        previous_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx] if a_t[1] < token_idx]\n",
    "    else:\n",
    "        previous_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx]]\n",
    "    \n",
    "    if previous_activities_sentence:\n",
    "        # return when just searching the first last activity OR when one was already found before\n",
    "        previous_activity = previous_activities_sentence[-1]\n",
    "        # A) base case: activity found\n",
    "        if not skip_first or one_already_found:\n",
    "            return (sentence_idx, previous_activity[1], previous_activity[0])\n",
    "        # B) recursive case: continue search for second previous activity at index of previous activity\n",
    "        else:\n",
    "            return get_previous_activity(sentence_idx, previous_activity[1], doc_activity_tokens, one_already_found=True)\n",
    "    # B) recursive case: continue search for previous activity in previous sentence\n",
    "    else:\n",
    "        next_sentence_idx = sentence_idx - 1\n",
    "        # no sentences any more to search\n",
    "        if next_sentence_idx == -1:\n",
    "            return None\n",
    "        # otherwise search recursively the previous sentence\n",
    "        else:\n",
    "            return get_previous_activity(next_sentence_idx, None, doc_activity_tokens, \n",
    "                                         skip_first=skip_first, one_already_found=one_already_found)\n",
    "\n",
    "def get_following_activity(sentence_idx, token_idx, doc_activity_tokens, skip_first=False, one_already_found=False):\n",
    "    \"\"\"\n",
    "    search recursive for the next following activity from a start point defined by sentence_idx and token_idx\n",
    "    sentence_idx: sentence index where to start the search\n",
    "    token_idx: token index where to stat the search\n",
    "    doc_activity_tokens: list of activity lists (describes whole document)\n",
    "    \n",
    "    returns: triple of (sentence idx, token_idx, token)\n",
    "    \"\"\"\n",
    "    # search for activities right to the token in target sentence if token is given else in the whole\n",
    "    if token_idx is not None:\n",
    "        following_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx] if a_t[1] > token_idx]\n",
    "    else:\n",
    "        following_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx]]\n",
    "\n",
    "    # if activities were found, take the first one\n",
    "    if following_activities_sentence:\n",
    "        # return when just searching the first following activity OR when one was already found before\n",
    "        following_activity = following_activities_sentence[0]\n",
    "        # 1a) base case: activity found\n",
    "        if not skip_first or one_already_found:\n",
    "            return (sentence_idx, following_activity[1], following_activity[0])\n",
    "        # 2a) recursive case: continue search for second following activity at index of following activity\n",
    "        else:\n",
    "            return get_following_activity(sentence_idx, following_activity[1], doc_activity_tokens,\n",
    "                                          one_already_found=True)\n",
    "\n",
    "    else:\n",
    "        next_sentence_idx = sentence_idx + 1\n",
    "        # 1b) base case: no sentences any more to search\n",
    "        if next_sentence_idx == len(doc_activity_tokens):\n",
    "            return None\n",
    "        # 2b) recursive case: continue search for following activity in following sentence\n",
    "        else:\n",
    "            return get_following_activity(next_sentence_idx, None, doc_activity_tokens,\n",
    "                                                skip_first=skip_first, one_already_found=one_already_found)\n",
    "\n",
    "        \n",
    "# 2) EXTRACT RELATIONS\n",
    "def extract_and_flow_relations(sentences, doc_activity_tokens, own_gateways):\n",
    "    \"\"\"\n",
    "    extract flow relations for already found AND gateways following the logic:\n",
    "    + for every gateway, to extract parallel branches, add relation to next activity after and before, because\n",
    "      thats the pattern how AND key phrases are usually used (oriented by rules of Ferreira et al. 2017)\n",
    "    + for each case, check over borders if not found in same sentence\n",
    "    + to extract the flow relation that points to the gateway merge point, take the second before\n",
    "    + Assumption: only one parallel gateway per sentence\n",
    "\n",
    "    sentences: list of sentences (used only for debugging)\n",
    "    doc_activity_tokens: list of activity tokens (word, idx) for each sentence\n",
    "    own_gateways: list of own extracted gateway for each sentence\n",
    "    \n",
    "    return: list of flow relations in source/target dict representation\n",
    "    \"\"\"\n",
    "    relations = []\n",
    "    \n",
    "    for s_idx, (sentence, activity_tokens, gateways) in enumerate(zip(sentences, doc_activity_tokens, own_gateways)):\n",
    "        if gateways:\n",
    "            # assume only one gateway\n",
    "            gateway_lead_token = gateways[0]\n",
    "            gateway_entity = [g[0] for g in gateways]\n",
    "\n",
    "            # 1) Find related activities (previous and following are concurrent activities; second previous the one before the gateway)\n",
    "            previous_activity = get_previous_activity(s_idx, gateway_lead_token[1], doc_activity_tokens)\n",
    "            second_previous_activity = get_previous_activity(s_idx, gateway_lead_token[1], doc_activity_tokens, skip_first=True)\n",
    "            following_activity = get_following_activity(s_idx, gateway_lead_token[1], doc_activity_tokens)\n",
    "\n",
    "            # 2) Get representations for flow object dictionaries\n",
    "            gateway_source_rep = get_flow_relation_representation(s_idx, gateway_lead_token[1], labels.AND_GATEWAY, \n",
    "                                                                  entity=gateway_entity, source=True)\n",
    "            gateway_target_rep = get_flow_relation_representation(s_idx, gateway_lead_token[1], labels.AND_GATEWAY, \n",
    "                                                                  entity=gateway_entity, source=False)\n",
    "\n",
    "            previous_activity_target_rep = get_flow_relation_representation(previous_activity[0], previous_activity[1], \n",
    "                                                                            labels.ACTIVITY, previous_activity[2], source=False)\n",
    "            second_previous_activity_target_rep = get_flow_relation_representation(second_previous_activity[0], second_previous_activity[1], \n",
    "                                                                                   labels.ACTIVITY, second_previous_activity[2], source=True)\n",
    "            following_activity_target_rep = get_flow_relation_representation(following_activity[0], following_activity[1], \n",
    "                                                                             labels.ACTIVITY, following_activity[2], source=False)\n",
    "\n",
    "            # 3) Create relations (second previous -> gateway; gateway -> previous; gateway -> following)\n",
    "            relations.append(merge_source_target_dicts(second_previous_activity_target_rep, gateway_target_rep))\n",
    "            relations.append(merge_source_target_dicts(gateway_source_rep, previous_activity_target_rep))\n",
    "            relations.append(merge_source_target_dicts(gateway_source_rep, following_activity_target_rep))\n",
    "    \n",
    "    return relations\n",
    "\n",
    "o_flow_relations_and = extract_and_flow_relations(doc_sentences_raw, doc_activity_tokens, o_and_gateways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0807c9a9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 17\n",
      "source-entity-type: Activity\n",
      "source-entity: ['tracked']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 15\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['meantime']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 15\n",
      "source-entity-type: AND Gateway\n",
      "source-entity: ['meantime']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 9\n",
      "target-entity-type: Activity\n",
      "target-entity: ['handed']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 15\n",
      "source-entity-type: AND Gateway\n",
      "source-entity: ['meantime']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 20\n",
      "target-entity-type: Activity\n",
      "target-entity: ['distributed']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, flow_relation in enumerate(o_flow_relations_and):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "257c7165",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 9\n",
      "source-entity-type: Activity\n",
      "source-entity: ['initiated']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 15\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['meantime']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 17\n",
      "source-entity-type: Activity\n",
      "source-entity: ['tracked']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 15\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['meantime']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 15\n",
      "source-entity-type: AND Gateway\n",
      "source-entity: ['meantime']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 9\n",
      "target-entity-type: Activity\n",
      "target-entity: ['handed']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 15\n",
      "source-entity-type: AND Gateway\n",
      "source-entity: ['meantime']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 20\n",
      "target-entity-type: Activity\n",
      "target-entity: ['distributed']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# control:\n",
    "for i, flow_relation in enumerate(doc_flow_relations_and):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea3914",
   "metadata": {},
   "source": [
    "### 3.2 Involving XOR gateways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e5d2f",
   "metadata": {},
   "source": [
    "#### Input B): Extracted Gateways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5b762d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [('for', 23, 'B-XOR Gateway')]\n",
      "1 [('If', 0, 'B-XOR Gateway'), ('otherwise', 11, 'B-XOR Gateway')]\n",
      "2 []\n",
      "3 []\n"
     ]
    }
   ],
   "source": [
    "for i, sentence_gateways in enumerate(o_xor_gateways):\n",
    "    print(i, sentence_gateways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32e20b",
   "metadata": {},
   "source": [
    "#### Input A): Activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "28d70a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Each morning , the files which have yet to be processed need to be checked , to make sure they are in order for the court hearing that day\n",
      "1 If some files are missing , a search is initiated , otherwise the files can be physically tracked to the intended location\n",
      "2 Once all the files are ready , these are handed to the Associate , and meantime the Judgeis Lawlist is distributed to the relevant people\n",
      "3 Afterwards , the directions hearings are conducted\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(doc_sentences_raw):\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "66f76530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [['checked']]\n",
      "1 [['initiated'], ['tracked']]\n",
      "2 [['handed'], ['distributed']]\n",
      "3 [['conducted']]\n"
     ]
    }
   ],
   "source": [
    "for i, sentence_activities in enumerate(doc_activities):\n",
    "    print(i, sentence_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33292b08",
   "metadata": {},
   "source": [
    "#### Gold Data: Flow Relations that involve AND Gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a97b3f64",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "source-head-sentence-ID: 0\n",
      "source-head-word-ID: 14\n",
      "source-entity-type: Activity\n",
      "source-entity: ['checked']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 1\n",
      "target-entity-type: Condition Specification\n",
      "target-entity: ['some', 'files', 'are', 'missing']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 1\n",
      "source-entity-type: Condition Specification\n",
      "source-entity: ['some', 'files', 'are', 'missing']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 9\n",
      "target-entity-type: Activity\n",
      "target-entity: ['initiated']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 9\n",
      "source-entity-type: Activity\n",
      "source-entity: ['initiated']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 15\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['meantime']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 11\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['otherwise']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 17\n",
      "target-entity-type: Activity\n",
      "target-entity: ['tracked']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 17\n",
      "source-entity-type: Activity\n",
      "source-entity: ['tracked']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 15\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['meantime']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 9\n",
      "source-entity-type: Activity\n",
      "source-entity: ['handed']\n",
      "target-head-sentence-ID: 3\n",
      "target-head-word-ID: 6\n",
      "target-entity-type: Activity\n",
      "target-entity: ['conducted']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 15\n",
      "source-entity-type: AND Gateway\n",
      "source-entity: ['meantime']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 9\n",
      "target-entity-type: Activity\n",
      "target-entity: ['handed']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 15\n",
      "source-entity-type: AND Gateway\n",
      "source-entity: ['meantime']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 20\n",
      "target-entity-type: Activity\n",
      "target-entity: ['distributed']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 20\n",
      "source-entity-type: Activity\n",
      "source-entity: ['distributed']\n",
      "target-head-sentence-ID: 3\n",
      "target-head-word-ID: 6\n",
      "target-entity-type: Activity\n",
      "target-entity: ['conducted']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_flow_relations))\n",
    "for i, flow_relation in enumerate(doc_flow_relations):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "31aac50b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source-head-sentence-ID: 0\n",
      "source-head-word-ID: 14\n",
      "source-entity-type: Activity\n",
      "source-entity: ['hands', 'out']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 1\n",
      "target-entity-type: Condition Specification\n",
      "target-entity: ['the', 'customer', 'decides', 'that', 'the', 'costs', 'are', 'acceptable']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 1\n",
      "source-entity-type: Condition Specification\n",
      "source-entity: ['the', 'customer', 'decides', 'that', 'the', 'costs', 'are', 'acceptable']\n",
      "target-head-sentence-ID: 3\n",
      "target-head-word-ID: 11\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['whereas']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 14\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['otherwise']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 16\n",
      "target-entity-type: Activity\n",
      "target-entity: ['takes']\n",
      "\n",
      "source-head-sentence-ID: 4\n",
      "source-head-word-ID: 11\n",
      "source-entity-type: Activity\n",
      "source-entity: ['tested']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n",
      "source-head-sentence-ID: 5\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 1\n",
      "target-entity-type: Condition Specification\n",
      "target-entity: ['an', 'error', 'is', 'detected']\n",
      "\n",
      "source-head-sentence-ID: 5\n",
      "source-head-word-ID: 1\n",
      "source-entity-type: Condition Specification\n",
      "source-entity: ['an', 'error', 'is', 'detected']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 10\n",
      "target-entity-type: Activity\n",
      "target-entity: ['executed']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, flow_relation in enumerate(doc_flow_relations_xor): # (doc_flow_relations_xor):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aa1878cb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [('for', 23, 'B-XOR Gateway')]\n",
      "1 [('If', 0, 'B-XOR Gateway'), ('otherwise', 11, 'B-XOR Gateway')]\n",
      "2 []\n",
      "3 []\n"
     ]
    }
   ],
   "source": [
    "for i, sentence_gateways in enumerate(o_xor_gateways):\n",
    "    print(i, sentence_gateways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2801ce97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Each morning , the files which have yet to be processed need to be checked , to make sure they are in order for the court hearing that day\n",
      "1 If some files are missing , a search is initiated , otherwise the files can be physically tracked to the intended location\n",
      "2 Once all the files are ready , these are handed to the Associate , and meantime the Judgeis Lawlist is distributed to the relevant people\n",
      "3 Afterwards , the directions hearings are conducted\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(doc_sentences_raw):\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2ea5bd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0, ['If'], ['if']), (1, 11, ['otherwise'], ['otherwise']), (4, 0, ['In', 'case', 'of'], ['in', 'case', 'of'])]\n",
      "source-head-sentence-ID: 0\n",
      "source-head-word-ID: 14\n",
      "source-entity-type: Activity\n",
      "source-entity: ['checked']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 9\n",
      "target-entity-type: Activity\n",
      "target-entity: ['initiated']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 9\n",
      "source-entity-type: Activity\n",
      "source-entity: ['initiated']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 9\n",
      "target-entity-type: Activity\n",
      "target-entity: ['handed']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 11\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['otherwise']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 17\n",
      "target-entity-type: Activity\n",
      "target-entity: ['tracked']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 17\n",
      "source-entity-type: Activity\n",
      "source-entity: ['tracked']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 9\n",
      "target-entity-type: Activity\n",
      "target-entity: ['handed']\n",
      "\n",
      "------------------------------\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 11\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['otherwise']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contradictory_gateways = [(['if'], ['otherwise']), (['if'], ['else'])]\n",
    "\n",
    "def _extract_exclusive_flows(doc_activity_tokens, extracted_gateways):\n",
    "    sequence_flows = []\n",
    "    same_gateway_relations = []\n",
    "    \n",
    "    # helper method only for this method\n",
    "    def preprocess_gateways(extracted_gateways):\n",
    "        \"\"\"\n",
    "        flatten gateways but keep sentence index; merge multiple gateway tokens into one gateway\n",
    "        :param extracted_gateways: gateways in PET format\n",
    "        :return: flattened gateway list filled with (sentence_idx, start_token_idx, ['Word', 'List'], ['word', 'list'])\n",
    "        \"\"\"\n",
    "        gateways = []\n",
    "        for sentence_idx, sentence_gateways in enumerate(extracted_gateways):\n",
    "            sentence_gateways_already_included = []\n",
    "            for i, gateway in enumerate(sentence_gateways):\n",
    "                if gateway not in sentence_gateways_already_included:\n",
    "                    gateway_tokens = [gateway[0]]\n",
    "                    start_token_idx = gateway[1]\n",
    "                    # append further tokens of same gateway ('I-' marked)\n",
    "                    I_index = i+1\n",
    "                    while I_index < len(sentence_gateways) and sentence_gateways[I_index][2].startswith('I-'):\n",
    "                        gateway_tokens.append(sentence_gateways[I_index][0])\n",
    "                        sentence_gateways_already_included.append(sentence_gateways[I_index])\n",
    "                        I_index += 1\n",
    "                    gateway_tokens_lower = [t.lower() for t in gateway_tokens]\n",
    "                    gateways.append((sentence_idx, start_token_idx, gateway_tokens, gateway_tokens_lower))    \n",
    "        return gateways\n",
    "    gateways = preprocess_gateways(extracted_gateways)\n",
    "    \n",
    "    for i in range(len(gateways)-1):\n",
    "        g1, g2 = gateways[i], gateways[i+1]\n",
    "        # check for every pair of following gateways if it fits to a gateway constellation with contradictory key words\n",
    "        for pattern_gateway_1, pattern_gateway_2 in contradictory_gateways:\n",
    "            if g1[3] == pattern_gateway_1 and g2[3] == pattern_gateway_2:\n",
    "                \n",
    "                # 1) find related activities\n",
    "                pa_g1 = get_previous_activity(g1[0], g1[1], doc_activity_tokens)\n",
    "                fa_g1 = get_following_activity(g1[0], g1[1], doc_activity_tokens)\n",
    "                fa_g2 = get_following_activity(g2[0], g2[1], doc_activity_tokens)\n",
    "                # check if following activities of g1 and g2 are the same -> if yes, the first branch is without activity\n",
    "                if fa_g1 == fa_g2:\n",
    "                    fa_g1 = None\n",
    "                ffa_g2 = get_following_activity(g2[0], g2[1], doc_activity_tokens, skip_first=True)\n",
    "                \n",
    "                \n",
    "                # 2) get dictionary representations\n",
    "                g1_source = get_flow_relation_representation(g1[0], g1[1], XOR_GATEWAY, g1[2], source=True)\n",
    "                g1_target = get_flow_relation_representation(g1[0], g1[1], XOR_GATEWAY, g1[2], source=False)\n",
    "                g2_source = get_flow_relation_representation(g2[0], g2[1], XOR_GATEWAY, g2[2], source=True)\n",
    "                g2_target = get_flow_relation_representation(g2[0], g2[1], XOR_GATEWAY, g2[2], source=False)\n",
    "                pa_g1_source = get_flow_relation_representation(pa_g1[0], pa_g1[1], ACTIVITY, pa_g1[2], source=True)\n",
    "                if fa_g1:\n",
    "                    fa_g1_source = get_flow_relation_representation(fa_g1[0], fa_g1[1], ACTIVITY, fa_g1[2], source=True)\n",
    "                    fa_g1_target = get_flow_relation_representation(fa_g1[0], fa_g1[1], ACTIVITY, fa_g1[2], source=False)\n",
    "                fa_g2_source = get_flow_relation_representation(fa_g2[0], fa_g2[1], ACTIVITY, fa_g2[2], source=True)\n",
    "                fa_g2_target = get_flow_relation_representation(fa_g2[0], fa_g2[1], ACTIVITY, fa_g2[2], source=False)\n",
    "                ffa_g2_target = get_flow_relation_representation(ffa_g2[0], ffa_g2[1], ACTIVITY, ffa_g2[2], source=False)\n",
    "                \n",
    "                \n",
    "                # 3.1) connect elements to sequence flows\n",
    "                # a) previous activity to first gateway -> split point\n",
    "                sequence_flows.append(merge_source_target_dicts(pa_g1_source, g1_target))\n",
    "                # b) gateway 1 to following activity and following activity to activity after gateway (second following of g2)\n",
    "                # if None because of empty branch then directly there\n",
    "                if fa_g1:\n",
    "                    sequence_flows.append(merge_source_target_dicts(g1_source, fa_g1_target))\n",
    "                    sequence_flows.append(merge_source_target_dicts(fa_g1_source, ffa_g2_target))\n",
    "                else:\n",
    "                    sequence_flows.append(merge_source_target_dicts(g1_source, ffa_g2_target))\n",
    "                # c) gateway 2 to following activity and following activity to activity after gateway (second following of g2)\n",
    "                sequence_flows.append(merge_source_target_dicts(g2_source, fa_g2_target))\n",
    "                sequence_flows.append(merge_source_target_dicts(fa_g2_source, ffa_g2_target))\n",
    "                \n",
    "                # 3.2) same gateway flows\n",
    "                same_gateway_relations.append(merge_source_target_dicts(g1_source, g2_target))\n",
    "                \n",
    "                    \n",
    "    # include as well the case when a pattern is detected but between the gateways no activity exists (doc-1.2)\n",
    "    # connect it then with first token after gateway (normally with condition between)\n",
    "    \n",
    "    return sequence_flows, same_gateway_relations\n",
    "\n",
    "\n",
    "\n",
    "test_gateways = o_xor_gateways.copy()  # o_xor_gateways_benchmark\n",
    "# test_gateways = test_gateways[1:]\n",
    "test_gateways[0] = []\n",
    "test_gateways.append([('In', 0, 'B-XOR Gateway'), ('case', 1, 'I-XOR Gateway'), ('of', 2, 'I-XOR Gateway')])\n",
    "o_flow_relations_xor, o_same_gateway_xor = _extract_concurrent_flows(doc_activity_tokens, test_gateways)\n",
    "\n",
    "for i, flow_relation in enumerate(o_flow_relations_xor):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()\n",
    "    \n",
    "print(\"-\"*30)\n",
    "    \n",
    "for i, flow_relation in enumerate(o_same_gateway_xor):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715c358",
   "metadata": {},
   "source": [
    "## 3.3 Involving Remaining Gold Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4bfb00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[['receives']]\n",
      "[['reject'], ['accept']]\n",
      "[]\n",
      "[['informed']]\n",
      "[['processes'], ['checks']]\n",
      "[['reserved']]\n",
      "[['back-ordered']]\n",
      "[]\n",
      "[['prepares']]\n",
      "[['assembles']]\n",
      "[['ships']]\n",
      "[['receives'], ['reject'], ['accept'], ['informed'], ['processes'], ['checks'], ['reserved'], ['back-ordered'], ['prepares'], ['assembles'], ['ships']]\n"
     ]
    }
   ],
   "source": [
    "# doc_activities = token_dataset.GetDocumentActivities(doc_name)\n",
    "for sentence_activities in doc_activities:\n",
    "    print(sentence_activities)\n",
    "\n",
    "# flatten activities to a list of document activities\n",
    "activities_flattened = [activitiy for sentence_activities in doc_activities for activitiy in sentence_activities]\n",
    "print(activities_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "406a20d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | source-entity: ['receives']\n",
      "0 | target-entity: ['reject']\n",
      "\n",
      "1 | source-entity: ['reject']\n",
      "1 | target-entity: ['accept']\n",
      "\n",
      "2 | source-entity: ['accept']\n",
      "2 | target-entity: ['informed']\n",
      "\n",
      "3 | source-entity: ['informed']\n",
      "3 | target-entity: ['processes']\n",
      "\n",
      "4 | source-entity: ['processes']\n",
      "4 | target-entity: ['checks']\n",
      "\n",
      "5 | source-entity: ['checks']\n",
      "5 | target-entity: ['reserved']\n",
      "\n",
      "6 | source-entity: ['reserved']\n",
      "6 | target-entity: ['back-ordered']\n",
      "\n",
      "7 | source-entity: ['back-ordered']\n",
      "7 | target-entity: ['prepares']\n",
      "\n",
      "8 | source-entity: ['prepares']\n",
      "8 | target-entity: ['assembles']\n",
      "\n",
      "9 | source-entity: ['assembles']\n",
      "9 | target-entity: ['ships']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_activity_flows(doc_activity_tokens):\n",
    "    activities_flattened = [(i, activitiy) for i, sentence_activities in enumerate(doc_activity_tokens) \n",
    "                            for activitiy in sentence_activities]\n",
    "    flow_relations = []\n",
    "    for i in range(len(activities_flattened) - 1):\n",
    "        s_idx_1, a1 = activities_flattened[i]\n",
    "        s_idx_2, a2 = activities_flattened[i+1]\n",
    "        if True:\n",
    "            flow_relations.append({labels.SOURCE_ENTITY: a1[0], labels.TARGET_ENTITY: a2[0]})\n",
    "        else:\n",
    "            a1 = get_flow_relation_representation(s_idx_1, a1[1], labels.ACTIVITY, a1[0], source=True)\n",
    "            a2 = get_flow_relation_representation(s_idx_2, a2[1], labels.ACTIVITY, a2[0], source=False)\n",
    "            flow_relations.append(merge_source_target_dicts(a1, a2))\n",
    "    return flow_relations\n",
    "\n",
    "\n",
    "gold_activity_flows = create_activity_flows(doc_activity_tokens)\n",
    "for i, flow_relation in enumerate(gold_activity_flows):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{i} | {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1df07",
   "metadata": {},
   "source": [
    "## 4 Evaluate Extraction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
