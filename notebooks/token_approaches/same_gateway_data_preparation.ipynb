{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00412f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent dir to sys path for import of modules\n",
    "import os\n",
    "import sys\n",
    "parentdir = os.path.abspath(os.path.join(os.path.abspath(''), os.pardir))\n",
    "sys.path.insert(0, parentdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba9ae107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import transformers\n",
    "from transformers import BatchEncoding\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from petreader.labels import *\n",
    "\n",
    "from labels import *\n",
    "from utils import config, generate_args_logdir\n",
    "from PetReader import pet_reader\n",
    "from token_approaches.SameGatewayClassifier import SameGatewayClassifier\n",
    "from token_approaches.token_data_augmentation import get_synonym_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dff72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "tf.random.set_seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114edb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = transformers.AutoTokenizer.from_pretrained(config[KEYWORDS_FILTERED_APPROACH][BERT_MODEL_NAME])\n",
    "assert isinstance(_tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9a5b3292",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Data Augmentation:Reload synonym_samples from C:\\Users\\janek\\Development\\Git\\master-thesis\\data/other/synonym_samples.pkl\n",
      "INFO:Data Augmentation:Reload synonym_samples from C:\\Users\\janek\\Development\\Git\\master-thesis\\data/other/synonym_samples.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 25 documents\n",
      "{'source-head-sentence-ID': 6, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['In', 'case'], 'target-head-sentence-ID': 7, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['In', 'case']}\n",
      "{'source-head-sentence-ID': 7, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['In', 'case'], 'target-head-sentence-ID': 7, 'target-head-word-ID': 26, 'target-entity-type': 'XOR Gateway', 'target-entity': ['otherwise']}\n",
      "{'source-head-sentence-ID': 12, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['In', 'case'], 'target-head-sentence-ID': 13, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['If']}\n",
      "{'source-head-sentence-ID': 15, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['If'], 'target-head-sentence-ID': 16, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['If']}\n",
      "{'source-head-sentence-ID': 21, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['For', 'the', 'case'], 'target-head-sentence-ID': 22, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['For', 'the', 'case']}\n",
      "{'source-head-sentence-ID': 22, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['For', 'the', 'case'], 'target-head-sentence-ID': 25, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['For', 'the', 'case']}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def _create_dataset(input_ids: tf.Tensor, attention_masks: tf.Tensor, indexes: tf.Tensor, labels: tf.Tensor)\\\n",
    "        -> tf.data.Dataset:\n",
    "    return tf.data.Dataset.from_tensor_slices(({'input_ids': input_ids, 'attention_mask': attention_masks, \"indexes\": indexes},\n",
    "                                               labels))\n",
    "\n",
    "\n",
    "def _shuffle_tokenization_data(input_ids: tf.Tensor, attention_masks: tf.Tensor, indexes: tf.Tensor, labels: tf.Tensor) \\\n",
    "                        -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    shuffle tensors of tokenized data; seed for shuffling is seed_general from args\n",
    "    :return: data tensors in same format but shuffled\n",
    "    \"\"\"\n",
    "    indices = tf.range(start=0, limit=input_ids.shape[0], dtype=tf.int32)\n",
    "    shuffled_indices = tf.random.shuffle(indices)\n",
    "    input_ids = tf.gather(input_ids, shuffled_indices)\n",
    "    attention_masks = tf.gather(attention_masks, shuffled_indices)\n",
    "    indexes = tf.gather(indexes, shuffled_indices)\n",
    "    labels = tf.gather(labels, shuffled_indices)\n",
    "    return input_ids, attention_masks, indexes, labels\n",
    "\n",
    "\n",
    "def _preprocess_gateway_pairs(gateway_type: str, context_sentences: int = 1, mode: str = CONCAT, n_gram: int = 1,\n",
    "                              use_synonyms: bool = False) -> Tuple[BatchEncoding, tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    extract and preprocess gateway pairs\n",
    "    :param gateway_type: type of gateway to extract data for (XOR_GATEWAY or AND_GATEWAY)\n",
    "    :param context_sentences: context size = number of sentences before and after first and second gateway to include\n",
    "    :param mode: flag how to include gateway information (by concatenating n_grams of gateways to text or by indexes)\n",
    "    :param n_gram: n of n_grams to include from gateways in CONCAT mode\n",
    "    :param use_synonyms: flag if synonym samples should be included;\n",
    "    :return: tokens as batch encoding, list of index pairs, list of labels\n",
    "    \"\"\"\n",
    "    # reload from cache if already exists\n",
    "#     cache_path = os.path.join(ROOT_DIR,\n",
    "#                               f\"data/other/same_gateway_data_{gateway_type}_{context_sentences}_{mode}_{n_gram}\")\n",
    "#     if os.path.exists(cache_path):\n",
    "#         tokens, indexes, labels = load_pickle(cache_path)\n",
    "#         logger.info(\"Reloaded same gateway data from cache\")\n",
    "#         return tokens, indexes, labels\n",
    "\n",
    "    if use_synonyms:\n",
    "        synonym_samples = get_synonym_samples()\n",
    "        synonyms_of_original_samples = get_synonyms_of_original_samples()\n",
    "\n",
    "    # lists to store results\n",
    "    texts = []  # context texts\n",
    "    n_gram_tuples = []  # tuples of gateway n_grams (only necessary for mode=CONCAT)\n",
    "    indexes = []  # index of gateway tokens in samples -> tuple\n",
    "    labels = []  # labels (0 or 1)\n",
    "\n",
    "    # A) GENERATE DATA\n",
    "    for i, doc_name in enumerate(pet_reader.document_names):\n",
    "        \n",
    "        if doc_name != 'doc-2.1':\n",
    "            continue\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f\"processed {i} documents\")\n",
    "\n",
    "        # 1) Prepare token data\n",
    "        text = pet_reader.get_doc_text(doc_name)\n",
    "        sample_ids = pet_reader.get_doc_sample_ids(doc_name)\n",
    "        doc_tokens = [list(zip(\n",
    "            [sample_id for i in range(len(pet_reader.token_dataset.GetTokens(sample_id)))],\n",
    "            [s_i for i in range(len(pet_reader.token_dataset.GetTokens(sample_id)))],\n",
    "            [i for i in range(len(pet_reader.token_dataset.GetTokens(sample_id)))],\n",
    "            pet_reader.token_dataset.GetTokens(sample_id),\n",
    "            pet_reader.token_dataset.GetNerTagLabels(sample_id))\n",
    "        ) for s_i, sample_id in enumerate(sample_ids)]\n",
    "        doc_tokens_flattened = list(itertools.chain(*doc_tokens))\n",
    "        doc_tokens_flattened = [(i,) + token_tuple for i, token_tuple in enumerate(doc_tokens_flattened)]\n",
    "        # token represented as tuple: (doc token index, sample id, sentence id, token id, token, ner-tag)\n",
    "\n",
    "        # 2) Identify gateway pairs\n",
    "        # filter for B- tokens, because I-s do not mark a new gateway of interest\n",
    "        gateway_tokens = [token_tuple for token_tuple in doc_tokens_flattened if token_tuple[5] == f\"B-{gateway_type}\"]\n",
    "        gateway_pairs = [(gateway_tokens[i], gateway_tokens[i + 1]) for i in range(len(gateway_tokens) - 1)]\n",
    "\n",
    "        # check if gateways are related\n",
    "        same_gateway_relations = pet_reader.get_doc_relations(doc_name)[SAME_GATEWAY]\n",
    "        for x in same_gateway_relations:\n",
    "            print(x)\n",
    "        pair_labels = []  # list of labels if gateway are related (1) or not (0)\n",
    "        # check if for pair of two subsequent gateways exists a same gateway relation\n",
    "        for g1, g2 in gateway_pairs:\n",
    "            same_gateway_found = False\n",
    "            for same_gateway_relation in same_gateway_relations:\n",
    "                if not same_gateway_found \\\n",
    "                        and g1[2] == same_gateway_relation[SOURCE_SENTENCE_ID] \\\n",
    "                        and g1[3] == same_gateway_relation[SOURCE_HEAD_TOKEN_ID] \\\n",
    "                        and g2[2] == same_gateway_relation[TARGET_SENTENCE_ID] \\\n",
    "                        and g2[3] == same_gateway_relation[TARGET_HEAD_TOKEN_ID]:\n",
    "                    pair_labels.append(1)\n",
    "                    same_gateway_found = True\n",
    "            if not same_gateway_found:\n",
    "                pair_labels.append(0)\n",
    "\n",
    "        # TODO: SYNONYM USAGE\n",
    "        # important variables so far: doc_tokens_flattened, gateway_pairs\n",
    "        # approach: multiply data so far if synonyms are involved in document\n",
    "        # concrete steps:\n",
    "        # - DONE record before the in the document involved sample IDs\n",
    "        # - DONE check if synonyms exist for one of the samples\n",
    "        # - DONE \"multiply\" document sample by all combinations of synonyms\n",
    "\n",
    "        # 3) prepare sample data\n",
    "        def get_token(token_tuple, gateways_sample_infos):\n",
    "                \"\"\"\n",
    "                returns the textual token of the given token tuple considering the different possible samples (normal or synonyms)\n",
    "                \"\"\"\n",
    "                if not gateways_sample_infos:\n",
    "                    return token_tuple[4]\n",
    "                \n",
    "                (g1_sample_id, g1_sample_id_original), (g2_sample_id, g2_sample_id_original) = gateways_sample_infos\n",
    "                \n",
    "                # check if both gateways are in same sentence and token is in the sentence\n",
    "                if g1_sample_id_original == g2_sample_id_original and token_tuple[1] == g1_sample_id_original:\n",
    "                    \n",
    "                    # prefer higher id to favor synonym samples (but all will be used once)\n",
    "                    sample_id_to_choose = max(g1_sample_id, g2_sample_id)\n",
    "#                      # if sample is original sample, take normal token\n",
    "#                     if g1_sample_id == g1_sample_id_original:\n",
    "#                         print(\"Take token from original sentence\", g1_sample_id, g2_sample_id, max(g1_sample_id, g2_sample_id))\n",
    "#                         return token_tuple[4]\n",
    "#                     # if not, take token at the same index from synonym sample\n",
    "#                     else:\n",
    "                    if sample_id_to_choose >= 500:\n",
    "                        return synonym_samples[sample_id_to_choose]['tokens'][token_tuple[3]]\n",
    "                    else:\n",
    "                        return token_tuple[4]\n",
    "                \n",
    "                # if token is in sentence of first gateway\n",
    "                elif token_tuple[1] == g1_sample_id_original:\n",
    "                    \n",
    "                    # if sample is original sample, take normal token\n",
    "                    if g1_sample_id == g1_sample_id_original:\n",
    "                        return token_tuple[4]\n",
    "                    # if not, take token at the same index from synonym sample\n",
    "                    else:\n",
    "                        return synonym_samples[g1_sample_id]['tokens'][token_tuple[3]]\n",
    "                \n",
    "                # if token is in sentence of second gateway\n",
    "                elif token_tuple[1] == g2_sample_id_original:\n",
    "                    # if sample is original sample, take normal token\n",
    "                    if g2_sample_id == g2_sample_id_original:\n",
    "                        return token_tuple[4]\n",
    "                    # if not, take token at the same index from synonym sample\n",
    "                    else:\n",
    "                        return synonym_samples[g2_sample_id]['tokens'][token_tuple[3]]\n",
    "                    \n",
    "                # if token is not in scope of gateway sentences but context -> return normal token\n",
    "                else:\n",
    "                    return token_tuple[4]\n",
    "        \n",
    "        def get_n_gram(token, gateways_sample_infos=None):\n",
    "            return ' '.join([get_token(token_tuple, gateways_sample_infos)\n",
    "                             for token_tuple in doc_tokens_flattened[max(token[0] - n_gram, 0):\n",
    "                                                                     min(token[0] + n_gram + 1, len(doc_tokens_flattened))]])\n",
    "\n",
    "        for (g1, g2), label in zip(gateway_pairs, pair_labels):\n",
    "            # Tokens/Text\n",
    "            num_s = context_sentences\n",
    "            sentences_in_scope = list(range(g1[2] - num_s if (g1[2] - num_s) > 0 else 0,\n",
    "                                            g2[2] + num_s + 1 if (g2[2] + num_s + 1) < len(sample_ids) else len(\n",
    "                                                sample_ids)))\n",
    "            if not use_synonyms:\n",
    "                # Tokens/Text\n",
    "                text_in_scope = ' '.join([token[4] for token in doc_tokens_flattened\n",
    "                                          if token[2] in sentences_in_scope])\n",
    "                texts.append((text_in_scope))\n",
    "                if mode == CONCAT:\n",
    "                    n_gram_tuples.append((get_n_gram(g1), get_n_gram(g2)))\n",
    "\n",
    "                # Indexes\n",
    "                indexes.append((g1[0], g2[0]))\n",
    "\n",
    "                # Label\n",
    "                labels.append(label)\n",
    "                \n",
    "            else:\n",
    "                print()\n",
    "                print(\"++++++++++++++++++++ New pair\", g1, g2)\n",
    "                print(sentences_in_scope)\n",
    "\n",
    "                print(\"Synonyms of sample id for g1:\", synonyms_of_original_samples[g1[1]])\n",
    "                print(\"Synonyms of sample id for g2:\", synonyms_of_original_samples[g2[1]])\n",
    "                # create lists of tuple of sample ids (normal and synonyms) for each gateway and original sample id\n",
    "                g1_sample_ids = [(sample_id, g1[1]) for sample_id in [g1[1]] + synonyms_of_original_samples[g1[1]]]\n",
    "                g2_sample_ids = [(sample_id, g2[1]) for sample_id in [g2[1]] + synonyms_of_original_samples[g2[1]]]\n",
    "                print(g1_sample_ids)\n",
    "                print(g2_sample_ids)\n",
    "                print(list(itertools.product(*[g1_sample_ids, g2_sample_ids])))\n",
    "\n",
    "                if g1[1] == g2[1]:\n",
    "                    print(\"AAA\", [(g1[1], g1[1])])\n",
    "                    print(\"AAA\", [(g1[1], g1[1])] + [(s, g1[1]) for s in synonyms_of_original_samples[g1[1]]])\n",
    "                    gateway_sample_combinations = itertools.product(*[[(g1[1], g1[1])],\n",
    "                                                                      [(g1[1], g1[1])] + [(s, g1[1]) for s in synonyms_of_original_samples[g1[1]]]])\n",
    "                else:\n",
    "                    g1_sample_ids = [(sample_id, g1[1]) for sample_id in [g1[1]] + synonyms_of_original_samples[g1[1]]]\n",
    "                    g2_sample_ids = [(sample_id, g2[1]) for sample_id in [g2[1]] + synonyms_of_original_samples[g2[1]]]\n",
    "                    gateway_sample_combinations = itertools.product(*[g1_sample_ids, g2_sample_ids])\n",
    "\n",
    "                # iterate over pairs of gateway sentences (multiple possible if synonyms are used)\n",
    "                for gateways_sample_infos in gateway_sample_combinations:\n",
    "                    #print(gateways_sample_infos)\n",
    "                    text_in_scope = ' '.join([get_token(token, gateways_sample_infos) for token in doc_tokens_flattened\n",
    "                                              if token[2] in sentences_in_scope])\n",
    "\n",
    "                    texts.append(text_in_scope)\n",
    "                    if mode == CONCAT:\n",
    "                        n_gram_tuples.append((get_n_gram(g1, gateways_sample_infos), get_n_gram(g2, gateways_sample_infos)))\n",
    "\n",
    "\n",
    "                    print(text_in_scope)\n",
    "                    print(get_n_gram(g1, gateways_sample_infos), '||', get_n_gram(g2, gateways_sample_infos))\n",
    "                    print('\\n' + 100 * '-' + '\\n')\n",
    "\n",
    "\n",
    "                    # Indexes\n",
    "                    indexes.append((g1[0], g2[0]))\n",
    "\n",
    "                    # Label\n",
    "                    labels.append(label)\n",
    "                \n",
    "        print(len(labels))\n",
    "    return\n",
    "\n",
    "    # B) TOKENIZE TEXT\n",
    "    if mode == INDEX:\n",
    "        tokens = _tokenizer(texts, padding=True, return_tensors='tf')\n",
    "    elif mode == CONCAT:\n",
    "        # tokenize text & pairs seperately, because it is not possible to concat triple\n",
    "        text_tokens = _tokenizer(texts, padding=True, return_tensors='tf')\n",
    "        n_gram_tokens = _tokenizer(n_gram_tuples, padding=True, return_tensors=\"tf\")\n",
    "        # concat manually after (cut the CLS token of the second pair / n_grams)\n",
    "        concatted_input_ids = tf.concat([text_tokens[\"input_ids\"], n_gram_tokens[\"input_ids\"][:, 1:]], axis=1)\n",
    "        concatted_attention_masks = tf.concat([text_tokens[\"attention_mask\"], n_gram_tokens[\"attention_mask\"][:, 1:]],\n",
    "                                              axis=1)\n",
    "        tokens = transformers.BatchEncoding(\n",
    "            {\"input_ids\": concatted_input_ids, \"attention_mask\": concatted_attention_masks})\n",
    "    else:\n",
    "        raise ValueError(f\"mode must be {INDEX} or {CONCAT}\")\n",
    "        \n",
    "        \n",
    "# _preprocess_gateway_pairs(XOR_GATEWAY, context_sentences=1, mode=CONCAT, n_gram=1, use_synonyms=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e913c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Data Augmentation:Reload synonym_samples from C:\\Users\\janek\\Development\\Git\\master-thesis\\data/other/synonym_samples.pkl\n",
      "INFO:Data Augmentation:Reload synonym_samples from C:\\Users\\janek\\Development\\Git\\master-thesis\\data/other/synonym_samples.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{399: [500, 501, 502, 503, 504, 505, 506, 507], 42: [508], 66: [509, 510, 511, 512, 513, 514, 515, 516], 371: [517, 518, 519], 7: [520, 521, 522, 523, 524, 525, 526, 527], 113: [528], 54: [529], 43: [\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_synonyms_of_original_samples():\n",
    "    synonyms = get_synonym_samples()\n",
    "    synonyms_of_original_samples = {}  # dict with {original sample id: list of synonym ids}\n",
    "    # record synonyms of sample ids\n",
    "    for synonym_id, synonym_dict in synonyms.items():\n",
    "        if synonym_dict['original_sample_number'] in synonyms_of_original_samples:\n",
    "            synonyms_of_original_samples[synonym_dict['original_sample_number']].append(synonym_id)\n",
    "        else:\n",
    "            synonyms_of_original_samples[synonym_dict['original_sample_number']] = [synonym_id]\n",
    "    # add empty lists for samples without synonyms\n",
    "    for sample_id in pet_reader.token_dataset.GetRandomizedSampleNumbers():\n",
    "        if sample_id not in synonyms_of_original_samples:\n",
    "            synonyms_of_original_samples[sample_id] = []\n",
    "    return synonyms_of_original_samples\n",
    "\n",
    "synonyms_of_original_samples = get_synonyms_of_original_samples()\n",
    "synonyms = get_synonym_samples()\n",
    "print(str(synonyms_of_original_samples)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a21e843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 documents\n",
      "processed 5 documents\n",
      "processed 10 documents\n",
      "processed 15 documents\n",
      "processed 20 documents\n",
      "processed 25 documents\n",
      "processed 30 documents\n",
      "processed 35 documents\n",
      "processed 40 documents\n",
      "8 3\n",
      "9 2\n",
      "9 2\n",
      "9 2\n",
      "9 2\n"
     ]
    }
   ],
   "source": [
    "def create_same_gateway_cls_dataset_cv(args: argparse.Namespace = None, shuffle: bool = True, batch_size: int = None,\n",
    "                                       context_sentences: int = 1, gateway_type: str = XOR_GATEWAY,\n",
    "                                       mode: str = CONCAT, n_gram: int = 1):\n",
    "    tokens, indexes, labels = _preprocess_gateway_pairs(context_sentences=context_sentences, gateway_type=gateway_type,\n",
    "                                                        mode=mode, n_gram=n_gram)\n",
    "    input_ids, attention_masks = tokens['input_ids'], tokens['attention_mask']\n",
    "    if shuffle:\n",
    "        input_ids, attention_masks, indexes, labels = _shuffle_tokenization_data(input_ids, attention_masks, indexes, labels)\n",
    "    \n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=5)\n",
    "\n",
    "    # create folds\n",
    "    folded_datasets = []\n",
    "    for train, test in kfold.split(input_ids):\n",
    "        train_tf_dataset = _create_dataset(tf.gather(input_ids, train),\n",
    "                                           tf.gather(attention_masks, train),\n",
    "                                           tf.gather(indexes, train),\n",
    "                                           tf.gather(labels, train))\n",
    "        dev_tf_dataset = _create_dataset(tf.gather(input_ids, test),\n",
    "                                         tf.gather(attention_masks, test),\n",
    "                                         tf.gather(indexes, test),\n",
    "                                         tf.gather(labels, test))\n",
    "        if batch_size:\n",
    "            train_tf_dataset = train_tf_dataset.batch(batch_size)\n",
    "            dev_tf_dataset = dev_tf_dataset.batch(batch_size)\n",
    "        folded_datasets.append((train_tf_dataset, dev_tf_dataset))\n",
    "    \n",
    "    return folded_datasets\n",
    "    \n",
    "folded_datasets = create_same_gateway_cls_dataset_cv(batch_size=8, context_sentences=1, gateway_type=XOR_GATEWAY, \n",
    "                                                     mode=CONCAT, n_gram=1)\n",
    "\n",
    "for train, dev in folded_datasets:\n",
    "    print(len(train), len(dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb69c273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 documents\n",
      "processed 5 documents\n",
      "processed 10 documents\n",
      "processed 15 documents\n",
      "processed 20 documents\n",
      "processed 25 documents\n",
      "processed 30 documents\n",
      "processed 35 documents\n",
      "processed 40 documents\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "def create_same_gateway_cls_dataset_full(args: argparse.Namespace = None, shuffle: bool = True, batch_size: int = None,\n",
    "                                       context_sentences: int = 1, gateway_type: str = XOR_GATEWAY,\n",
    "                                       mode: str = CONCAT, n_gram: int = 1, use_synonyms=False):\n",
    "    tokens, indexes, labels = _preprocess_gateway_pairs(context_sentences=context_sentences, gateway_type=gateway_type,\n",
    "                                                        mode=mode, n_gram=n_gram)\n",
    "    input_ids, attention_masks = tokens['input_ids'], tokens['attention_mask']\n",
    "    if shuffle:\n",
    "        input_ids, attention_masks, indexes, labels = _shuffle_tokenization_data(input_ids, attention_masks, indexes, labels)\n",
    "    dataset = _create_dataset(input_ids, attention_masks, indexes, labels)\n",
    "    \n",
    "    if batch_size:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "datasets_full = create_same_gateway_cls_dataset_full(batch_size=None, context_sentences=1, gateway_type=XOR_GATEWAY, \n",
    "                                                     mode=INDEX, n_gram=1, use_synonyms=True)\n",
    "print(len(datasets_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131ba2f",
   "metadata": {},
   "source": [
    "## Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d07421ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 41, 1: 40})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "labels = [x[1].numpy() for x in datasets_full]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8e8896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'type'>\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(type(Counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9677f",
   "metadata": {},
   "source": [
    "## Dummy Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48d55857",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Standard params\n",
    "parser.add_argument(\"--batch_size\", default=8, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--epochs\", default=1, type=int, help=\"Number of epochs.\")\n",
    "parser.add_argument(\"--seed_general\", default=42, type=int, help=\"Random seed.\")\n",
    "# routine params\n",
    "parser.add_argument(\"--routine\", default=\"cv\", type=str, help=\"Simple split training 'sp', cross validation 'cv' or \"\n",
    "                                                              \"full training without validation 'ft'.\")\n",
    "parser.add_argument(\"--folds\", default=2, type=int, help=\"Number of folds in cross validation routine.\")\n",
    "parser.add_argument(\"--store_weights\", default=False, type=bool, help=\"Flag if best weights should be stored.\")\n",
    "# Architecture / data params\n",
    "parser.add_argument(\"--context_size\", default=1, type=int, help=\"Number of sentences around to include in text.\")\n",
    "parser.add_argument(\"--mode\", default=CONCAT, type=str, help=\"How to include gateway information.\")\n",
    "parser.add_argument(\"--n_gram\", default=1, type=int, help=\"Number of tokens to include for gateway in CONCAT mode.\")\n",
    "\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)\n",
    "args.logdir = generate_args_logdir(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5afd361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"same_gateway_classifier\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_2[0][0]',                \n",
      " BertModel)                     ast_hidden_state=(N               'input_1[0][0]']                \n",
      "                                one, None, 768),                                                  \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_distil_bert_model[0][0]']   \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 768)          0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            769         ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,363,649\n",
      "Trainable params: 66,363,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 124s 13s/step - loss: 0.8263 - binary_accuracy: 0.4375 - val_loss: 0.6622 - val_binary_accuracy: 0.6471\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 100s 13s/step - loss: 0.7770 - binary_accuracy: 0.4531 - val_loss: 0.6622 - val_binary_accuracy: 0.6471\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 100s 13s/step - loss: 0.7922 - binary_accuracy: 0.4531 - val_loss: 0.6622 - val_binary_accuracy: 0.6471\n"
     ]
    }
   ],
   "source": [
    "train_dataset, dev_dataset = folded_datasets[0][0], folded_datasets[0][1]\n",
    "model = SameGatewayClassifier(args, bert_model=None, mode=CONCAT, train_size=len(train_dataset))\n",
    "history = model.fit(train_dataset, epochs=3, validation_data=dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03430029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"same_gateway_classifier_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model_9 (TFDist  TFBaseModelOutput(l  66362880   ['input_29[0][0]',               \n",
      " ilBertModel)                   ast_hidden_state=(N               'input_28[0][0]']               \n",
      "                                one, None, 768),                                                  \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  (None, 768)         0           ['tf_distil_bert_model_9[0][0]'] \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dropout_199 (Dropout)          (None, 768)          0           ['tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " input_30 (InputLayer)          [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            769         ['dropout_199[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,363,649\n",
      "Trainable params: 66,363,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 94s 11s/step - loss: 0.7247 - binary_accuracy: 0.4844 - precision: 0.4091 - recall: 0.3103 - val_loss: 0.7051 - val_binary_accuracy: 0.4706 - val_precision: 0.7500 - val_recall: 0.2727\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 82s 10s/step - loss: 0.6958 - binary_accuracy: 0.5469 - precision: 0.5000 - recall: 0.4828 - val_loss: 0.7051 - val_binary_accuracy: 0.4706 - val_precision: 0.7500 - val_recall: 0.2727\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 82s 10s/step - loss: 0.6805 - binary_accuracy: 0.5156 - precision: 0.4643 - recall: 0.4483 - val_loss: 0.7051 - val_binary_accuracy: 0.4706 - val_precision: 0.7500 - val_recall: 0.2727\n"
     ]
    }
   ],
   "source": [
    "model = SameGatewayClassifier(args, bert_model=None, mode=CONCAT, train_size=len(train_dataset))\n",
    "\n",
    "optimizer, lr_schedule = transformers.create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=(len(train_dataset) // args.batch_size) * args.epochs,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(name=\"precision\"), \n",
    "                      tf.keras.metrics.Recall(name=\"recall\")])  # , tfa.metrics.F1Score(num_classes=1)\n",
    "    \n",
    "history = model.fit(train_dataset, epochs=3, validation_data=dev_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
