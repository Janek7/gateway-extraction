{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc21820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# add parent dir to sys path for import of modules\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "# find recursively the project root dir\n",
    "parent_dir = str(os.getcwdb())\n",
    "while not os.path.exists(os.path.join(parent_dir, \"README.md\")):\n",
    "    parent_dir = os.path.abspath(os.path.join(parent_dir, os.pardir))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba46e7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\janek\\AppData\\Local\\Temp\\ipykernel_24868\\2336865246.py:13: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Utilities:Loaded config: {'general-seed': 42, 'keywords-filtered-approach': {'bert-model-name': 'distilbert-base-uncased', 'label-set': 'all', 'label-number': 9, 'other-labels-weight': 0.1, 'num-labels': 9}, 'same-gateway-classifier': {'context_label_length': 350}, 'synonym-samples-start-number': 500}\n",
      "INFO:PetReader:Reload pet_reader from C:\\Users\\janek\\Development\\Git\\master-thesis\\data/other/pet_reader.pkl\n",
      "INFO:Data Augmentation:Reload synonym_samples from C:\\Users\\janek\\Development\\Git\\master-thesis\\data/other/synonym_samples.pkl\n",
      "INFO:Data Augmentation:Reload synonym_samples from C:\\Users\\janek\\Development\\Git\\master-thesis\\data/other/synonym_samples.pkl\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os.path\n",
    "from typing import List, Tuple, Dict\n",
    "import argparse\n",
    "\n",
    "from petreader.labels import *\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "# fix for exception \"Attempting to perform BLAS operation using StreamExecutor without BLAS support\"\n",
    "config = tf.compat.v1.ConfigProto(gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8))\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "\n",
    "from token_approaches.SameGatewayClassifier import SameGatewayClassifier\n",
    "from token_approaches.same_gateway_data_preparation import preprocess_gateway_pair\n",
    "from Ensemble import Ensemble\n",
    "from token_approaches.KeywordsApproach import KeywordsApproach\n",
    "from PetReader import pet_reader\n",
    "from utils import config, set_seeds, NumpyEncoder\n",
    "from labels import *\n",
    "\n",
    "logger = logging.getLogger('Keywords Same Gateway Filtered Approach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a6bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Standard params\n",
    "parser.add_argument(\"--batch_size\", default=8, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--epochs\", default=1, type=int, help=\"Number of epochs.\")\n",
    "parser.add_argument(\"--seed_general\", default=42, type=int, help=\"Random seed.\")\n",
    "parser.add_argument(\"--ensemble\", default=True, type=bool, help=\"Use ensemble learning with config.json seeds.\")\n",
    "parser.add_argument(\"--seeds_ensemble\", default=\"0-1\", type=str, help=\"Random seed range to use for ensembles\")\n",
    "# routine params\n",
    "parser.add_argument(\"--routine\", default=\"cv\", type=str, help=\"Simple split training 'sp', cross validation 'cv' or \"\n",
    "                                                              \"full training without validation 'ft'.\")\n",
    "parser.add_argument(\"--folds\", default=2, type=int, help=\"Number of folds in cross validation routine.\")\n",
    "parser.add_argument(\"--store_weights\", default=False, type=bool, help=\"Flag if best weights should be stored.\")\n",
    "# Data params\n",
    "parser.add_argument(\"--gateway\", default=XOR_GATEWAY, type=str, help=\"Type of gateway to classify\")\n",
    "parser.add_argument(\"--use_synonyms\", default=False, type=str, help=\"Include synonym samples.\")\n",
    "parser.add_argument(\"--context_size\", default=1, type=int, help=\"Number of sentences around to include in text.\")\n",
    "parser.add_argument(\"--mode\", default=CONTEXT_NGRAM, type=str, help=\"How to include gateway information.\")\n",
    "parser.add_argument(\"--n_gram\", default=1, type=int, help=\"Number of tokens to include for gateway in CONCAT mode.\")\n",
    "parser.add_argument(\"--activity_masking\", default=NOT, type=str, help=\"How to include activity data.\")\n",
    "# Architecture params\n",
    "parser.add_argument(\"--dropout\", default=0.2, type=float, help=\"Dropout rate.\")\n",
    "parser.add_argument(\"--hidden_layer\", default=\"32\", type=str, help=\"Hidden layer sizes sep. by '-'\")\n",
    "parser.add_argument(\"--learning_rate\", default=2e-5, type=float, help=\"Learning rate.\")\n",
    "parser.add_argument(\"--warmup\", default=0, type=int, help=\"Number of warmup steps.\")\n",
    "\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17f374c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class SameGatewayClassifier(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    binary classification model to classify if two gateways belong to the same gateway construct\n",
    "    \"\"\"\n",
    "    def __init__(self, args: argparse.Namespace, bert_model, train_size: int = None):\n",
    "        self.args = args\n",
    "\n",
    "        # A) ARCHITECTURE\n",
    "        inputs = {\n",
    "            \"input_ids\": tf.keras.layers.Input(shape=[None], dtype=tf.int32),\n",
    "            \"attention_mask\": tf.keras.layers.Input(shape=[None], dtype=tf.int32),\n",
    "            \"indexes\": tf.keras.layers.Input(shape=[2], dtype=tf.int32),\n",
    "            \"context_labels\": tf.keras.layers.Input(shape=[config[SAME_GATEWAY_CLASSIFIER][CONTEXT_LABEL_LENGTH]],\n",
    "                                                    dtype=tf.int32),\n",
    "        }\n",
    "\n",
    "        if not bert_model:\n",
    "            bert_model = transformers.TFAutoModel.from_pretrained(config[KEYWORDS_FILTERED_APPROACH][BERT_MODEL_NAME])\n",
    "        # includes one dense layer with linear activation function\n",
    "        bert_output = bert_model({\"input_ids\": inputs[\"input_ids\"],\n",
    "                                  \"attention_mask\": inputs[\"attention_mask\"]}).last_hidden_state\n",
    "        # extract cls token for every sample\n",
    "        cls_token = bert_output[:, 0]\n",
    "        dropout1 = tf.keras.layers.Dropout(args.dropout)(cls_token)\n",
    "\n",
    "        # for only textual modes add immediately output layers\n",
    "        if args.mode == CONTEXT_NGRAM or args.mode == N_GRAM:\n",
    "            predictions = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(dropout1)\n",
    "\n",
    "        # for modes that include more features, combine them with hidden layer(s) with BERT output\n",
    "        elif args.mode in [CONTEXT_INDEX, CONTEXT_LABELS_NGRAM, CONTEXT_TEXT_AND_LABELS_NGRAM]:\n",
    "            if args.mode == CONTEXT_INDEX:\n",
    "                additional_information = inputs[\"indexes\"]\n",
    "            elif args.mode in [CONTEXT_LABELS_NGRAM, CONTEXT_TEXT_AND_LABELS_NGRAM]:\n",
    "                additional_information = inputs[\"context_labels\"]\n",
    "            additional_information = tf.cast(additional_information, tf.float32)\n",
    "            hidden = tf.keras.layers.Concatenate()([dropout1, additional_information])\n",
    "            for hidden_layer_size in args.hidden_layer.split(\"-\"):\n",
    "                hidden = tf.keras.layers.Dense(int(hidden_layer_size), activation=tf.nn.relu)(hidden)\n",
    "                hidden = tf.keras.layers.Dropout(args.dropout)(hidden)\n",
    "            predictions = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(hidden)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"mode must be {N_GRAM}, {CONTEXT_INDEX}, {CONTEXT_NGRAM}, {CONTEXT_LABELS_NGRAM} or\"\n",
    "                             f\" {CONTEXT_TEXT_AND_LABELS_NGRAM}\")\n",
    "\n",
    "        super().__init__(inputs=inputs, outputs=predictions)\n",
    "\n",
    "        # B) COMPILE (only needed when training is intended)\n",
    "        optimizer, lr_schedule = transformers.create_optimizer(\n",
    "            init_lr=args.learning_rate,\n",
    "            num_train_steps=(train_size // args.batch_size) * args.epochs,\n",
    "            weight_decay_rate=0.01,\n",
    "            num_warmup_steps=args.warmup,\n",
    "        )\n",
    "\n",
    "        self.compile(optimizer=optimizer,\n",
    "                     loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                     metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "                              tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")])\n",
    "\n",
    "        # self.summary()\n",
    "\n",
    "    def classify_pair(self, doc_name, g1, g2) -> bool:\n",
    "        \"\"\"\n",
    "        create predictions for given data\n",
    "        :param doc_name: document where gateways belong to\n",
    "        :param g1: first gateway of pair to evaluate\n",
    "        :param g2: second gateway of pair to evaluate\n",
    "        :return: true or false (threshold 0.5 because of binary classification head)\n",
    "        \"\"\"\n",
    "        # preprocess data\n",
    "        tokens, indexes, context_labels = preprocess_gateway_pair(self.args, doc_name, g1, g2)\n",
    "        inputs = {\n",
    "            \"input_ids\": tokens[\"input_ids\"],\n",
    "            \"attention_mask\": tokens[\"attention_mask\"],\n",
    "            \"indexes\": indexes,\n",
    "            \"context_labels\": context_labels\n",
    "        }\n",
    "        p = super().predict(inputs)[0][0]\n",
    "        result = p > 0.5\n",
    "        print(p, type(p), result)\n",
    "        logger.info(f\"Custom predict in {doc_name}: {g1} - {g2} -> {result}\")\n",
    "        return result\n",
    "\n",
    "sgc = SameGatewayClassifier(args, None, train_size=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15670d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Utilities:Set seeds to 42 (caller: Set first seed)\n",
      "INFO:Utilities:Load keywords 'literature' ...\n",
      "INFO:Utilities:Loaded 15 XOR and 11 AND keywords (literature)\n",
      "INFO:Utilities:Used XOR keywords: ['either', 'else', 'if', 'if not', 'in case', 'in case of', 'only', 'only if', 'or', 'otherwise', 'till', 'unless', 'until', 'when', 'whether']\n",
      "INFO:Utilities:Used AND keywords: ['at the same time', 'concurrently', 'in addition to', 'in parallel', 'in parallel with this', 'in the meantime', 'meantime', 'meanwhile', 'simultaneously', 'whereas', 'while']\n",
      "INFO:Utilities:Loaded 14 gold pairs of contradictory keywords\n",
      "INFO:Utilities:Set seeds to 42 (caller: Reset after initialization of SameGatewayClassifierEnsemble)\n",
      "INFO:Keyword Approach:10 gateway flows\n",
      "INFO:Keyword Approach:10 gold activity flows\n",
      "INFO:Keyword Approach:15 doc flows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'element': (2, 9, ['or'], ['or']), 'source': {'source-head-sentence-ID': 2, 'source-head-word-ID': 9, 'source-entity-type': 'XOR Gateway', 'source-entity': ['or']}, 'target': {'target-head-sentence-ID': 2, 'target-head-word-ID': 9, 'target-entity-type': 'XOR Gateway', 'target-entity': ['or']}}, {'element': (6, 0, ['If'], ['if']), 'source': {'source-head-sentence-ID': 6, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['If']}, 'target': {'target-head-sentence-ID': 6, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['If']}}, {'element': (7, 0, ['If'], ['if']), 'source': {'source-head-sentence-ID': 7, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['If']}, 'target': {'target-head-sentence-ID': 7, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['If']}}, {'element': (10, 0, ['If'], ['if']), 'source': {'source-head-sentence-ID': 10, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['If']}, 'target': {'target-head-sentence-ID': 10, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['If']}}, {'element': (10, 6, ['or'], ['or']), 'source': {'source-head-sentence-ID': 10, 'source-head-word-ID': 6, 'source-entity-type': 'XOR Gateway', 'source-entity': ['or']}, 'target': {'target-head-sentence-ID': 10, 'target-head-word-ID': 6, 'target-entity-type': 'XOR Gateway', 'target-entity': ['or']}}]\n",
      "OLD APPROACH\n",
      "(6, 0, ['If'], ['if']) , (7, 0, ['If'], ['if'])\n",
      "-------------- Concurrent gateways ---------------\n",
      "['In', 'the', 'meantime']\n",
      "--------------- Exclusive gateways ---------------\n",
      "['or']\n",
      "['If']\n",
      "['If']\n",
      "['If']\n",
      "['or']\n",
      "------------- Same gateway relations -------------\n",
      "{'source-entity': ['If'], 'target-entity': ['If']}\n",
      "NEW APPROACH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Keywords Same Gateway Filtered Approach:Custom predict in doc-1.1: (2, 9, ['or'], ['or']) - (6, 0, ['If'], ['if']) -> True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5473748 <class 'numpy.float32'> True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Keywords Same Gateway Filtered Approach:Custom predict in doc-1.1: (6, 0, ['If'], ['if']) - (7, 0, ['If'], ['if']) -> True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5678432 <class 'numpy.float32'> True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Keywords Same Gateway Filtered Approach:Custom predict in doc-1.1: (7, 0, ['If'], ['if']) - (10, 0, ['If'], ['if']) -> True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5629023 <class 'numpy.float32'> True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Keywords Same Gateway Filtered Approach:Custom predict in doc-1.1: (10, 0, ['If'], ['if']) - (10, 6, ['or'], ['or']) -> True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5770768 <class 'numpy.float32'> True\n",
      "(2, 9, ['or'], ['or']) , (6, 0, ['If'], ['if'])\n",
      "(6, 0, ['If'], ['if']) , (7, 0, ['If'], ['if'])\n",
      "(7, 0, ['If'], ['if']) , (10, 0, ['If'], ['if'])\n",
      "(10, 0, ['If'], ['if']) , (10, 6, ['or'], ['or'])\n"
     ]
    }
   ],
   "source": [
    "class KeywordsSGCApproach(KeywordsApproach):\n",
    "    \"\"\"\n",
    "    extend KeywordsApproach by evaluating same gateway relations with model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, approach_name: str = None, keywords: str = LITERATURE, contradictory_keywords: str = GOLD,\n",
    "                 same_xor_gateway_threshold: int = 1, multiple_branches_allowed: bool = False,\n",
    "                 output_format: str = BENCHMARK, output_folder: str = None,\n",
    "                 xor_rule_c: bool = True, xor_rule_or: bool = True, xor_rule_op: bool = True,\n",
    "                 # class / ensemble specific params:\n",
    "                 ensemble_path: str = None, seed_limit: int = None):\n",
    "        \"\"\"\n",
    "        creates new instance of the same gateway relation classification approach\n",
    "        ---- super class params ----\n",
    "        :param approach_name: description of approach to use in result folder name; if not set use key word variant\n",
    "        :param keywords: flag/variant which keywords to use; available: literature, gold, own\n",
    "        :param same_xor_gateway_threshold: threshold to recognize subsequent (contradictory xor) gateways as same\n",
    "        :param output_format: output format of extracted element and flows; available: benchmark, pet\n",
    "        :param output_folder: name of output folder; if none -> create based on approach name\n",
    "        :param xor_rule_c: flag if rule for detection of contradictory gateways should be applied\n",
    "        :param xor_rule_or: flag if rule for detection of 'or' gateways should be applied\n",
    "        :param xor_rule_op: flag if rule for detection of one branch (optional branches) should be applied\n",
    "        -- class / ensemble params ---\n",
    "        :param ensemble_path: path of ensemble model to restore weights from;\n",
    "                              if None, a random initialized model will be used\n",
    "        :param seed_limit: limit of seeds to reload from the ensemble (in case of OOM errors)\n",
    "        \"\"\"\n",
    "        super().__init__(approach_name=approach_name, keywords=keywords, contradictory_keywords=contradictory_keywords,\n",
    "                         same_xor_gateway_threshold=same_xor_gateway_threshold,\n",
    "                         multiple_branches_allowed=multiple_branches_allowed, output_format=output_format,\n",
    "                         output_folder=output_folder,\n",
    "                         xor_rule_c=xor_rule_c, xor_rule_or=xor_rule_or, xor_rule_op=xor_rule_op)\n",
    "        # self.same_gateway_classifier = Ensemble(args=None, model_class=SameGatewayClassifier, ensemble_path=ensemble_path, seed_limit=seed_limit)\n",
    "        self.same_gateway_classifier = sgc\n",
    "        set_seeds(config[SEED], \"Reset after initialization of SameGatewayClassifierEnsemble\")\n",
    "\n",
    "    def _extract_exclusive_flows(self, doc_activity_tokens: List[List[Tuple[str, int]]],\n",
    "                                 extracted_gateways: List[List[Tuple[str, int, str]]],\n",
    "                                 doc_name: str = None) -> Tuple[List[Dict], List[Dict]]:\n",
    "        \"\"\"\n",
    "        extracts sequence flows surrounding exclusive gateways based on rules TODO describe rules\n",
    "        :param doc_activity_tokens: list of activity tokens (word, idx) for each sentence\n",
    "        :param extracted_gateways: list of own extracted gateway for each sentence\n",
    "        :param doc_name: doc_name (used by override of KeywordsSGCApproach)\n",
    "        :return: list of flow relations as source/target dicts; list of same gateway relations as source/target dicts\n",
    "        \"\"\"\n",
    "        sequence_flows = []\n",
    "        same_gateway_relations = []\n",
    "\n",
    "        gateways = self._preprocess_extracted_gateways(extracted_gateways, XOR_GATEWAY)\n",
    "        gateways_involved = []  # list for gateways already involved into sequence flows\n",
    "        gateways_involved_contradictory = []  # list for gateways already involved into a contradictory gateway pair\n",
    "\n",
    "        # RULE 1): check for every pair of following gateways if it fits to a gateway constellation with\n",
    "        # contradictory key words. Gateways must be in range of same_xor_gateway_threshold sentences, otherwise they\n",
    "        # would be seen as separate ones\n",
    "        print(gateways)\n",
    "        if self.xor_rule_c:\n",
    "            for g1, g2 in self.extract_same_gateway_pairs_old(doc_name, gateways, gateways_involved, gateways_involved_contradictory):\n",
    "                gateways_involved.append(g1[ELEMENT])\n",
    "                gateways_involved.append(g2[ELEMENT])\n",
    "                gateways_involved_contradictory.append(g1[ELEMENT])\n",
    "                gateways_involved_contradictory.append(g2[ELEMENT])\n",
    "\n",
    "                # A) find related activities\n",
    "                _, pa_g1, fa_g1, _ = self._get_surrounding_activities(g1, doc_activity_tokens)\n",
    "                _, _, fa_g2, ffa_g2 = self._get_surrounding_activities(g2, doc_activity_tokens)\n",
    "\n",
    "                # B.1) connect elements to sequence flows\n",
    "                # check if fol. activities of g1 and g2 are equal -> if yes, the first branch is without activity\n",
    "                empty_branch = fa_g1[ELEMENT] == fa_g2[ELEMENT]\n",
    "                # 1) previous activity to first gateway -> split point (if not None because of document start)\n",
    "                if pa_g1[ELEMENT]:\n",
    "                    sequence_flows.append(self._merge_flow(pa_g1, g1))\n",
    "                # 2) gateway 1 to following activity and following activity to activity after gateway (second\n",
    "                # following of g2) -> merge point\n",
    "                # if None because of empty branch then directly there\n",
    "                if not empty_branch and fa_g1[ELEMENT]:  # could be None if at document end\n",
    "                    sequence_flows.append(self._merge_flow(g1, fa_g1))\n",
    "                    if ffa_g2[ELEMENT]:  # could be None if at document end\n",
    "                        sequence_flows.append(self._merge_flow(fa_g1, ffa_g2))\n",
    "                elif empty_branch and ffa_g2[ELEMENT]:  # could be None if at document end\n",
    "                    sequence_flows.append(self._merge_flow(g1, ffa_g2))\n",
    "                # 3) gateway 2 to following activity and following activity to activity after gateway (second\n",
    "                # following of g2) -> merge point\n",
    "                if fa_g2[ELEMENT]:  # could be None if at document end\n",
    "                    sequence_flows.append(self._merge_flow(g2, fa_g2))\n",
    "                if ffa_g2[ELEMENT]:  # could be None if at document end\n",
    "                    sequence_flows.append(self._merge_flow(fa_g2, ffa_g2))\n",
    "\n",
    "                # B.2) same gateway flows\n",
    "                same_gateway_relations.append(self._merge_flow(g1, g2))\n",
    "\n",
    "                # log gateway frame for later usage in flow merging of whole document\n",
    "                closing = fa_g2 if fa_g2[ELEMENT] else g2\n",
    "                self._log_gateway_frame(g1[ELEMENT][0], g1[ELEMENT][1], g1,\n",
    "                                        closing[ELEMENT][0], closing[ELEMENT][1], closing)\n",
    "\n",
    "        return sequence_flows, same_gateway_relations\n",
    "    \n",
    "    def extract_same_gateway_pairs_old(self, doc_name, gateways, gateways_involved, gateways_involved_contradictory):\n",
    "        print(\"OLD APPROACH\")\n",
    "        same_gateway_pairs = []\n",
    "        for i in range(len(gateways) - 1):\n",
    "            g1, g2 = gateways[i], gateways[i + 1]\n",
    "            # if sentence distances is larger than threshold, reject possible pair\n",
    "            if abs(g2[ELEMENT][0] - g1[ELEMENT][0]) > self.same_xor_gateway_threshold:\n",
    "                continue\n",
    "            # check for every pair of following gateways if it fits to a gateway pair of contradictory key words\n",
    "            # and check that first gateway is at the beginning of a sentence\n",
    "            # and check if gateways already matched another pair; possible because of partly same phrase\n",
    "            for pattern_gateway_1, pattern_gateway_2 in self._contradictory_gateways:\n",
    "                if g1[ELEMENT][3] == pattern_gateway_1 and g2[ELEMENT][3] == pattern_gateway_2 \\\n",
    "                        and g1[ELEMENT][1] == 0 \\\n",
    "                        and ((g1[ELEMENT] not in gateways_involved_contradictory\n",
    "                              and g2[ELEMENT] not in gateways_involved_contradictory)\n",
    "                             or self.multiple_branches_allowed):\n",
    "                    same_gateway_pairs.append((g1, g2))\n",
    "        for g1, g2 in same_gateway_pairs:\n",
    "            print(g1[ELEMENT], \",\", g2[ELEMENT])\n",
    "        return same_gateway_pairs\n",
    "    \n",
    "    def extract_same_gateway_pairs_new(self, doc_name, gateways, gateways_involved, gateways_involved_contradictory):\n",
    "        print(\"NEW APPROACH\")\n",
    "        same_gateway_pairs = []\n",
    "        for i in range(len(gateways) - 1):\n",
    "            g1, g2 = gateways[i], gateways[i + 1]\n",
    "            if self.same_gateway_classifier.classify_pair(doc_name, g1[ELEMENT], g2[ELEMENT]):\n",
    "                same_gateway_pairs.append((g1, g2))\n",
    "        for g1, g2 in same_gateway_pairs:\n",
    "            print(g1[ELEMENT], \",\", g2[ELEMENT])\n",
    "        return same_gateway_pairs\n",
    "\n",
    "    \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "set_seeds(config[SEED], \"Set first seed\")\n",
    "keyword_filtered_approach = KeywordsSGCApproach(approach_name='key_words_literature_sg_classified_{model_params}',\n",
    "                                                 # three cases to evaluate with filter model\n",
    "                                                 keywords=LITERATURE,\n",
    "                                                 # keywords=CUSTOM,\n",
    "                                                 # keywords=CUSTOM, contradictory_keywords=GOLD, same_xor_gateway_threshold=3, multiple_branches_allowed=True, seed_limit=15,\n",
    "                                                 ensemble_path=\"?\")\n",
    "if False:\n",
    "    keyword_filtered_approach.evaluate_documents(evaluate_token_cls=True, evaluate_relation_extraction=True)\n",
    "\n",
    "if True:\n",
    "    doc_name = 'doc-1.1'\n",
    "    xor_gateways, and_gateways, doc_flows, same_gateway_relations = keyword_filtered_approach.process_document(doc_name)\n",
    "\n",
    "    print(\" Concurrent gateways \".center(50, '-'))\n",
    "    for gateway in and_gateways:\n",
    "        print(gateway)\n",
    "\n",
    "    print(\" Exclusive gateways \".center(50, '-'))\n",
    "    for gateway in xor_gateways:\n",
    "        print(gateway)\n",
    "    \n",
    "    print(\" Same gateway relations \".center(50, '-'))\n",
    "    for sg in same_gateway_relations:\n",
    "        print(sg)\n",
    "        \n",
    "if True:\n",
    "    test_gateways = [{'element': (2, 9, ['or'], ['or']), 'source': {'source-head-sentence-ID': 2, 'source-head-word-ID': 9, 'source-entity-type': 'XOR Gateway', 'source-entity': ['or']}, 'target': {'target-head-sentence-ID': 2, 'target-head-word-ID': 9, 'target-entity-type': 'XOR Gateway', 'target-entity': ['or']}}, {'element': (6, 0, ['If'], ['if']), 'source': {'source-head-sentence-ID': 6, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['If']}, 'target': {'target-head-sentence-ID': 6, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['If']}}, {'element': (7, 0, ['If'], ['if']), 'source': {'source-head-sentence-ID': 7, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['If']}, 'target': {'target-head-sentence-ID': 7, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['If']}}, {'element': (10, 0, ['If'], ['if']), 'source': {'source-head-sentence-ID': 10, 'source-head-word-ID': 0, 'source-entity-type': 'XOR Gateway', 'source-entity': ['If']}, 'target': {'target-head-sentence-ID': 10, 'target-head-word-ID': 0, 'target-entity-type': 'XOR Gateway', 'target-entity': ['If']}}, {'element': (10, 6, ['or'], ['or']), 'source': {'source-head-sentence-ID': 10, 'source-head-word-ID': 6, 'source-entity-type': 'XOR Gateway', 'source-entity': ['or']}, 'target': {'target-head-sentence-ID': 10, 'target-head-word-ID': 6, 'target-entity-type': 'XOR Gateway', 'target-entity': ['or']}}]\n",
    "    keyword_filtered_approach.extract_same_gateway_pairs_new('doc-1.1', test_gateways, [], [])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
