{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa2b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent dir to sys path for import of modules\n",
    "import os\n",
    "import sys\n",
    "parentdir = os.path.abspath(os.path.join(os.path.abspath(''), os.pardir))\n",
    "sys.path.insert(0, parentdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e934f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PetReader:Load RelationsExtraction dataset ...\n",
      "WARNING:datasets.builder:Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\relations-extraction\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04288601875305176,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fa091781844bbe95a4344cd87d62f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PetReader:Load TokenClassification dataset ...\n",
      "WARNING:datasets.builder:Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\token-classification\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.06691980361938477,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b78d0cf71a45fe998ed450ccf14dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from petreader.labels import *\n",
    "import pandas as pd\n",
    "\n",
    "from PetReader import pet_reader\n",
    "from labels import *\n",
    "from utils import read_json_to_dict, read_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a2f984",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f7fd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cls_goldstandard = read_json_to_dict(\"data/other/token_goldstandard.json\")\n",
    "\n",
    "keywords_gold_token_cls_results = read_json_to_dict(\"data/results/key_words_gold/results-token-classification.json\")\n",
    "keywords_gold_token_cls = read_json_to_dict(\"data/results/key_words_gold/token-classification.json\")\n",
    "\n",
    "keywords_literature_token_cls_results = read_json_to_dict(\"data/results/key_words_literature/results-token-classification.json\")\n",
    "keywords_literature_token_cls = read_json_to_dict(\"data/results/key_words_literature/token-classification.json\")\n",
    "\n",
    "keywords_literature_filtered_token_cls_results = read_json_to_dict(\"data/results/key_words_literature_filtered/results-token-classification.json\")\n",
    "keywords_literature_filtered_token_cls = read_json_to_dict(\"data/results/key_words_literature_filtered/token-classification.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f6558",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdb8a98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'otherwise']\n"
     ]
    }
   ],
   "source": [
    "from petreader.TokenClassification import TokenClassification\n",
    "from petbenchmarks.tokenclassification import TokenClassificationBenchmark\n",
    "benchmark = TokenClassificationBenchmark(pet_reader.token_dataset)\n",
    "\n",
    "gold = benchmark.GetGoldStandard()\n",
    "print(gold['doc-3.2'][XOR_GATEWAY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6aa14462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [['If'], ['otherwise']], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(pet_reader.token_dataset.GetXORGateways('doc-3.2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eff1e2",
   "metadata": {},
   "source": [
    "## 1) Analyze Keyword Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d784cd62",
   "metadata": {},
   "source": [
    "### a) check for documents with recall != 1 (GOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f8dd4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_name, doc_dict in keywords_gold_token_cls_results.items():\n",
    "    if doc_name.startswith('doc'):\n",
    "        if doc_dict[XOR_GATEWAY][RECALL] != 1 and doc_dict[XOR_GATEWAY][SUPPORT] != 0:\n",
    "            print(doc_name.center(100, '-'))\n",
    "            print(\"--Text--\")\n",
    "            for i, line in enumerate(pet_reader.get_doc_sentences(doc_name)):\n",
    "                print(i, ' '.join(line))\n",
    "            print()\n",
    "            print(\"--Results--:\", doc_dict[XOR_GATEWAY])\n",
    "            print()\n",
    "            print(\"--Extracted--:\", keywords_gold_token_cls[doc_name][XOR_GATEWAY])\n",
    "            print()\n",
    "            print(\"--Gold standard--:\", token_cls_goldstandard[doc_name][XOR_GATEWAY])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd8e78",
   "metadata": {},
   "source": [
    "### b) In how many different documents appear the goldstandard gateways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "204306e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldstandard_object_path = \"data/other/token_goldstandard.pkl\"\n",
    "with open(os.path.abspath(goldstandard_object_path), 'rb') as file:\n",
    "    goldstandard = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22538468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gateway_stats(classifications, gateway_type):\n",
    "    gateway_stats = {}\n",
    "    for doc_name, doc_dict in classifications.items():\n",
    "        for gateway_token in doc_dict[gateway_type]:\n",
    "            if gateway_token not in gateway_stats:\n",
    "                gateway_stats[gateway_token] = {\"count\": 1, \"docs\": 1, \"doc_list\": set([doc_name])}\n",
    "            else:\n",
    "                gateway_stats[gateway_token][\"doc_list\"].add(doc_name)\n",
    "                gateway_stats[gateway_token][\"docs\"] = len(gateway_stats[gateway_token][\"doc_list\"])\n",
    "            \n",
    "    gateway_stats = collections.OrderedDict(sorted(gateway_stats.items(), \n",
    "                                                   key=lambda kv_pair: (kv_pair[1][\"count\"], kv_pair[1][\"docs\"]),\n",
    "                                                   reverse=True))\n",
    "    return gateway_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "184b082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If              {'count': 52, 'docs': 23, 'doc_list': {'doc-9.1', 'doc-5.3', 'doc-1.1', 'doc-5.1', 'doc-3.8', 'doc-9\n",
      "or              {'count': 20, 'docs': 15, 'doc_list': {'doc-10.12', 'doc-10.4', 'doc-10.10', 'doc-2.2', 'doc-10.7', \n",
      "case            {'count': 12, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.1', 'doc-2.2'}}\n",
      "if              {'count': 9, 'docs': 7, 'doc_list': {'doc-9.5', 'doc-4.1', 'doc-10.2', 'doc-9.1', 'doc-6.1', 'doc-1.\n",
      "In              {'count': 9, 'docs': 3, 'doc_list': {'doc-9.5', 'doc-2.1', 'doc-2.2'}}\n",
      "Otherwise       {'count': 6, 'docs': 5, 'doc_list': {'doc-9.5', 'doc-3.5', 'doc-7.1', 'doc-3.6', 'doc-3.8'}}\n",
      "otherwise       {'count': 6, 'docs': 5, 'doc_list': {'doc-8.3', 'doc-9.1', 'doc-2.1', 'doc-3.2', 'doc-1.2'}}\n",
      "of              {'count': 5, 'docs': 2, 'doc_list': {'doc-9.5', 'doc-2.2'}}\n",
      "For             {'count': 5, 'docs': 2, 'doc_list': {'doc-2.1', 'doc-4.1'}}\n",
      "the             {'count': 4, 'docs': 2, 'doc_list': {'doc-2.1', 'doc-2.2'}}\n",
      "Sometimes       {'count': 2, 'docs': 2, 'doc_list': {'doc-6.4', 'doc-8.2'}}\n",
      "Should          {'count': 2, 'docs': 1, 'doc_list': {'doc-6.1'}}\n",
      "sometimes       {'count': 2, 'docs': 1, 'doc_list': {'doc-6.4'}}\n",
      "either          {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "whereas         {'count': 1, 'docs': 1, 'doc_list': {'doc-9.5'}}\n",
      "each            {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "patient         {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "for             {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "which           {'count': 1, 'docs': 1, 'doc_list': {'doc-4.1'}}\n",
      "it              {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "can             {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "also            {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "happen          {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "that            {'count': 1, 'docs': 1, 'doc_list': {'doc-8.2'}}\n",
      "Under           {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "certain         {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n",
      "circumstances   {'count': 1, 'docs': 1, 'doc_list': {'doc-8.3'}}\n"
     ]
    }
   ],
   "source": [
    "# XOR GOLDSTANDARD\n",
    "goldstandard_xor_gateway_stats = get_gateway_stats(goldstandard, XOR_GATEWAY)\n",
    "for gateway, stats in goldstandard_xor_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "066f8c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             {'count': 2, 'docs': 2, 'doc_list': {'doc-1.1', 'doc-2.2'}}\n",
      "meantime        {'count': 2, 'docs': 2, 'doc_list': {'doc-3.2', 'doc-1.1'}}\n",
      "While           {'count': 2, 'docs': 2, 'doc_list': {'doc-1.4', 'doc-1.3'}}\n",
      "At              {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "same            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "time            {'count': 1, 'docs': 1, 'doc_list': {'doc-2.2'}}\n",
      "In              {'count': 1, 'docs': 1, 'doc_list': {'doc-1.1'}}\n",
      "two             {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "concurrent      {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "activities      {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "are             {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "triggered       {'count': 1, 'docs': 1, 'doc_list': {'doc-2.1'}}\n",
      "Meantime        {'count': 1, 'docs': 1, 'doc_list': {'doc-3.5'}}\n",
      "whereas         {'count': 1, 'docs': 1, 'doc_list': {'doc-1.2'}}\n"
     ]
    }
   ],
   "source": [
    "# AND GOLDSTANDARD\n",
    "goldstandard_and_gateway_stats = get_gateway_stats(goldstandard, AND_GATEWAY)\n",
    "for gateway, stats in goldstandard_and_gateway_stats.items():\n",
    "    print(gateway.ljust(15), str(stats)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e10b8bb",
   "metadata": {},
   "source": [
    "### c) Analyze False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8f1f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_keyword_metrics(gold_standard, extracted_tokens, gateway_type: str, doc_names=[]):\n",
    "    keyword_metrics = {}\n",
    "    empty_stats_dict = {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TPs\": [], \"FPs\": [], \"FNs\": []}\n",
    "\n",
    "    for doc_name, doc_dict in keywords_gold_token_cls_results.items():\n",
    "        \n",
    "        if doc_name.startswith('doc'):  # result dict contains as well keys for statistics            \n",
    "            if not doc_names or doc_name in doc_names:  # filter for (optionally) passed list\n",
    "\n",
    "                # remove gateways step by step if they were identified; left overs are false positives\n",
    "                not_discovered_golds = [g.lower() for g in gold_standard[doc_name][gateway_type].copy()]\n",
    "\n",
    "                for extracted in extracted_tokens[doc_name][gateway_type]:\n",
    "                    extracted = extracted.lower()\n",
    "\n",
    "                    # setup keyword dict in case it was not observed yet\n",
    "                    if extracted not in keyword_metrics:\n",
    "                        keyword_metrics[extracted] = copy.deepcopy(empty_stats_dict)\n",
    "\n",
    "                    # 1) CHECK FOR FPs\n",
    "                    if extracted not in not_discovered_golds:\n",
    "                        keyword_metrics[extracted][\"FP\"] += 1\n",
    "                        keyword_metrics[extracted][\"FPs\"].append(doc_name)\n",
    "\n",
    "                    # 2) CHECK FOR TPs\n",
    "                    else:\n",
    "                        keyword_metrics[extracted][\"TP\"] += 1\n",
    "                        keyword_metrics[extracted][\"TPs\"].append(doc_name)\n",
    "                        not_discovered_golds.remove(extracted)\n",
    "\n",
    "                # 3) FILL FNs (FPs from list not_discovered_xor_golds got removed during previous loop)\n",
    "                for not_extracted in not_discovered_golds:\n",
    "\n",
    "                    # setup keyword dict in case it was not observed yet\n",
    "                    if not_extracted not in keyword_metrics:\n",
    "                        keyword_metrics[not_extracted] = copy.deepcopy(empty_stats_dict)\n",
    "\n",
    "                    keyword_metrics[not_extracted][\"FN\"] += 1\n",
    "                    keyword_metrics[not_extracted][\"FNs\"].append(doc_name)\n",
    "        \n",
    "        for kw, metrics in keyword_metrics.items():\n",
    "            keyword_metrics[kw][\"count\"] = keyword_metrics[kw][\"TP\"] + keyword_metrics[kw][\"FP\"] + keyword_metrics[kw][\"FN\"]\n",
    "            keyword_metrics[kw][\"doc_list\"] = list(set(keyword_metrics[kw][\"TPs\"] + keyword_metrics[kw][\"FPs\"] \n",
    "                                                + keyword_metrics[kw][\"FNs\"]))\n",
    "            keyword_metrics[kw][\"docs\"] = len(keyword_metrics[kw][\"doc_list\"])\n",
    "            \n",
    "    return keyword_metrics\n",
    "\n",
    "\n",
    "def print_keyword_dict(keyword_metrics, first_x=None, order_by=None):\n",
    "    \n",
    "    if order_by:\n",
    "        keyword_metrics = collections.OrderedDict(sorted(keyword_metrics.items(), key=lambda kv_pair: kv_pair[1][order_by],\n",
    "                                                         reverse=True))\n",
    "    for keyword, metrics in keyword_metrics.items():\n",
    "        print(f\"{keyword} \".ljust(15), end='')\n",
    "        for k, v in metrics.items():\n",
    "            if (len(k) == 3 and k.endswith(\"s\")) or k == 'doc_list':\n",
    "                if first_x:\n",
    "                    print(f\"{k}: \", end='')\n",
    "                    print(list(v)[:first_x], \"...\", end='')\n",
    "                    print(\", \", end='')\n",
    "            else:\n",
    "                print(f\"{k}: \", end='')\n",
    "                print(v, end='')\n",
    "                print(\", \", end='')\n",
    "        print()\n",
    "\n",
    "\n",
    "gold_xor_keyword_metrics = analyze_keyword_metrics(token_cls_goldstandard, keywords_gold_token_cls, XOR_GATEWAY)\n",
    "gold_and_keyword_metrics = analyze_keyword_metrics(token_cls_goldstandard, keywords_gold_token_cls, AND_GATEWAY)\n",
    "literature_xor_keyword_metrics = analyze_keyword_metrics(token_cls_goldstandard, keywords_literature_token_cls, XOR_GATEWAY)\n",
    "literature_and_keyword_metrics = analyze_keyword_metrics(token_cls_goldstandard, keywords_literature_token_cls, AND_GATEWAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04728d33",
   "metadata": {},
   "source": [
    "#### EXCLUSIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b057e4c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for            TP: 6, FP: 63, FN: 0, count: 69, docs: 23, \n",
      "or             TP: 20, FP: 44, FN: 0, count: 64, docs: 29, \n",
      "if             TP: 61, FP: 6, FN: 0, count: 67, docs: 27, \n",
      "should         TP: 2, FP: 6, FN: 0, count: 8, docs: 4, \n",
      "either         TP: 1, FP: 4, FN: 0, count: 5, docs: 3, \n",
      "in             TP: 9, FP: 3, FN: 0, count: 12, docs: 4, \n",
      "case           TP: 12, FP: 3, FN: 0, count: 15, docs: 4, \n",
      "of             TP: 5, FP: 3, FN: 0, count: 8, docs: 3, \n",
      "otherwise      TP: 12, FP: 1, FN: 0, count: 13, docs: 10, \n",
      "whereas        TP: 1, FP: 1, FN: 0, count: 2, docs: 2, \n",
      "sometimes      TP: 4, FP: 1, FN: 0, count: 5, docs: 3, \n",
      "the            TP: 4, FP: 0, FN: 0, count: 4, docs: 2, \n",
      "each           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "patient        TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "which          TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "it             TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "can            TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "also           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "happen         TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "that           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "under          TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "certain        TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "circumstances  TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n"
     ]
    }
   ],
   "source": [
    "# GOLD keywords\n",
    "print_keyword_dict(gold_xor_keyword_metrics, first_x=0, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf975b67",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or             TP: 20, FP: 44, FN: 0, count: 64, docs: 29, \n",
      "when           TP: 0, FP: 15, FN: 0, count: 15, docs: 14, \n",
      "whether        TP: 0, FP: 13, FN: 0, count: 13, docs: 9, \n",
      "if             TP: 61, FP: 6, FN: 0, count: 67, docs: 27, \n",
      "either         TP: 1, FP: 4, FN: 0, count: 5, docs: 3, \n",
      "in             TP: 9, FP: 2, FN: 0, count: 11, docs: 4, \n",
      "case           TP: 9, FP: 2, FN: 3, count: 14, docs: 4, \n",
      "of             TP: 5, FP: 2, FN: 0, count: 7, docs: 3, \n",
      "only           TP: 0, FP: 2, FN: 0, count: 2, docs: 2, \n",
      "not            TP: 0, FP: 1, FN: 0, count: 1, docs: 1, \n",
      "otherwise      TP: 12, FP: 1, FN: 0, count: 13, docs: 10, \n",
      "unless         TP: 0, FP: 1, FN: 0, count: 1, docs: 1, \n",
      "until          TP: 0, FP: 1, FN: 0, count: 1, docs: 1, \n",
      "the            TP: 0, FP: 0, FN: 4, count: 4, docs: 2, \n",
      "for            TP: 0, FP: 0, FN: 6, count: 6, docs: 2, \n",
      "whereas        TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "should         TP: 0, FP: 0, FN: 2, count: 2, docs: 1, \n",
      "each           TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "patient        TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "which          TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "sometimes      TP: 0, FP: 0, FN: 4, count: 4, docs: 2, \n",
      "it             TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "can            TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "also           TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "happen         TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "that           TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "under          TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "certain        TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "circumstances  TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n"
     ]
    }
   ],
   "source": [
    "# LITERATURE keywords\n",
    "print_keyword_dict(literature_xor_keyword_metrics, first_x=0, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced8668",
   "metadata": {},
   "source": [
    "#### PARALLEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e22cb2ea",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereas        TP: 1, FP: 1, FN: 0, count: 2, docs: 2, \n",
      "at             TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "the            TP: 2, FP: 0, FN: 0, count: 2, docs: 2, \n",
      "same           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "time           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "in             TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "meantime       TP: 3, FP: 0, FN: 0, count: 3, docs: 3, \n",
      "two            TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "concurrent     TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "activities     TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "are            TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "triggered      TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "while          TP: 2, FP: 0, FN: 0, count: 2, docs: 2, \n"
     ]
    }
   ],
   "source": [
    "# GOLD keywords\n",
    "print_keyword_dict(gold_and_keyword_metrics, first_x=0, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "afd2200a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in             TP: 1, FP: 2, FN: 0, count: 3, docs: 2, \n",
      "addition       TP: 0, FP: 2, FN: 0, count: 2, docs: 1, \n",
      "to             TP: 0, FP: 2, FN: 0, count: 2, docs: 1, \n",
      "whereas        TP: 1, FP: 1, FN: 0, count: 2, docs: 2, \n",
      "at             TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "the            TP: 2, FP: 0, FN: 0, count: 2, docs: 2, \n",
      "same           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "time           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "meantime       TP: 3, FP: 0, FN: 0, count: 3, docs: 3, \n",
      "two            TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "concurrent     TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "activities     TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "are            TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "triggered      TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "while          TP: 2, FP: 0, FN: 0, count: 2, docs: 2, \n"
     ]
    }
   ],
   "source": [
    "# LITERATURE keywords\n",
    "print_keyword_dict(literature_and_keyword_metrics, first_x=0, order_by=\"FP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504c99a",
   "metadata": {},
   "source": [
    "### d) Combined analysis (counts & FPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "31102059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gold_xor_keyword_metrics\n",
    "gold_and_keyword_metrics\n",
    "literature_xor_keyword_metrics\n",
    "literature_and_keyword_metrics\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4443c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utilities:Load keywords 'literature' ...\n",
      "INFO:utilities:Loaded 15 XOR and 11 AND keywords (literature)\n",
      "INFO:utilities:Used XOR keywords: ['either', 'else', 'if', 'if not', 'in case', 'in case of', 'only', 'only if', 'or', 'otherwise', 'till', 'unless', 'until', 'when', 'whether']\n",
      "INFO:utilities:Used AND keywords: ['at the same time', 'concurrently', 'in addition to', 'in parallel', 'in parallel with this', 'in the meantime', 'meantime', 'meanwhile', 'simultaneously', 'whereas', 'while']\n"
     ]
    }
   ],
   "source": [
    "literature_xor_keywords, literature_and_keywords = read_keywords(LITERATURE)\n",
    "import itertools\n",
    "literature_xor_keywords_tokens = list(set(itertools.chain(*[keyword.split(\" \") for keyword in literature_xor_keywords])))\n",
    "literature_and_keywords_tokens = list(set(itertools.chain(*[keyword.split(\" \") for keyword in literature_and_keywords])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8ece4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keyword_dict(keyword_dictionary, path, order_by=None, reference_literature_set=None):\n",
    "    keyword_dictionary = copy.deepcopy(keyword_dictionary)\n",
    "    # prepare list of dicts (one for each row) for convert to df\n",
    "    output_list = []\n",
    "    for keyword, d in keyword_dictionary.items():\n",
    "        d[\"keyword\"] = keyword\n",
    "        if reference_literature_set:\n",
    "            d[\"in_literature\"] = keyword in reference_literature_set\n",
    "        output_list.append(d)\n",
    "    df = pd.DataFrame(output_list)\n",
    "    # reorder columns\n",
    "    if reference_literature_set:\n",
    "        cols = ['keyword', 'in_literature', 'docs', 'count', 'TP', 'FP', 'FN', 'doc_list', 'TPs', 'FPs', 'FNs']\n",
    "    else:\n",
    "        cols = ['keyword', 'docs', 'count', 'TP', 'FP', 'FN', 'doc_list', 'TPs', 'FPs', 'FNs']\n",
    "    df = df[cols]\n",
    "    df[\"comment\"] = None\n",
    "    # sort\n",
    "    if order_by:\n",
    "        df.sort_values(by=order_by, ascending=False, inplace=True)\n",
    "    # save as excel\n",
    "    df.to_excel(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693f445",
   "metadata": {},
   "source": [
    "#### 1 - Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5060a028",
   "metadata": {},
   "source": [
    "##### xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dd889d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or             TP: 20, FP: 44, FN: 0, count: 64, docs: 29, \n",
      "when           TP: 0, FP: 15, FN: 0, count: 15, docs: 14, \n",
      "whether        TP: 0, FP: 13, FN: 0, count: 13, docs: 9, \n",
      "if             TP: 61, FP: 6, FN: 0, count: 67, docs: 27, \n",
      "either         TP: 1, FP: 4, FN: 0, count: 5, docs: 3, \n",
      "in             TP: 9, FP: 2, FN: 0, count: 11, docs: 4, \n",
      "case           TP: 9, FP: 2, FN: 3, count: 14, docs: 4, \n",
      "of             TP: 5, FP: 2, FN: 0, count: 7, docs: 3, \n",
      "only           TP: 0, FP: 2, FN: 0, count: 2, docs: 2, \n",
      "not            TP: 0, FP: 1, FN: 0, count: 1, docs: 1, \n",
      "otherwise      TP: 12, FP: 1, FN: 0, count: 13, docs: 10, \n",
      "unless         TP: 0, FP: 1, FN: 0, count: 1, docs: 1, \n",
      "until          TP: 0, FP: 1, FN: 0, count: 1, docs: 1, \n",
      "the            TP: 0, FP: 0, FN: 4, count: 4, docs: 2, \n",
      "for            TP: 0, FP: 0, FN: 6, count: 6, docs: 2, \n",
      "whereas        TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "should         TP: 0, FP: 0, FN: 2, count: 2, docs: 1, \n",
      "each           TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "patient        TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "which          TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "sometimes      TP: 0, FP: 0, FN: 4, count: 4, docs: 2, \n",
      "it             TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "can            TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "also           TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "happen         TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "that           TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "under          TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "certain        TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "circumstances  TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n"
     ]
    }
   ],
   "source": [
    "print_keyword_dict(literature_xor_keyword_metrics, first_x=0, order_by='FP')\n",
    "save_keyword_dict(literature_xor_keyword_metrics, \"data/keywords/analysis/xor.xlsx\", order_by='FP',\n",
    "                  reference_literature_set=literature_xor_keywords_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba067c",
   "metadata": {},
   "source": [
    "##### and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "50ddc74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in             TP: 1, FP: 2, FN: 0, count: 3, docs: 2, \n",
      "addition       TP: 0, FP: 2, FN: 0, count: 2, docs: 1, \n",
      "to             TP: 0, FP: 2, FN: 0, count: 2, docs: 1, \n",
      "whereas        TP: 1, FP: 1, FN: 0, count: 2, docs: 2, \n",
      "at             TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "the            TP: 2, FP: 0, FN: 0, count: 2, docs: 2, \n",
      "same           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "time           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "meantime       TP: 3, FP: 0, FN: 0, count: 3, docs: 3, \n",
      "two            TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "concurrent     TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "activities     TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "are            TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "triggered      TP: 0, FP: 0, FN: 1, count: 1, docs: 1, \n",
      "while          TP: 2, FP: 0, FN: 0, count: 2, docs: 2, \n"
     ]
    }
   ],
   "source": [
    "print_keyword_dict(literature_and_keyword_metrics, first_x=0, order_by='FP')\n",
    "save_keyword_dict(literature_and_keyword_metrics, \"data/keywords/analysis/and.xlsx\", order_by='FP',\n",
    "                  reference_literature_set=literature_and_keywords_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff72706",
   "metadata": {},
   "source": [
    "#### 2 - GOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd4e81",
   "metadata": {},
   "source": [
    "##### xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ec14e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for            TP: 6, FP: 63, FN: 0, count: 69, docs: 23, \n",
      "or             TP: 20, FP: 44, FN: 0, count: 64, docs: 29, \n",
      "if             TP: 61, FP: 6, FN: 0, count: 67, docs: 27, \n",
      "should         TP: 2, FP: 6, FN: 0, count: 8, docs: 4, \n",
      "either         TP: 1, FP: 4, FN: 0, count: 5, docs: 3, \n",
      "in             TP: 9, FP: 3, FN: 0, count: 12, docs: 4, \n",
      "case           TP: 12, FP: 3, FN: 0, count: 15, docs: 4, \n",
      "of             TP: 5, FP: 3, FN: 0, count: 8, docs: 3, \n",
      "otherwise      TP: 12, FP: 1, FN: 0, count: 13, docs: 10, \n",
      "whereas        TP: 1, FP: 1, FN: 0, count: 2, docs: 2, \n",
      "sometimes      TP: 4, FP: 1, FN: 0, count: 5, docs: 3, \n",
      "the            TP: 4, FP: 0, FN: 0, count: 4, docs: 2, \n",
      "each           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "patient        TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "which          TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "it             TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "can            TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "also           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "happen         TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "that           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "under          TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "certain        TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "circumstances  TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n"
     ]
    }
   ],
   "source": [
    "print_keyword_dict(gold_xor_keyword_metrics, first_x=0, order_by='FP')\n",
    "save_keyword_dict(gold_xor_keyword_metrics, \"data/keywords/analysis/gold_xor.xlsx\", order_by='FP',\n",
    "                  reference_literature_set=literature_xor_keywords_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7dfdcd",
   "metadata": {},
   "source": [
    "##### and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ea0191a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereas        TP: 1, FP: 1, FN: 0, count: 2, docs: 2, \n",
      "at             TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "the            TP: 2, FP: 0, FN: 0, count: 2, docs: 2, \n",
      "same           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "time           TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "in             TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "meantime       TP: 3, FP: 0, FN: 0, count: 3, docs: 3, \n",
      "two            TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "concurrent     TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "activities     TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "are            TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "triggered      TP: 1, FP: 0, FN: 0, count: 1, docs: 1, \n",
      "while          TP: 2, FP: 0, FN: 0, count: 2, docs: 2, \n"
     ]
    }
   ],
   "source": [
    "print_keyword_dict(gold_and_keyword_metrics, first_x=0, order_by='FP')\n",
    "save_keyword_dict(gold_and_keyword_metrics, \"data/keywords/analysis/gold_and.xlsx\", order_by='FP',\n",
    "                  reference_literature_set=literature_and_keywords_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
