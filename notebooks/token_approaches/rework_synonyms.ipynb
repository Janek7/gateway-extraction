{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a64f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent dir to sys path for import of modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# find recursively the project root dir\n",
    "parent_dir = str(os.getcwdb())\n",
    "while not os.path.exists(os.path.join(parent_dir, \"README.md\")):\n",
    "    parent_dir = os.path.abspath(os.path.join(parent_dir, os.pardir))\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ab0d6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from sklearn.model_selection import KFold\n",
    "from petreader.labels import *\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "from labels import *\n",
    "from utils import config, ROOT_DIR, load_pickle, save_as_pickle, set_seeds\n",
    "from PetReader import pet_reader\n",
    "from token_approaches.token_data_augmentation import get_synonym_samples, get_synonyms_of_original_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216aafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = transformers.AutoTokenizer.from_pretrained(config[KEYWORDS_FILTERED_APPROACH][BERT_MODEL_NAME])\n",
    "assert isinstance(_tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08e16122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Data Augmentation:Reload synonym_samples from C:\\Users\\janek\\Development\\Git\\master-thesis\\data/other/synonym_samples.pkl\n",
      "INFO:Data Augmentation:Reload synonym_samples from C:\\Users\\janek\\Development\\Git\\master-thesis\\data/other/synonym_samples.pkl\n"
     ]
    }
   ],
   "source": [
    "# load synonym data\n",
    "synonym_samples = get_synonym_samples()\n",
    "synonyms_of_original_samples = get_synonyms_of_original_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943b730",
   "metadata": {},
   "source": [
    "## Same Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90872fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('Data Preparation [Same Gateway CLS]')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batch_size\", default=1, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--gateway\", default=XOR_GATEWAY, type=str, help=\"Type of gateway to classify\")\n",
    "parser.add_argument(\"--use_synonyms\", default=True, type=str, help=\"Include synonym samples.\")\n",
    "parser.add_argument(\"--activity_masking\", default=MULTI_MASK, type=str, help=\"How to include activity data.\")\n",
    "parser.add_argument(\"--context_size\", default=1, type=int, help=\"Number of sentences around to include in text.\")\n",
    "parser.add_argument(\"--mode\", default=CONTEXT_TEXT_AND_LABELS_NGRAM, type=str, help=\"How to include gateway information.\")\n",
    "parser.add_argument(\"--n_gram\", default=1, type=int, help=\"Number of tokens to include for gateway in CONCAT mode.\")\n",
    "\n",
    "args_sg = parser.parse_args([] if \"__file__\" not in globals() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ca5e2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Data Preparation [Same Gateway CLS]:Fold 0 -> 40 / 41\n",
      "INFO:Data Preparation [Same Gateway CLS]:Process pair 0 of 40\n",
      "INFO:Data Preparation [Same Gateway CLS]:Process pair 0 of 41\n",
      "INFO:Data Preparation [Same Gateway CLS]:Fold 1 -> 41 / 40\n",
      "INFO:Data Preparation [Same Gateway CLS]:Process pair 0 of 41\n",
      "INFO:Data Preparation [Same Gateway CLS]:Process pair 0 of 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: train 13 / dev 2\n",
      "Fold 1: train 37 / dev 2\n"
     ]
    }
   ],
   "source": [
    "def _get_doc_tokens_flattened(doc_name: str) -> Tuple[List[List], List[int]]:\n",
    "    \"\"\"\n",
    "    extract, enrich and flatten tokens of given document\n",
    "    :param doc_name: doc_name\n",
    "    :return:\n",
    "        - list of tuples -> (doc token index, sample id, sentence id, token id, token, ner-tag, #I-tokens)\n",
    "        - list of sample_ids\n",
    "    \"\"\"\n",
    "    sample_ids = pet_reader.get_doc_sample_ids(doc_name)\n",
    "    doc_tokens = [list(zip(\n",
    "        [sample_id for i in range(len(pet_reader.token_dataset.GetTokens(sample_id)))],\n",
    "        [s_i for i in range(len(pet_reader.token_dataset.GetTokens(sample_id)))],\n",
    "        [i for i in range(len(pet_reader.token_dataset.GetTokens(sample_id)))],\n",
    "        pet_reader.token_dataset.GetTokens(sample_id),\n",
    "        pet_reader.token_dataset.GetNerTagLabels(sample_id))\n",
    "    ) for s_i, sample_id in enumerate(sample_ids)]\n",
    "    doc_tokens_flattened = list(itertools.chain(*doc_tokens))\n",
    "    doc_tokens_flattened = [[i] + list(token_tuple) for i, token_tuple in enumerate(doc_tokens_flattened)]\n",
    "    \n",
    "    def get_following_i_tokens(token_index):\n",
    "        \"\"\"\n",
    "        append number of following I- tokens in case of B- token for usage when computing n_grams\n",
    "        :param token_index: token index\n",
    "        :return: list of following I- tokens\n",
    "        \"\"\"\n",
    "        following_i_tokens = []\n",
    "        for token in doc_tokens_flattened[token_index + 1:]:\n",
    "            if token[5].startswith(\"I-\"):\n",
    "                following_i_tokens.append(token)\n",
    "            else:\n",
    "                break\n",
    "        return following_i_tokens\n",
    "\n",
    "    doc_tokens_flattened = [doc_token + [len(get_following_i_tokens(doc_token[0]))]\n",
    "                            for doc_token in doc_tokens_flattened]\n",
    "    return doc_tokens_flattened, sample_ids\n",
    "    \n",
    "    \n",
    "def _get_textual_token(token_tuple, gateways_sample_infos):\n",
    "    \"\"\"\n",
    "    returns the textual token of the given token tuple considering the different possible samples (normal or synonyms)\n",
    "    :param token_tuple: token tuple\n",
    "    :param gateways_sample_infos: infos about which samples are used for surrounding gateways\n",
    "    :returns: token\n",
    "    \"\"\"\n",
    "    if not gateways_sample_infos:\n",
    "        return token_tuple[4]\n",
    "\n",
    "    (g1_sample_id, g1_sample_id_original), (g2_sample_id, g2_sample_id_original) = gateways_sample_infos\n",
    "\n",
    "    # check if both gateways are in same sentence and token is in the sentence\n",
    "    if g1_sample_id_original == g2_sample_id_original and token_tuple[1] == g1_sample_id_original:\n",
    "\n",
    "        # prefer higher id to favor synonym samples (but all will be used once)\n",
    "        sample_id_to_choose = max(g1_sample_id, g2_sample_id)\n",
    "        if sample_id_to_choose >= config[SYNONYM_SAMPLES_START_NUMBER]:\n",
    "            return synonym_samples[sample_id_to_choose]['tokens'][token_tuple[3]]\n",
    "        else:\n",
    "            return token_tuple[4]\n",
    "\n",
    "    # if token is in sentence of first gateway\n",
    "    elif token_tuple[1] == g1_sample_id_original:\n",
    "\n",
    "        # if sample is original sample, take normal token\n",
    "        if g1_sample_id == g1_sample_id_original:\n",
    "            return token_tuple[4]\n",
    "        # if not, take token at the same index from synonym sample\n",
    "        else:\n",
    "            return synonym_samples[g1_sample_id]['tokens'][token_tuple[3]]\n",
    "\n",
    "    # if token is in sentence of second gateway\n",
    "    elif token_tuple[1] == g2_sample_id_original:\n",
    "        # if sample is original sample, take normal token\n",
    "        if g2_sample_id == g2_sample_id_original:\n",
    "            return token_tuple[4]\n",
    "        # if not, take token at the same index from synonym sample\n",
    "        else:\n",
    "            return synonym_samples[g2_sample_id]['tokens'][token_tuple[3]]\n",
    "\n",
    "    # if token is not in scope of gateway sentences but context -> return normal token\n",
    "    else:\n",
    "        return token_tuple[4]\n",
    "\n",
    "\n",
    "def _get_n_gram(token, n_gram, doc_tokens_flattened, gateways_sample_infos=None):\n",
    "    \"\"\"\n",
    "    create n gram of a given token\n",
    "    for gateway elements that consist of multiple tokens, include I- tokens as well by adding token[6] to range\n",
    "    :param token: token tuple\n",
    "    :param gateways_sample_infos: infos about which samples are used for surrounding gateways\n",
    "    :return: textual n-gram\n",
    "    \"\"\"\n",
    "    return ' '.join([_get_textual_token(token_tuple, gateways_sample_infos)\n",
    "                     for token_tuple in doc_tokens_flattened[max(token[0] - n_gram, 0):\n",
    "                                                             min(token[0] + n_gram + token[6] + 1,\n",
    "                                                                 len(doc_tokens_flattened))]])\n",
    "\n",
    "\n",
    "def _tokenize_textual_features(mode, texts, n_gram_tuples) -> transformers.BatchEncoding:\n",
    "    \"\"\"\n",
    "    create a tokenization with different inputs based on passed mode\n",
    "    :param mode: architecture variant / mode\n",
    "    :param texts: texts\n",
    "    :param n_gram_tuples: n gram tuples\n",
    "    :return: encoded tokens\n",
    "    \"\"\"\n",
    "    if mode == N_GRAM or mode == CONTEXT_LABELS_NGRAM:\n",
    "        tokens = _tokenizer(n_gram_tuples, padding=True, return_tensors=\"tf\")\n",
    "    elif mode == CONTEXT_INDEX:\n",
    "        tokens = _tokenizer(texts, padding=True, return_tensors='tf')\n",
    "    elif mode == CONTEXT_NGRAM or mode == CONTEXT_TEXT_AND_LABELS_NGRAM:\n",
    "        # tokenize text & pairs separately, because it is not possible to concat triple\n",
    "        text_tokens = _tokenizer(texts, padding=True, return_tensors='tf')\n",
    "        n_gram_tokens = _tokenizer(n_gram_tuples, padding=True, return_tensors=\"tf\")\n",
    "        # concat manually after (cut the CLS token of the second pair / n_grams)\n",
    "        concatted_input_ids = tf.concat([text_tokens[\"input_ids\"], n_gram_tokens[\"input_ids\"][:, 1:]], axis=1)\n",
    "        concatted_attention_masks = tf.concat([text_tokens[\"attention_mask\"], n_gram_tokens[\"attention_mask\"][:, 1:]],\n",
    "                                              axis=1)\n",
    "        tokens = transformers.BatchEncoding(\n",
    "            {\"input_ids\": concatted_input_ids, \"attention_mask\": concatted_attention_masks})\n",
    "    else:\n",
    "        raise ValueError(f\"mode must be {N_GRAM}, {CONTEXT_INDEX}, {CONTEXT_NGRAM}, {CONTEXT_LABELS_NGRAM} or\"\n",
    "                         f\" {CONTEXT_TEXT_AND_LABELS_NGRAM}\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def _pad_context_labels(context_labels: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    pad context labels to static maximum length from config (necessary for passing to dense prediction layer)\n",
    "    :param context_labels: list of context labels unpadded\n",
    "    :return: list of context labels padded\n",
    "    \"\"\"\n",
    "    # pad context labels to same fixed length (pad with 0, label for activities = 1, label for other tokens = 2\n",
    "    max_context = config[SAME_GATEWAY_CLASSIFIER][CONTEXT_LABEL_LENGTH]\n",
    "    context_labels_padded = [row[:max_context] + [SGC_CONTEXT_LABEL_PADDING for i in range(max_context - len(row))]\n",
    "                             for row in context_labels]\n",
    "    return context_labels_padded\n",
    "\n",
    "\n",
    "def _mask_activities(doc_tokens_flattened: List[List], masking_strategy: str) -> List[List]:\n",
    "    \"\"\"\n",
    "    mask activities with \"dummy\", most common activity or most common activities (if multiple in one sentence)\n",
    "    :param doc_tokens_flattened: list of tokens of a document\n",
    "    :param masking_strategy: how activities should be masked\n",
    "    :return: list of tokens with masked texts\n",
    "    \"\"\"\n",
    "    found_activities = 0\n",
    "    for token in doc_tokens_flattened:\n",
    "        if token[5].endswith(ACTIVITY):\n",
    "            if masking_strategy == DUMMY:\n",
    "                masked = 'activity'\n",
    "            elif masking_strategy == SINGLE_MASK:\n",
    "                masked = pet_reader.most_common_activities[0]\n",
    "            elif masking_strategy == MULTI_MASK:\n",
    "                masked = pet_reader.most_common_activities[found_activities]\n",
    "            found_activities += 1\n",
    "            token[4] = masked\n",
    "    return doc_tokens_flattened\n",
    "\n",
    "    \n",
    "def _get_gateway_pairs(gateway_type: str, doc_names: List[str] = []) -> List[Tuple]:\n",
    "    pairs = []\n",
    "    for i, doc_name in enumerate(pet_reader.document_names):\n",
    "\n",
    "        if doc_names and (doc_name not in doc_names):\n",
    "            continue\n",
    "            \n",
    "        if i % 5 == 0:\n",
    "            print(f\"processed {i} documents\")\n",
    "\n",
    "        # 1) Prepare token data\n",
    "        doc_tokens_flattened, sample_ids = _get_doc_tokens_flattened(doc_name)\n",
    "\n",
    "        # 2) Identify gateway pairs\n",
    "        # filter for B- tokens, because I-s do not mark a new gateway of interest\n",
    "        gateway_tokens = [token_tuple for token_tuple in doc_tokens_flattened if token_tuple[5] == f\"B-{gateway_type}\"]\n",
    "        gateway_pairs = [(gateway_tokens[i], gateway_tokens[i + 1]) for i in range(len(gateway_tokens) - 1)]\n",
    "        \n",
    "        same_gateway_relations = pet_reader.get_doc_relations(doc_name)[SAME_GATEWAY]\n",
    "        label = None  # if gateway are related (1) or not (0)\n",
    "        # check if for pair of two subsequent gateways exists a same gateway relation\n",
    "        for g1, g2 in gateway_pairs:\n",
    "            same_gateway_found = False\n",
    "            for same_gateway_relation in same_gateway_relations:\n",
    "                if not same_gateway_found \\\n",
    "                        and g1[2] == same_gateway_relation[SOURCE_SENTENCE_ID] \\\n",
    "                        and g1[3] == same_gateway_relation[SOURCE_HEAD_TOKEN_ID] \\\n",
    "                        and g2[2] == same_gateway_relation[TARGET_SENTENCE_ID] \\\n",
    "                        and g2[3] == same_gateway_relation[TARGET_HEAD_TOKEN_ID]:\n",
    "                    label = 1\n",
    "                    same_gateway_found = True\n",
    "            if not same_gateway_found:\n",
    "                label = 0\n",
    "        \n",
    "        for pair in gateway_pairs:\n",
    "            pairs.append((doc_name, pair, label))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def _create_dataset_sg(input_ids: tf.Tensor, attention_masks: tf.Tensor, indexes: tf.Tensor, context_labels: tf.Tensor,\n",
    "                       labels: tf.Tensor) -> tf.data.Dataset:\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        ({'input_ids': input_ids, 'attention_mask': attention_masks, \"indexes\": indexes,\n",
    "          \"context_labels\": context_labels}, labels))\n",
    "\n",
    "\n",
    "def _generate_data_sg(gateway_type, args, pairs=None):\n",
    "    \n",
    "    if not pairs:\n",
    "        pairs = _get_gateway_pairs(gateway_type)\n",
    "        random.shuffle(pairs)\n",
    "\n",
    "    \n",
    "    # reload from cache if already exists\n",
    "    param_string = \"reworked_\" + '_'.join([str(p) for p in [gateway_type, args.use_synonyms, args.activity_masking, \n",
    "                                                            args.mode, args.context_size, args.n_gram]])\n",
    "    cache_path = os.path.join(ROOT_DIR, f\"data/other/data_cache/same_gateway/same_gateway_data_{param_string}\")\n",
    "\n",
    "    \n",
    "    # create datasets for k fold cross validation\n",
    "    folded_datasets = []\n",
    "        \n",
    "    kfold = KFold(n_splits=2)\n",
    "    for i, (train, dev) in enumerate(kfold.split(pairs)):\n",
    "        train_pairs = [p for i, p in enumerate(pairs) if i in train]\n",
    "        dev_pairs = [p for i, p in enumerate(pairs) if i in dev]\n",
    "        logger.info(f\"Fold {i} -> {len(train_pairs)} / {len(dev_pairs)}\")\n",
    "        \n",
    "        cache_path_train, cache_path_dev = f\"{cache_path}__fold{i}_train\", f\"{cache_path}__fold{i}_dev\"\n",
    "        train_tf_dataset = _prepare_dataset_sg(cache_path_train, train_pairs, args.mode, args.use_synonyms, \n",
    "                                               args.activity_masking, args.context_size, args.n_gram)\n",
    "        dev_tf_dataset = _prepare_dataset_sg(cache_path_dev, dev_pairs, args.mode, False, \n",
    "                                             args.activity_masking, args.context_size, args.n_gram)\n",
    "        \n",
    "        if args.batch_size:\n",
    "            train_tf_dataset = train_tf_dataset.batch(args.batch_size)\n",
    "            dev_tf_dataset = dev_tf_dataset.batch(args.batch_size)\n",
    "        \n",
    "        folded_datasets.append((train_tf_dataset, dev_tf_dataset))\n",
    "        \n",
    "    return folded_datasets\n",
    "        \n",
    "        \n",
    "def _prepare_dataset_sg(cache_path, pairs, mode, use_synonyms=False, activity_masking=NOT, context_sentences=1, n_gram=0):\n",
    "    \n",
    "    # reload from cache if already exists\n",
    "    if os.path.exists(cache_path):\n",
    "        tokens, indexes, context_labels, labels = load_pickle(cache_path)\n",
    "        logger.info(\"Reloaded same gateway data from cache\")\n",
    "        results = (tokens, indexes, context_labels, labels)\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        # lists to store results\n",
    "        texts = []  # context texts\n",
    "        n_gram_tuples = []  # tuples of gateway n_grams (only necessary for mode=context_n_gram)\n",
    "        indexes = []  # index of gateway tokens in samples -> tuple\n",
    "        context_labels = []  # list of context token labels\n",
    "        labels = []  # labels (0 or 1)   \n",
    "\n",
    "        for i, (doc_name, (g1, g2), label) in enumerate(pairs[:2]):\n",
    "            doc_tokens_flattened, sample_ids = _get_doc_tokens_flattened(doc_name)\n",
    "            \n",
    "            if activity_masking in [DUMMY, SINGLE_MASK, MULTI_MASK]:\n",
    "                doc_tokens_flattened = _mask_activities(doc_tokens_flattened, activity_masking)\n",
    "            \n",
    "            if i % 15 == 0:\n",
    "                logger.info(f\"Process pair {i} of {len(pairs)}\")\n",
    "\n",
    "            # Tokens/Text\n",
    "            num_s = context_sentences\n",
    "            sentences_in_scope = list(range(g1[2] - num_s if (g1[2] - num_s) > 0 else 0,\n",
    "                                            g2[2] + num_s + 1 if (g2[2] + num_s + 1) < len(sample_ids) else len(\n",
    "                                                sample_ids)))\n",
    "\n",
    "            def append_not_token_data():\n",
    "                \"\"\"\n",
    "                appending indexes, context_labels and labels of g1/g2 sample to dataset wide lists\n",
    "                defined for reuse because of normal and synonym mode\n",
    "                \"\"\"\n",
    "                # Indexes\n",
    "                indexes.append((g1[0], g2[0]))\n",
    "                # Context token labels\n",
    "                context_labels.append([SGC_CONTEXT_LABEL_ACTIVITY if token[5] == ACTIVITY\n",
    "                                       else SGC_CONTEXT_LABEL_OTHER for token in doc_tokens_flattened\n",
    "                                       if token[2] in sentences_in_scope])\n",
    "                # Label\n",
    "                labels.append(label)\n",
    "\n",
    "            if not use_synonyms:\n",
    "                # Tokens/Text\n",
    "                text_in_scope = ' '.join([token[4] for token in doc_tokens_flattened\n",
    "                                          if token[2] in sentences_in_scope])\n",
    "                texts.append(text_in_scope)\n",
    "                if mode in [N_GRAM, CONTEXT_NGRAM, CONTEXT_LABELS_NGRAM, CONTEXT_TEXT_AND_LABELS_NGRAM]:\n",
    "                    n_gram_tuples.append((_get_n_gram(g1, n_gram, doc_tokens_flattened),\n",
    "                                          _get_n_gram(g2, n_gram, doc_tokens_flattened)))\n",
    "\n",
    "                append_not_token_data()\n",
    "\n",
    "            else:\n",
    "                # create cartesian product between different samples of sentences that include gateways\n",
    "                # use for each gateway the sentence itself and optional synonyms\n",
    "                if g1[1] == g2[1]:\n",
    "                    gateway_sample_combinations = itertools.product(*[\n",
    "                        [(g1[1], g1[1])],\n",
    "                        [(g1[1], g1[1])] + [(s, g1[1]) for s in synonyms_of_original_samples[g1[1]]]])\n",
    "                else:\n",
    "                    g1_sample_ids = [(sample_id, g1[1]) for sample_id in [g1[1]] + synonyms_of_original_samples[g1[1]]]\n",
    "                    g2_sample_ids = [(sample_id, g2[1]) for sample_id in [g2[1]] + synonyms_of_original_samples[g2[1]]]\n",
    "                    gateway_sample_combinations = itertools.product(*[g1_sample_ids, g2_sample_ids])\n",
    "\n",
    "                # iterate over pairs of gateway sentences (multiple possible if synonyms are used)\n",
    "                for gateways_sample_infos in gateway_sample_combinations:\n",
    "                    text_in_scope = ' '.join([_get_textual_token(token, gateways_sample_infos)\n",
    "                                              for token in doc_tokens_flattened if token[2] in sentences_in_scope])\n",
    "\n",
    "                    texts.append(text_in_scope)\n",
    "                    if mode in [N_GRAM, CONTEXT_NGRAM, CONTEXT_LABELS_NGRAM, CONTEXT_TEXT_AND_LABELS_NGRAM]:\n",
    "                        n_gram_tuples.append(\n",
    "                            (_get_n_gram(g1, n_gram, doc_tokens_flattened, gateways_sample_infos),\n",
    "                             _get_n_gram(g2, n_gram, doc_tokens_flattened, gateways_sample_infos)))\n",
    "\n",
    "                    append_not_token_data()\n",
    "                    \n",
    "        results = (_tokenize_textual_features(mode, texts, n_gram_tuples),\n",
    "                   tf.constant(indexes),\n",
    "                   tf.constant(_pad_context_labels(context_labels)),\n",
    "                   tf.constant(labels))\n",
    "\n",
    "        # save in cache\n",
    "        save_as_pickle(results, cache_path)\n",
    "\n",
    "    return _create_dataset_sg(results[0][\"input_ids\"], results[0][\"attention_mask\"], results[1], \n",
    "                           results[2], results[3])\n",
    "    \n",
    "folded_datasets_sg = _generate_data_sg(XOR_GATEWAY, args_sg, pairs=pairs)\n",
    "for i, (train, dev) in enumerate(folded_datasets_sg):\n",
    "    print(f\"Fold {i}: train {len(train)} / dev {len(dev)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b11cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 documents\n",
      "processed 5 documents\n",
      "processed 10 documents\n",
      "processed 15 documents\n",
      "processed 20 documents\n",
      "processed 25 documents\n",
      "processed 30 documents\n",
      "processed 35 documents\n",
      "processed 40 documents\n"
     ]
    }
   ],
   "source": [
    "pairs = _get_gateway_pairs(XOR_GATEWAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d7129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Utilities:Set seeds to 42 (caller: None)\n"
     ]
    }
   ],
   "source": [
    "set_seeds(42)\n",
    "random.shuffle(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297513c",
   "metadata": {},
   "source": [
    "## Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8a489e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--labels\", default=ALL, type=str, help=\"Label set to use.\")\n",
    "parser.add_argument(\"--other_labels_weight\", default=0.1, type=float, help=\"Sample weight for non gateway tokens.\")\n",
    "parser.add_argument(\"--sampling_strategy\", default=NORMAL, type=str, help=\"How to sample samples.\")\n",
    "parser.add_argument(\"--use_synonyms\", default=False, type=str, help=\"Include synonym samples.\")\n",
    "parser.add_argument(\"--activity_masking\", default=NOT, type=str, help=\"How to include activity data.\")\n",
    "\n",
    "args_tc = parser.parse_args([] if \"__file__\" not in globals() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eb06e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLING STRATEGIES -> provide list of sample IDs to use\n",
    "\n",
    "def _get_sample_ids(strategy: str = None) -> List[int]:\n",
    "    \"\"\"\n",
    "    unified method to get list of samples to include in a dataset; which samples is controlled by strategy parameter\n",
    "    use use_synonyms=True only with \"normal\" and \"only gateway\" strategy\n",
    "    :param strategy: strategy which samples to include\n",
    "    :param use_synonyms: flag if synonym samples should be included;\n",
    "                         WARNING: True will change up/down sampling logic -> DO NOT USE TOGETHER\n",
    "    :return: list of sample numbers\n",
    "    \"\"\"\n",
    "    all_sample_ids = pet_reader.token_dataset.GetRandomizedSampleNumbers()\n",
    "\n",
    "    # modify all_sample_ids list based on sampling strategy\n",
    "    if strategy == NORMAL or strategy is None:\n",
    "        return all_sample_ids\n",
    "    elif strategy == UP_SAMPLING:\n",
    "        return _up_sample_gateway_samples(all_sample_ids)\n",
    "    elif strategy == DOWN_SAMPLING:\n",
    "        return _down_sample_other_samples(all_sample_ids)\n",
    "    elif strategy == ONLY_GATEWAYS:\n",
    "        return _only_gateway_samples(all_sample_ids)\n",
    "    else:\n",
    "        raise ValueError(f\"{strategy} is not a valid sampling strategy\")\n",
    "\n",
    "def _up_sample_gateway_samples(all_sample_ids: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    create a (shuffled) list of samples where gateway samples get upsampled to number of samples without gateway\n",
    "    :return: list of sample ids\n",
    "    \"\"\"\n",
    "    gateway_samples = _only_gateway_samples()\n",
    "    without_gateway_samples = list(set(all_sample_ids) - set(gateway_samples))\n",
    "\n",
    "    # sample samples with gateway until number of samples without gateway is reached\n",
    "    upsampled_gateway_samples = []\n",
    "    i = 0\n",
    "    while len(upsampled_gateway_samples) < len(without_gateway_samples):\n",
    "        upsampled_gateway_samples.append(gateway_samples[i])\n",
    "        i += 1\n",
    "        i %= len(gateway_samples)\n",
    "\n",
    "    up_sampled_samples = without_gateway_samples + upsampled_gateway_samples\n",
    "    random.seed(CURRENT_USED_SEED)\n",
    "    random.shuffle(up_sampled_samples)\n",
    "    return up_sampled_samples\n",
    "\n",
    "\n",
    "def _down_sample_other_samples(all_sample_ids: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    create a (shuffled) list of samples where samples without gateway get down sampled to the number of samples with\n",
    "    gateway\n",
    "    :return: list of sample ids\n",
    "    \"\"\"\n",
    "    gateway_samples = _only_gateway_samples()\n",
    "    without_gateway_samples = list(set(all_sample_ids) - set(gateway_samples))\n",
    "    # not all samples without gateway will be included -> shuffle to sample random ones\n",
    "    random.seed(CURRENT_USED_SEED)\n",
    "    random.shuffle(without_gateway_samples)\n",
    "\n",
    "    # sample samples without gateway until number of samples with gateway is reached\n",
    "    down_sampled_without_gateway_samples = []\n",
    "    i = 0\n",
    "    while len(down_sampled_without_gateway_samples) < len(gateway_samples):\n",
    "        down_sampled_without_gateway_samples.append(without_gateway_samples[i])\n",
    "        i += 1\n",
    "\n",
    "    down_sampled_samples = gateway_samples + down_sampled_without_gateway_samples\n",
    "    random.seed(CURRENT_USED_SEED)\n",
    "    random.shuffle(down_sampled_samples)\n",
    "    return down_sampled_samples\n",
    "\n",
    "\n",
    "def _only_gateway_samples(all_sample_ids: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    return filtered list of samples ids that contain at least one gateway token\n",
    "    :param use_synonyms: flag if synonym samples should be included\n",
    "    \"\"\"\n",
    "    only_gateway_samples = [s for s in pet_reader.token_dataset.GetRandomizedSampleNumbers()\n",
    "                            if f\"B-{XOR_GATEWAY}\" in pet_reader.token_dataset.GetSampleDictWithNerLabels(s)[\"ner-tags\"]\n",
    "                            or f\"B-{AND_GATEWAY}\" in pet_reader.token_dataset.GetSampleDictWithNerLabels(s)[\"ner-tags\"]]\n",
    "    return only_gateway_samples\n",
    "\n",
    "\n",
    "def _create_dataset(input_ids: tf.Tensor, attention_masks: tf.Tensor, labels: tf.Tensor, sample_weights: tf.Tensor)\\\n",
    "        -> tf.data.Dataset:\n",
    "    return tf.data.Dataset.from_tensor_slices(({'input_ids': input_ids, 'attention_mask': attention_masks},\n",
    "                                               labels,\n",
    "                                               sample_weights))\n",
    "\n",
    "\n",
    "def _mask_activities(sample_dicts: List[Dict], masking_strategy: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    mask activities with \"dummy\", most common activity or most common activities (if multiple in one sentence)\n",
    "    :param sample_dicts: list of samples represented as dictionaries (including tokens and ner-tags)\n",
    "    :param masking_strategy: how activities should be asked\n",
    "    :return: list of sample dictionaries with masked tokens\n",
    "    \"\"\"\n",
    "    for dictionary in sample_dicts:\n",
    "        found_activities = 0\n",
    "        masked_tokens = []\n",
    "        for token, tag in zip(dictionary[\"tokens\"], dictionary[\"ner-tags\"]):\n",
    "            if tag.endswith(ACTIVITY):\n",
    "                if masking_strategy == DUMMY:\n",
    "                    token = 'activity'\n",
    "                elif masking_strategy == SINGLE_MASK:\n",
    "                    token = pet_reader.most_common_activities[0]\n",
    "                elif masking_strategy == MULTI_MASK:\n",
    "                    token = pet_reader.most_common_activities[found_activities]\n",
    "                found_activities += 1\n",
    "            masked_tokens.append(token)\n",
    "        dictionary[\"tokens\"] = masked_tokens\n",
    "    return sample_dicts\n",
    "\n",
    "\n",
    "def _prepare_data_tc(sample_numbers: List[int], use_synonyms: bool = False, other_labels_weight: float = 0.1, \n",
    "                     label_set: str = 'filtered', activity_masking: str = None) \\\n",
    "        -> Tuple[BatchEncoding, tf.Tensor, tf.Tensor, List[List[int]]]:\n",
    "    \"\"\"\n",
    "    create token classification samples from whole PET dataset -> samples (tokens) and their labels and weights for\n",
    "    usage in a tensorflow dataset\n",
    "    include either samples from sample_numbers list OR sample samples with sampling_strategy\n",
    "    :param sample_numbers: list of concrete sample numbers\n",
    "    :param use_synonyms: flag if synonym samples should be included;\n",
    "    :param other_labels_weight: sample weight to assign samples with tokens != gateway tokens\n",
    "    :param label_set: flag if to use all labels ('all') or only gateway labels and one rest label ('filtered')\n",
    "    :param activity_masking: flag how to use activity data in tokenization\n",
    "    :return: tokens, labels & weights as tensors, original word ids (2-dim integer list)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) prepare sample data\n",
    "    sample_dicts = []\n",
    "    if use_synonyms:\n",
    "        # synonym_samples = get_synonym_samples()\n",
    "        pass\n",
    "    for sample_number in sample_numbers:\n",
    "        # in case sample is normal sample\n",
    "        if sample_number < config[SYNONYM_SAMPLES_START_NUMBER]:\n",
    "            sample_dicts.append(pet_reader.token_dataset.GetSampleDictWithNerLabels(sample_number))\n",
    "        # in case sample is synonym sample\n",
    "        else:\n",
    "            sample_dicts.append(synonym_samples[sample_number])\n",
    "\n",
    "    # apply optional activity masking\n",
    "    if activity_masking in [SINGLE_MASK, MULTI_MASK]:\n",
    "        sample_dicts = _mask_activities(sample_dicts, activity_masking)\n",
    "\n",
    "    sample_sentences = [sample_dict['tokens'] for sample_dict in sample_dicts]\n",
    "\n",
    "    # 2) transform tokens tags into IDs classification\n",
    "    dataset_tokens = _tokenizer(sample_sentences, is_split_into_words=True, padding=True, return_tensors='tf')\n",
    "    max_sentence_length = dataset_tokens['input_ids'].shape[1]\n",
    "\n",
    "    # 3) transform NER token tags into labels for classification\n",
    "    dataset_labels = []\n",
    "    dataset_sample_weights = []\n",
    "    dataset_word_ids = []\n",
    "    for i, sample_dict in enumerate(sample_dicts):\n",
    "        # tokenize again every single sample to get access to .word_ids()\n",
    "        tokenization = _tokenizer(sample_dict['tokens'], is_split_into_words=True,\n",
    "                                  padding='max_length', max_length=max_sentence_length, return_tensors='tf')\n",
    "        sample_tokens = _tokenizer.convert_ids_to_tokens(tokenization['input_ids'][0])\n",
    "\n",
    "        sample_labels = []\n",
    "        sample_sample_weights = []\n",
    "        # word index necessary, because one token in PET could be splitted into multiple tokens with tokenizer\n",
    "        # multiple tokens have all the same word_id -> allows retrieval of the same one NER label from PET tokens\n",
    "        for token, word_index in zip(sample_tokens, tokenization.word_ids()):\n",
    "            # set special class for special tokens\n",
    "            if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                sample_labels.append(TC_LABEL_OUT_OF_SCOPE)\n",
    "                sample_sample_weights.append(TC_WEIGHTS_BERT_TOKENS)\n",
    "            else:\n",
    "                token_tag = sample_dict['ner-tags'][word_index]\n",
    "                # XOR\n",
    "                if token_tag.endswith(XOR_GATEWAY):\n",
    "                    sample_labels.append(TC_LABEL_XOR)  # 2\n",
    "                    sample_sample_weights.append(TC_WEIGHTS_GATEWAY_LABELS)\n",
    "                # AND\n",
    "                elif token_tag.endswith(AND_GATEWAY):\n",
    "                    sample_labels.append(TC_LABEL_AND)  # 3\n",
    "                    sample_sample_weights.append(TC_WEIGHTS_GATEWAY_LABELS)\n",
    "                else:\n",
    "                    if label_set == 'filtered':\n",
    "                        sample_labels.append(TC_LABEL_OTHER)\n",
    "                        sample_sample_weights.append(other_labels_weight)\n",
    "                    else:\n",
    "                        sample_sample_weights.append(other_labels_weight)\n",
    "                        if token_tag.endswith(\"O\"):\n",
    "                            sample_labels.append(TC_LABEL_OTHER)\n",
    "                        elif token_tag.endswith(ACTIVITY):\n",
    "                            sample_labels.append(TC_LABEL_ACTIVITY)\n",
    "                        elif token_tag.endswith(ACTIVITY_DATA):\n",
    "                            sample_labels.append(TC_LABEL_ACTIVITY_DATA)\n",
    "                        elif token_tag.endswith(ACTOR):\n",
    "                            sample_labels.append(TC_LABEL_ACTOR)\n",
    "                        elif token_tag.endswith(FURTHER_SPECIFICATION):\n",
    "                            sample_labels.append(TC_LABEL_FURTHER_SPECIFICATION)\n",
    "                        elif token_tag.endswith(CONDITION_SPECIFICATION):\n",
    "                            sample_labels.append(TC_LABEL_CONDITION_SPECIFICATION)\n",
    "                        else:\n",
    "                            raise ValueError(\"Unexpected token tag:\", token_tag)\n",
    "\n",
    "        dataset_sample_weights.append(sample_sample_weights)\n",
    "        dataset_labels.append(sample_labels)\n",
    "        dataset_word_ids.append(tokenization.word_ids())\n",
    "\n",
    "    dataset_labels = tf.constant(dataset_labels)\n",
    "    dataset_sample_weights = tf.constant(dataset_sample_weights)\n",
    "    return dataset_tokens, dataset_labels, dataset_sample_weights, dataset_word_ids\n",
    "\n",
    "\n",
    "def create_token_cls_dataset_full(args: argparse.Namespace) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    create one training dataset of the whole data without separating a dev set\n",
    "    :param args: args namespace\n",
    "    :return: one tensorflow dataset\n",
    "    \"\"\"\n",
    "    logger.info(f\"Create full token classification dataset (batch_size={args.batch_size})\")\n",
    "    \n",
    "    # load samples to include in dataset\n",
    "    sample_ids = _get_sample_ids(strategy=args.sampling_strategy)\n",
    "    random.shuffle(sample_ids)\n",
    "    logger.info(f\"Generate token data with params: sampling_strategy={args.sampling_strategy} - use_synonyms={args.use_synonyms}\"\n",
    "                    f\" - labels={args.labels} - other_labels_weight={args.other_labels_weight}\")\n",
    "    logger.info(f\"Basis are {len(sample_ids)} samples from strategy '{args.sampling_strategy}'\")\n",
    "    \n",
    "    # include synonyms in samples\n",
    "    samples_number_old = len(sample_ids)\n",
    "    if args.use_synonyms:\n",
    "        synonym_samples = [synonyms for original_sample_id, synonyms in synonyms_of_original_samples.items()\n",
    "                           if original_sample_id in sample_ids]\n",
    "        synonym_samples_flattened = [item for sublist in synonym_samples for item in sublist]\n",
    "        sample_ids += synonym_samples_flattened\n",
    "        random.shuffle(sample_ids)\n",
    "\n",
    "    logger.info(f\"Final Dataset -> {len(sample_ids)}{f' ({samples_number_old} without syn.)' if args.use_synonyms else ''}\")            \n",
    "\n",
    "    # create data based on number of samples and transform to tf dataset\n",
    "    tokens, labels, sample_weights, _ = _prepare_data_tc(\n",
    "        sample_numbers=sample_ids,\n",
    "        use_synonyms=args.use_synonyms,\n",
    "        other_labels_weight=args.other_labels_weight,\n",
    "        label_set=args.labels,\n",
    "        activity_masking=args.activity_masking\n",
    "    )\n",
    "    \n",
    "    # create and batch tf dataset\n",
    "    tf_dataset = _create_dataset(tokens[\"input_ids\"], tokens[\"attention_mask\"], labels, sample_weights)\n",
    "    if args.batch_size:\n",
    "        tf_dataset = tf_dataset.batch(args.batch_size)        \n",
    "        \n",
    "    return tf_dataset\n",
    "\n",
    "\n",
    "def create_token_cls_dataset_cv(args: argparse.Namespace) -> List[Tuple[tf.data.Dataset, tf.data.Dataset]]:\n",
    "    \"\"\"\n",
    "    create the dataset for token classification with huggingface transformers bert like models\n",
    "    split into kfolds splits to use for cross validation\n",
    "    :param args: args namespace\n",
    "    :return: list of tuples (train, dev) as tf.data.Dataset objects\n",
    "    \"\"\"\n",
    "    logger.info(f\"Create token classification cv dataset (folds={args.folds} - batch_size={args.batch_size})\")\n",
    "    # load samples to include in dataset\n",
    "    sample_ids = _get_sample_ids(strategy=args.sampling_strategy)\n",
    "    random.shuffle(sample_ids)\n",
    "    logger.info(f\"Generate token data with params: sampling_strategy={args.sampling_strategy} - use_synonyms={args.use_synonyms}\"\n",
    "                    f\" - labels={args.labels} - other_labels_weight={args.other_labels_weight}\")\n",
    "    logger.info(f\"Basis are {len(sample_ids)} samples from strategy '{args.sampling_strategy}'\")\n",
    "    \n",
    "    # create datasets for k fold cross validation\n",
    "    folded_datasets = []\n",
    "        \n",
    "    kfold = KFold(n_splits=5)\n",
    "    for i, (train, dev) in enumerate(kfold.split(sample_ids)):\n",
    "        \n",
    "        train_samples = [p for j, p in enumerate(sample_ids) if j in train]\n",
    "        dev_samples = [p for j, p in enumerate(sample_ids) if j in dev]\n",
    "        \n",
    "        # include synonyms in train samples\n",
    "        train_samples_number_old = len(train_samples)\n",
    "        if args.use_synonyms:\n",
    "            train_synonym_samples = [synonyms for original_sample_id, synonyms in synonyms_of_original_samples.items()\n",
    "                                     if original_sample_id in train_samples]\n",
    "            train_synonym_samples_flattened = [item for sublist in train_synonym_samples for item in sublist]\n",
    "            train_samples += train_synonym_samples_flattened\n",
    "            random.shuffle(train_samples)\n",
    "        \n",
    "        logger.info(f\"Fold {i} -> {len(train_samples)}{f' ({train_samples_number_old} without syn.)' if args.use_synonyms else ''}\"\n",
    "                            f\"/ {len(dev_samples)}\")            \n",
    "        \n",
    "        # create train data based on number of samples and transform to tf dataset\n",
    "        tokens, labels, sample_weights, _ = _prepare_data_tc(\n",
    "            sample_numbers=train_samples,\n",
    "            use_synonyms=args.use_synonyms,\n",
    "            other_labels_weight=args.other_labels_weight,\n",
    "            label_set=args.labels,\n",
    "            activity_masking=args.activity_masking\n",
    "        )\n",
    "        train_tf_dataset = _create_dataset(tokens[\"input_ids\"], tokens[\"attention_mask\"], labels, sample_weights)\n",
    "        \n",
    "        # create dev data based on number of samples and transform to tf dataset\n",
    "        tokens, labels, sample_weights, _ = _prepare_data_tc(\n",
    "            sample_numbers=dev_samples,\n",
    "            use_synonyms=False,\n",
    "            other_labels_weight=args.other_labels_weight,\n",
    "            label_set=args.labels,\n",
    "            activity_masking=args.activity_masking\n",
    "        )\n",
    "        dev_tf_dataset = _create_dataset(tokens[\"input_ids\"], tokens[\"attention_mask\"], labels, sample_weights)\n",
    "        \n",
    "        # batch both datasets\n",
    "        if args.batch_size:\n",
    "            train_tf_dataset = train_tf_dataset.batch(args.batch_size)\n",
    "            dev_tf_dataset = dev_tf_dataset.batch(args.batch_size)\n",
    "        \n",
    "        folded_datasets.append((train_tf_dataset, dev_tf_dataset))\n",
    "        \n",
    "    return folded_datasets\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--batch_size\", default=8, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--folds\", default=2, type=int, help=\"Number of folds in cross validation routine.\")\n",
    "parser.add_argument(\"--labels\", default=ALL, type=str, help=\"Label set to use.\")\n",
    "parser.add_argument(\"--other_labels_weight\", default=0.1, type=float, help=\"Sample weight for non gateway tokens.\")\n",
    "parser.add_argument(\"--sampling_strategy\", default=NORMAL, type=str, help=\"How to sample samples.\")\n",
    "parser.add_argument(\"--use_synonyms\", default=True, type=str, help=\"Include synonym samples.\")\n",
    "parser.add_argument(\"--activity_masking\", default=NOT, type=str, help=\"How to include activity data.\")\n",
    "\n",
    "args_tc = parser.parse_args([] if \"__file__\" not in globals() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "76432167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Data Preparation [Same Gateway CLS]:Create token classification cv dataset (folds=2 - batch_size=8)\n",
      "INFO:Data Preparation [Same Gateway CLS]:Generate token data with params: sampling_strategy=normal - use_synonyms=True - labels=all - other_labels_weight=0.1\n",
      "INFO:Data Preparation [Same Gateway CLS]:Basis are 417 samples from strategy 'normal'\n",
      "INFO:Data Preparation [Same Gateway CLS]:Fold 0 -> 824 (333 without syn.)/ 84\n",
      "INFO:Data Preparation [Same Gateway CLS]:Fold 1 -> 791 (333 without syn.)/ 84\n",
      "INFO:Data Preparation [Same Gateway CLS]:Fold 2 -> 753 (334 without syn.)/ 83\n",
      "INFO:Data Preparation [Same Gateway CLS]:Fold 3 -> 721 (334 without syn.)/ 83\n",
      "INFO:Data Preparation [Same Gateway CLS]:Fold 4 -> 771 (334 without syn.)/ 83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 (batched): train 103 / dev 11\n",
      "Fold 1 (batched): train 99 / dev 11\n",
      "Fold 2 (batched): train 95 / dev 11\n",
      "Fold 3 (batched): train 91 / dev 11\n",
      "Fold 4 (batched): train 97 / dev 11\n"
     ]
    }
   ],
   "source": [
    "folded_datasets_tc = create_token_cls_dataset_cv(args_tc)\n",
    "for i, (train, dev) in enumerate(folded_datasets_tc):\n",
    "    print(f\"Fold {i} (batched): train {len(train)} / dev {len(dev)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "86d0f98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Data Preparation [Same Gateway CLS]:Create full token classification dataset (batch_size=8)\n",
      "INFO:Data Preparation [Same Gateway CLS]:Generate token data with params: sampling_strategy=normal - use_synonyms=True - labels=all - other_labels_weight=0.1\n",
      "INFO:Data Preparation [Same Gateway CLS]:Basis are 417 samples from strategy 'normal'\n",
      "INFO:Data Preparation [Same Gateway CLS]:Final Dataset -> 965 (417 without syn.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 121\n"
     ]
    }
   ],
   "source": [
    "full_dataset_tc = create_token_cls_dataset_full(args_tc)\n",
    "print(f\"Full dataset size (batched): {len(full_dataset_tc)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
