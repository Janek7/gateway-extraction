{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ced632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent dir to sys path for import of modules\n",
    "import os\n",
    "import sys\n",
    "parentdir = os.path.abspath(os.path.join(os.path.abspath(''), os.pardir))\n",
    "sys.path.insert(0, parentdir) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85bfe7",
   "metadata": {},
   "source": [
    "# Key Word Approach\n",
    "variables with prefix ``doc_`` contain data from the dataset\n",
    "variables with prefix ``o_`` contain data from own computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bdd7e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from petreader.RelationsExtraction import RelationsExtraction\n",
    "from petreader.TokenClassification import TokenClassification\n",
    "from petreader import labels\n",
    "from petreader.labels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c49670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\relations-extraction\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022540807723999023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfa21554bb8474e838235b45f405732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pet (C:\\Users\\janek\\.cache\\huggingface\\datasets\\patriziobellan___pet\\token-classification\\1.0.1\\38434e2af57af533c400c8975f37e43c08bb77739085a3c026a862b2efb668d2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " _______ _     _ _______       _____  _______ _______      ______  _______ _______ _______ _______ _______ _______\n",
      "    |    |_____| |______      |_____] |______    |         |     \\ |_____|    |    |_____| |______ |______    |   \n",
      "    |    |     | |______      |       |______    |         |_____/ |     |    |    |     | ______| |______    |   \n",
      "                                                                                                                  \n",
      "Discover more at: [https://pdi.fbk.eu/pet-dataset/]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018558740615844727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354da6037a634599ba49b72259dc7ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relations_dataset = RelationsExtraction()\n",
    "token_dataset = TokenClassification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d905d3",
   "metadata": {},
   "source": [
    "## 1 Prepare Document Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff3eac56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc-1.3', 'doc-1.4', 'doc-1.2', 'doc-1.1', 'doc-10.11', 'doc-10.2', 'doc-10.8', 'doc-10.10', 'doc-10.7', 'doc-10.6', 'doc-10.12', 'doc-10.4', 'doc-10.1', 'doc-10.5', 'doc-10.3', 'doc-10.13', 'doc-10.14', 'doc-10.9', 'doc-2.2', 'doc-2.1', 'doc-3.6', 'doc-3.7', 'doc-3.2', 'doc-3.5', 'doc-3.3', 'doc-3.1', 'doc-3.8', 'doc-4.1', 'doc-5.4', 'doc-5.2', 'doc-5.1', 'doc-5.3', 'doc-6.3', 'doc-6.4', 'doc-6.1', 'doc-6.2', 'doc-7.1', 'doc-8.3', 'doc-8.1', 'doc-8.2', 'doc-9.3', 'doc-9.4', 'doc-9.2', 'doc-9.5', 'doc-9.1']\n"
     ]
    }
   ],
   "source": [
    "doc_names = token_dataset.GetDocumentNames()\n",
    "doc_names.sort(key=lambda name: (int(name[4]), name[5]))\n",
    "\n",
    "print(doc_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc1372",
   "metadata": {},
   "source": [
    "### 1.1 Read Example Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "48d0e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  doc-9.5  ********************\n",
      "After the Expense Report is received , a new account must be created if the employee does not already have one .\n",
      "The report is then reviewed for automatic approval .\n",
      "Amounts under $200 are automatically approved , whereas amounts equal to or over $200 require approval of the supervisor .\n",
      "In case of rejection , the employee must receive a rejection notice by email .\n",
      "Otherwise , the reimbursement goes to the employees direct deposit bank account .\n",
      "If the request is not completed in 7 days , then the employee must receive an approval in progress email .\n",
      "If the request is not finished within 30 days , then the process is stopped and the employee receives an email cancellation notice and must re-submit the expense report .\n"
     ]
    }
   ],
   "source": [
    "doc_name = \"doc-9.5\"  # \"doc-1.2\"  # doc-3.2\n",
    "doc_number = relations_dataset.GetDocumentNumber(doc_name)\n",
    "print(f\"  {doc_name}  \".center(50, '*'))\n",
    "doc_text = token_dataset.GetDocumentText(doc_name)\n",
    "print(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cbe36154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** activities and NER labels (per sentences) ****\n",
      "[[['received'], ['created']], [['reviewed']], [['approved'], ['require']], [['receive']], [['goes']], [['receive']], [['receives'], ['re-submit']]]\n",
      "[[('After', 0, 'O'), ('the', 1, 'B-Activity Data'), ('Expense', 2, 'I-Activity Data'), ('Report', 3, 'I-Activity Data'), ('is', 4, 'O'), ('received', 5, 'B-Activity'), (',', 6, 'O'), ('a', 7, 'B-Activity Data'), ('new', 8, 'I-Activity Data'), ('account', 9, 'I-Activity Data'), ('must', 10, 'O'), ('be', 11, 'O'), ('created', 12, 'B-Activity'), ('if', 13, 'B-XOR Gateway'), ('the', 14, 'B-Condition Specification'), ('employee', 15, 'I-Condition Specification'), ('does', 16, 'I-Condition Specification'), ('not', 17, 'I-Condition Specification'), ('already', 18, 'I-Condition Specification'), ('have', 19, 'I-Condition Specification'), ('one', 20, 'I-Condition Specification'), ('.', 21, 'O')], [('The', 0, 'B-Activity Data'), ('report', 1, 'I-Activity Data'), ('is', 2, 'O'), ('then', 3, 'O'), ('reviewed', 4, 'B-Activity'), ('for', 5, 'B-Further Specification'), ('automatic', 6, 'I-Further Specification'), ('approval', 7, 'I-Further Specification'), ('.', 8, 'O')], [('Amounts', 0, 'B-Condition Specification'), ('under', 1, 'I-Condition Specification'), ('$200', 2, 'I-Condition Specification'), ('are', 3, 'O'), ('automatically', 4, 'B-Further Specification'), ('approved', 5, 'B-Activity'), (',', 6, 'O'), ('whereas', 7, 'B-XOR Gateway'), ('amounts', 8, 'B-Condition Specification'), ('equal', 9, 'I-Condition Specification'), ('to', 10, 'I-Condition Specification'), ('or', 11, 'I-Condition Specification'), ('over', 12, 'I-Condition Specification'), ('$200', 13, 'I-Condition Specification'), ('require', 14, 'B-Activity'), ('approval', 15, 'B-Activity Data'), ('of', 16, 'O'), ('the', 17, 'B-Actor'), ('supervisor', 18, 'I-Actor'), ('.', 19, 'O')], [('In', 0, 'B-XOR Gateway'), ('case', 1, 'I-XOR Gateway'), ('of', 2, 'I-XOR Gateway'), ('rejection', 3, 'B-Condition Specification'), (',', 4, 'O'), ('the', 5, 'B-Actor'), ('employee', 6, 'I-Actor'), ('must', 7, 'O'), ('receive', 8, 'B-Activity'), ('a', 9, 'B-Activity Data'), ('rejection', 10, 'I-Activity Data'), ('notice', 11, 'I-Activity Data'), ('by', 12, 'B-Further Specification'), ('email', 13, 'I-Further Specification'), ('.', 14, 'O')]]\n",
      "************* same gateway relations *************\n",
      "source-head-sentence-ID: 3\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['In', 'case', 'of']\n",
      "target-head-sentence-ID: 4\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['Otherwise']\n",
      "\n",
      "source-head-sentence-ID: 4\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['Otherwise']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_activities = token_dataset.GetDocumentActivities(doc_name)\n",
    "print(\" activities and NER labels (per sentences) \".center(50, '*'))\n",
    "print(doc_activities)\n",
    "doc_sentence_ner_labels = relations_dataset.GetSentencesWithIdsAndNerTagLabels(doc_number)\n",
    "print(doc_sentence_ner_labels[:4])\n",
    "doc_relations = relations_dataset.GetRelations(doc_number)\n",
    "doc_flow_relations, doc_same_gateway_relations = doc_relations[labels.FLOW], doc_relations[labels.SAME_GATEWAY]\n",
    "\n",
    "print(\" same gateway relations \".center(50, '*'))\n",
    "for same_gateway_relation in doc_same_gateway_relations:\n",
    "    for key, value in same_gateway_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b84532",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9b8a529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0 After the Expense Report is received , a new account must be created if the employee does not already have one\n",
      "1 The report is then reviewed for automatic approval\n",
      "2 Amounts under $200 are automatically approved , whereas amounts equal to or over $200 require approval of the supervisor\n",
      "3 In case of rejection , the employee must receive a rejection notice by email\n",
      "4 Otherwise , the reimbursement goes to the employees direct deposit bank account\n",
      "5 If the request is not completed in 7 days , then the employee must receive an approval in progress email\n",
      "6 If the request is not finished within 30 days , then the process is stopped and the employee receives an email cancellation notice and must re-submit the expense report\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(doc_activities) # activities is 2 dim list (one per sentence)\n",
    "print(num_sentences)\n",
    "doc_sentences_raw = [sentence.strip() for sentence in doc_text.split(\".\") if sentence.strip() != \"\"]\n",
    "for i, s in enumerate(doc_sentences_raw):\n",
    "    print(i, s)\n",
    "assert num_sentences == len(doc_sentences_raw)  # check if number of extracted sentences == from dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a98c16",
   "metadata": {},
   "source": [
    "### 1.3 Filter Gateways Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5b7a2435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('if', 13, 'B-XOR Gateway')], [], [('whereas', 7, 'B-XOR Gateway')], [('In', 0, 'B-XOR Gateway'), ('case', 1, 'I-XOR Gateway'), ('of', 2, 'I-XOR Gateway')], [('Otherwise', 0, 'B-XOR Gateway')], [('If', 0, 'B-XOR Gateway')], [('If', 0, 'B-XOR Gateway')]]\n",
      "[[], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "def filter_ner_labels(sentence_ner_labels, target_label):\n",
    "    return [[token for token in s_list if target_label in token[2]]\n",
    "                        for s_list in sentence_ner_labels]\n",
    "\n",
    "doc_xor_gateway = filter_ner_labels(doc_sentence_ner_labels, labels.XOR_GATEWAY)\n",
    "doc_and_gateway = filter_ner_labels(doc_sentence_ner_labels, labels.AND_GATEWAY)\n",
    "print(doc_xor_gateway)\n",
    "print(doc_and_gateway)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a1ea2",
   "metadata": {},
   "source": [
    "### 1.4 Filter Sequence Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "9f1ecabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow relations involving XOR gateways 18; AND gateways 0; overall gateways 18; overall 21\n"
     ]
    }
   ],
   "source": [
    "def filter_flow_relations(flow_relations, entity_type_list):\n",
    "    \"\"\"\n",
    "    filter list of flow relations (single dictionaries) for source or target entity type = given entity tyoe\n",
    "    \"\"\"\n",
    "    return [flow_relation for flow_relation in flow_relations if flow_relation[labels.SOURCE_ENTITY_TYPE] in entity_type_list\n",
    "                                                               or flow_relation[labels.TARGET_ENTITY_TYPE] in entity_type_list]\n",
    "\n",
    "doc_flow_relations_xor = filter_flow_relations(doc_flow_relations, [labels.XOR_GATEWAY, labels.CONDITION_SPECIFICATION])\n",
    "doc_flow_relations_and = filter_flow_relations(doc_flow_relations, [labels.AND_GATEWAY])\n",
    "doc_flow_relations_gateways = filter_flow_relations(doc_flow_relations, [labels.XOR_GATEWAY, labels.AND_GATEWAY, labels.CONDITION_SPECIFICATION])\n",
    "print(f\"Flow relations involving XOR gateways {len(doc_flow_relations_xor)}; \"\\\n",
    "      f\"AND gateways {len(doc_flow_relations_and)}; overall gateways {len(doc_flow_relations_gateways)}; overall {len(doc_flow_relations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4b20d",
   "metadata": {},
   "source": [
    "### 1.5 Enrich activities with token index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "da4cae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_activity_tokens = []\n",
    "for i, (tokens, activities) in enumerate(zip(doc_sentence_ner_labels, doc_activities)):\n",
    "    sentence_activity_tokens = []\n",
    "    # note: activity is a list because it could consist of more words (neglect here)\n",
    "    for activity in activities:\n",
    "        activity_token_triple = [token_triple for token_triple in tokens if token_triple[0] == activity[0]][0]\n",
    "        sentence_activity_tokens.append((activity, activity_token_triple[1]))\n",
    "    doc_activity_tokens.append(sentence_activity_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4e627",
   "metadata": {},
   "source": [
    "## 2 Extract Gateways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d4d41",
   "metadata": {},
   "source": [
    "### 2.1 Key Word List\n",
    "#### A) take words from all existing gateways in PET dataset as gold list for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d0cb69a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR gold (15) ['either', 'for', 'for each patient for which', 'for the case', 'if', 'in case', 'in case of', 'in the case of', 'it can also happen that', 'or', 'otherwise', 'should', 'sometimes', 'under certain circumstances', 'whereas']\n",
      "AND gold (6) ['at the same time', 'in the meantime', 'meantime', 'two concurrent activities are triggered', 'whereas', 'while']\n"
     ]
    }
   ],
   "source": [
    "def get_gateway_key_words(dataset_gateway_list):\n",
    "    flattened = list(itertools.chain(*dataset_gateway_list))\n",
    "    phrases = [\" \".join(g).lower() for g in flattened]  # join phrases together if multiple words\n",
    "    unique = list(set(phrases))\n",
    "    unique.sort()\n",
    "    return unique\n",
    "\n",
    "xor_key_words_gold = get_gateway_key_words(token_dataset.GetXORGateways())\n",
    "and_key_words_gold = get_gateway_key_words(token_dataset.GetANDGateways())\n",
    "\n",
    "print(f\"XOR gold ({len(xor_key_words_gold)})\", xor_key_words_gold)\n",
    "print(f\"AND gold ({len(and_key_words_gold)})\", and_key_words_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efcd79",
   "metadata": {},
   "source": [
    "#### B) Curated List from Literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "744985cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR literature (14) ['either', 'if', 'if not', 'in case', 'in case of', 'only', 'only if', 'or', 'otherwise', 'till', 'unless', 'until', 'when', 'whether']\n",
      "AND literature (11) ['at the same time', 'concurrently', 'in addition to', 'in parallel', 'in parallel with this', 'in the meantime', 'meantime', 'meanwhile', 'simultaneously', 'whereas', 'while']\n"
     ]
    }
   ],
   "source": [
    "# Ferreira et al. 2017\n",
    "with open('data/keywords/literature_xor.txt') as f:\n",
    "    xor_key_words_literature = f.read().splitlines()\n",
    "    xor_key_words_literature.sort()\n",
    "\n",
    "with open('data/keywords/literature_and.txt') as f:\n",
    "    and_key_words_literature = f.read().splitlines()\n",
    "    and_key_words_literature.sort()   \n",
    "\n",
    "print(f\"XOR literature ({len(xor_key_words_literature)})\", xor_key_words_literature)\n",
    "print(f\"AND literature ({len(and_key_words_literature)})\", and_key_words_literature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20306e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small company manufactures customized bicycles\n",
      "Whenever the sales department receives an order , a new process instance is created\n",
      "A member of the sales department can then reject or accept the order for a customized bike\n",
      "In the former case , the process instance is finished\n",
      "In the latter case , the storehouse and the engineering department are informed\n",
      "The storehouse immediately processes the part list of the order and checks the required quantity of each part\n",
      "If the part is available in-house , it is reserved\n",
      "If it is not available , it is back-ordered\n",
      "This procedure is repeated for each item on the part list\n",
      "In the meantime , the engineering department prepares everything for the assembling of the ordered bicycle\n",
      "If the storehouse has successfully reserved or back-ordered every item of the part list and the preparation activity has finished , the engineering department assembles the bicycle\n",
      "Afterwards , the sales department ships the bicycle to the customer and finishes the process instance\n"
     ]
    }
   ],
   "source": [
    "for s in doc_sentences_raw:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8d965",
   "metadata": {},
   "source": [
    "### 2.2 Extraction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a421ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gateways(sentence_list, key_words, target_gateway_label):\n",
    "    \"\"\"\n",
    "    extracts gateways in a key-word-based manner given a document structured in a list of sentences\n",
    "    if two phrases would match to a token (e.g. 'in the meantime' and 'meantime'), the longer phrase is extracted\n",
    "    target_gateway_label: str, must be 'XOR Gateway' or 'AND Gateway'\n",
    "    \n",
    "    result list is two dimensional -> list of tuples (word, position in sentence, tag) for each sentence\n",
    "    this produces the same structure as sentences and their NER labels are annotated in PET dataset\n",
    "    \"\"\"\n",
    "    gateways = []\n",
    "    benchmark_gateways = []\n",
    "    # sort key words descending by length of words in phrase\n",
    "    key_words.sort(key=lambda key_word_phrase: len(key_word_phrase.split(\" \")), reverse=True)\n",
    "\n",
    "    # 1) extract gateways\n",
    "    for s_idx, sentence in enumerate(sentence_list):\n",
    "        # print(f\" SENTENCE {s_idx} \".center(50, '-'))\n",
    "        # print(sentence_list[s_idx])\n",
    "        sentence_gateways = []\n",
    "        sentence_to_search = f\" {sentence.lower()} \"  # lowercase and wrap with spaces for search of key words\n",
    "        tokens = sentence.split(\" \")\n",
    "        tokens_lower = sentence.lower().split(\" \")\n",
    "        tokens_already_matched_with_key_phrase = []\n",
    "\n",
    "        # iterate over key phrases\n",
    "        for key_phrase in key_words:\n",
    "            key_phrase_to_search = f\" {key_phrase} \"\n",
    "\n",
    "            # if key phrase is in sentence, search index and extract\n",
    "            if key_phrase_to_search in sentence_to_search:\n",
    "                key_phrase_tokens = key_phrase.split(\" \")\n",
    "                \n",
    "                # check key phrase for every token\n",
    "                for t_idx, token in enumerate(tokens_lower):\n",
    "                    candidate = True\n",
    "                    # iterate over key phrase tokens in case of multiple world phrase\n",
    "                    for key_phrase_token_idx, key_phrase_token in enumerate(key_phrase_tokens):\n",
    "                        # check if token is not part of key phrase or token is already matched with another phrase\n",
    "                        # if yes, stop processing candidate\n",
    "                        if not tokens_lower[t_idx + key_phrase_token_idx] == key_phrase_token or \\\n",
    "                            t_idx + key_phrase_token_idx in tokens_already_matched_with_key_phrase:\n",
    "                            candidate = False\n",
    "                            break\n",
    "                    \n",
    "                    # add tokens to result only if all tokens are matched and not already part of a longer phrase\n",
    "                    if candidate:\n",
    "                        for i, key_phrase_token in enumerate(key_phrase_tokens):\n",
    "                            prefix = \"B\" if i == 0 else \"I\"\n",
    "                            # append tuples with extract information as in PET and process information about gateways to filter later\n",
    "                            sentence_gateways.append((tokens[t_idx + i], t_idx + i, f\"{prefix}-{labels.XOR_GATEWAY}\"))\n",
    "                            tokens_already_matched_with_key_phrase.append(t_idx + i)\n",
    "                            benchmark_gateways.append([tokens[t_idx + i] for i, x in enumerate(key_phrase_tokens)])\n",
    "\n",
    "        sentence_gateways.sort(key=lambda gateway_triple: gateway_triple[1])\n",
    "        gateways.append(sentence_gateways)\n",
    "\n",
    "    return gateways, benchmark_gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ea280708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR GATEWAYS\n",
      "0 [('if', 13, 'B-XOR Gateway')]\n",
      "1 []\n",
      "2 [('or', 11, 'B-XOR Gateway')]\n",
      "3 [('In', 0, 'B-XOR Gateway'), ('case', 1, 'I-XOR Gateway'), ('of', 2, 'I-XOR Gateway')]\n",
      "4 [('Otherwise', 0, 'B-XOR Gateway')]\n",
      "5 [('If', 0, 'B-XOR Gateway')]\n",
      "6 [('If', 0, 'B-XOR Gateway')]\n",
      "\n",
      "AND GATEWAYS\n",
      "0 []\n",
      "1 []\n",
      "2 [('whereas', 7, 'B-XOR Gateway')]\n",
      "3 []\n",
      "4 []\n",
      "5 []\n",
      "6 []\n"
     ]
    }
   ],
   "source": [
    "# available key word lists: xor_key_words_gold, and_key_words_gold, xor_key_words_literature, and_key_words_literature\n",
    "o_xor_gateways, o_xor_gateways_benchmark = extract_gateways(doc_sentences_raw, xor_key_words_literature, labels.XOR_GATEWAY)\n",
    "o_and_gateways, o_and_gateways_benchmark = extract_gateways(doc_sentences_raw, and_key_words_literature, labels.AND_GATEWAY)\n",
    "\n",
    "print(\"XOR GATEWAYS\")\n",
    "for idx, sentence_gateways in enumerate(o_xor_gateways):\n",
    "    print(idx, sentence_gateways)\n",
    "print(\"\\nAND GATEWAYS\")\n",
    "for idx, sentence_gateways in enumerate(o_and_gateways):\n",
    "    print(idx, sentence_gateways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7a5cd",
   "metadata": {},
   "source": [
    "## 3 Extract Control Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc9175",
   "metadata": {},
   "source": [
    "### 3.1 Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd5d83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flow_relation_representation(sentence_idx, token_idx, entity_type, entity, source=True):\n",
    "    if source:\n",
    "        return {\n",
    "            labels.SOURCE_SENTENCE_ID: sentence_idx,\n",
    "            labels.SOURCE_HEAD_TOKEN_ID: token_idx,\n",
    "            labels.SOURCE_ENTITY_TYPE: entity_type,\n",
    "            labels.SOURCE_ENTITY: entity\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            labels.TARGET_SENTENCE_ID: sentence_idx,\n",
    "            labels.TARGET_HEAD_TOKEN_ID: token_idx,\n",
    "            labels.TARGET_ENTITY_TYPE: entity_type,\n",
    "            labels.TARGET_ENTITY: entity\n",
    "        }\n",
    "        \n",
    "\n",
    "def merge_source_target_dicts(source_dict, target_dict):\n",
    "    return {**source_dict, **target_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6442043",
   "metadata": {},
   "source": [
    "### 3.2 Involving AND gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "051e3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) METHODS FOR EXTRACTING THE PREVIOUS (INCL. SECOND PREVIOUS) AND NEXT ACTIVITY\n",
    "\n",
    "def get_previous_activity(sentence_idx, token_idx, doc_activity_tokens, skip_first=False, one_already_found=False):\n",
    "    \"\"\"\n",
    "    search recursive for the second last previous activity from a start point defined by sentence_idx and token_idx\n",
    "    sentence_idx: sentence index where to start the search\n",
    "    token_idx: token index where to stat the search\n",
    "    doc_activity_tokens: list of activity lists (describes whole document)\n",
    "    skip_first: True if searching for the second previous activity, False (default) when searching for the previous activity\n",
    "    one_already_found: flag if one activity was already found and skipped for return in course of search for the second previous\n",
    "    \n",
    "    returns: triple of (sentence idx, token_idx, token)\n",
    "    \"\"\"\n",
    "    # search for activities left to the token in target sentence if token is given else in the whole\n",
    "    if token_idx is not None:\n",
    "        previous_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx] if a_t[1] < token_idx]\n",
    "    else:\n",
    "        previous_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx]]\n",
    "    \n",
    "    if previous_activities_sentence:\n",
    "        # return when just searching the first last activity OR when one was already found before\n",
    "        previous_activity = previous_activities_sentence[-1]\n",
    "        # A) base case: activity found\n",
    "        if not skip_first or one_already_found:\n",
    "            return (sentence_idx, previous_activity[1], previous_activity[0])\n",
    "        # B) recursive case: continue search for second previous activity at index of previous activity\n",
    "        else:\n",
    "            return get_previous_activity(sentence_idx, previous_activity[1], doc_activity_tokens, one_already_found=True)\n",
    "    # B) recursive case: continue search for previous activity in previous sentence\n",
    "    else:\n",
    "        next_sentence_idx = sentence_idx - 1\n",
    "        # no sentences any more to search\n",
    "        if next_sentence_idx == -1:\n",
    "            return None\n",
    "        # otherwise search recursively the previous sentence\n",
    "        else:\n",
    "            return get_previous_activity(next_sentence_idx, None, doc_activity_tokens, \n",
    "                                         skip_first=skip_first, one_already_found=one_already_found)\n",
    "\n",
    "def get_following_activity(sentence_idx, token_idx, doc_activity_tokens, skip_first=False, one_already_found=False):\n",
    "    \"\"\"\n",
    "    search recursive for the next following activity from a start point defined by sentence_idx and token_idx\n",
    "    sentence_idx: sentence index where to start the search\n",
    "    token_idx: token index where to stat the search\n",
    "    doc_activity_tokens: list of activity lists (describes whole document)\n",
    "    \n",
    "    returns: triple of (sentence idx, token_idx, token)\n",
    "    \"\"\"\n",
    "    # search for activities right to the token in target sentence if token is given else in the whole\n",
    "    if token_idx is not None:\n",
    "        following_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx] if a_t[1] > token_idx]\n",
    "    else:\n",
    "        following_activities_sentence = [a_t for a_t in doc_activity_tokens[sentence_idx]]\n",
    "\n",
    "    # if activities were found, take the first one\n",
    "    if following_activities_sentence:\n",
    "        # return when just searching the first following activity OR when one was already found before\n",
    "        following_activity = following_activities_sentence[0]\n",
    "        # 1a) base case: activity found\n",
    "        if not skip_first or one_already_found:\n",
    "            return (sentence_idx, following_activity[1], following_activity[0])\n",
    "        # 2a) recursive case: continue search for second following activity at index of following activity\n",
    "        else:\n",
    "            return get_following_activity(sentence_idx, following_activity[1], doc_activity_tokens,\n",
    "                                          one_already_found=True)\n",
    "\n",
    "    else:\n",
    "        next_sentence_idx = sentence_idx + 1\n",
    "        # 1b) base case: no sentences any more to search\n",
    "        if next_sentence_idx == len(doc_activity_tokens):\n",
    "            return None\n",
    "        # 2b) recursive case: continue search for following activity in following sentence\n",
    "        else:\n",
    "            return get_following_activity(next_sentence_idx, None, doc_activity_tokens,\n",
    "                                                skip_first=skip_first, one_already_found=one_already_found)\n",
    "\n",
    "        \n",
    "# 2) EXTRACT RELATIONS\n",
    "def _extract_concurrent_flows(sentences, doc_activity_tokens, own_gateways):\n",
    "    \"\"\"\n",
    "    extract flow relations for already found AND gateways following the logic:\n",
    "    + for every gateway, to extract parallel branches, add relation to next activity after and before, because\n",
    "      thats the pattern how AND key phrases are usually used (oriented by rules of Ferreira et al. 2017)\n",
    "    + for each case, check over borders if not found in same sentence\n",
    "    + to extract the flow relation that points to the gateway merge point, take the second before\n",
    "    + Assumption: only one parallel gateway per sentence\n",
    "\n",
    "    sentences: list of sentences (used only for debugging)\n",
    "    doc_activity_tokens: list of activity tokens (word, idx) for each sentence\n",
    "    own_gateways: list of own extracted gateway for each sentence\n",
    "    \n",
    "    return: list of flow relations in source/target dict representation\n",
    "    \"\"\"\n",
    "    relations = []\n",
    "    \n",
    "    for s_idx, (sentence, activity_tokens, gateways) in enumerate(zip(sentences, doc_activity_tokens, own_gateways)):\n",
    "        if gateways:\n",
    "            # assume only one gateway\n",
    "            gateway_lead_token = gateways[0]\n",
    "            gateway_entity = [g[0] for g in gateways]\n",
    "            \n",
    "            # 1) Find related activities (previous and following are concurrent activities; second previous the one\n",
    "            # before the gateway; second following the one after the gateway)\n",
    "            pa = get_previous_activity(s_idx, gateway_lead_token[1], doc_activity_tokens)\n",
    "            ppa = get_previous_activity(s_idx, gateway_lead_token[1], doc_activity_tokens, skip_first=True)\n",
    "            fa = get_following_activity(s_idx, gateway_lead_token[1], doc_activity_tokens)\n",
    "            ffa = get_following_activity(s_idx, gateway_lead_token[1], doc_activity_tokens, skip_first=True)\n",
    "            \n",
    "            # 2) Get representations for flow object dictionaries\n",
    "            g_source = get_flow_relation_representation(s_idx, gateway_lead_token[1], AND_GATEWAY, gateway_entity,\n",
    "                                                  source=True)\n",
    "            g_target = get_flow_relation_representation(s_idx, gateway_lead_token[1], AND_GATEWAY, gateway_entity,\n",
    "                                                  source=False)\n",
    "            if pa: # could be None if at document start\n",
    "                pa_target = get_flow_relation_representation(pa[0], pa[1], ACTIVITY, pa[2], source=False)\n",
    "                pa_source = get_flow_relation_representation(pa[0], pa[1], ACTIVITY, pa[2], source=True)\n",
    "            if ppa: # could be None if at document start\n",
    "                ppa_source = get_flow_relation_representation(ppa[0], ppa[1], ACTIVITY, ppa[2], source=True)\n",
    "            if fa: # could be None if at document end\n",
    "                fa_target = get_flow_relation_representation(fa[0], fa[1], ACTIVITY, fa[2], source=False)\n",
    "                fa_source = get_flow_relation_representation(fa[0], fa[1], ACTIVITY, fa[2], source=True)\n",
    "            if ffa: # could be None if at document end\n",
    "                ffa_target = get_flow_relation_representation(ffa[0], ffa[1], ACTIVITY, ffa[2], source=False)\n",
    "\n",
    "            # 3) Create relations\n",
    "            # a) flow to gateway: second previous -> gateway\n",
    "            if ppa: # could be None if at document start\n",
    "                relations.append(merge_source_target_dicts(ppa_source, g_target))\n",
    "            # b) split into concurrent gateway branches: gateway -> previous; gateway -> following\n",
    "            # following two None checks (probably) wont never be False, but for safety included\n",
    "            if pa: # could be None if at document start\n",
    "                relations.append(merge_source_target_dicts(g_source, pa_target))\n",
    "            if fa:  # could be None if at document end\n",
    "                relations.append(merge_source_target_dicts(g_source, fa_target))\n",
    "            # c) merge branches together: previous -> second following; following -> second following\n",
    "            if ffa: # could be None if at document end\n",
    "                relations.append(merge_source_target_dicts(pa_source, ffa_target))\n",
    "                relations.append(merge_source_target_dicts(fa_source, ffa_target))\n",
    "\n",
    "    return relations\n",
    "\n",
    "o_flow_relations_and = _extract_concurrent_flows(doc_sentences_raw, doc_activity_tokens, o_and_gateways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0807c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, flow_relation in enumerate(o_flow_relations_and):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "24ddc181",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 9\n",
      "source-entity-type: Activity\n",
      "source-entity: ['initiated']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 15\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['meantime']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 17\n",
      "source-entity-type: Activity\n",
      "source-entity: ['tracked']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 15\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['meantime']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 15\n",
      "source-entity-type: AND Gateway\n",
      "source-entity: ['meantime']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 9\n",
      "target-entity-type: Activity\n",
      "target-entity: ['handed']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 15\n",
      "source-entity-type: AND Gateway\n",
      "source-entity: ['meantime']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 20\n",
      "target-entity-type: Activity\n",
      "target-entity: ['distributed']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# control:\n",
    "for i, flow_relation in enumerate(doc_flow_relations_and):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea3914",
   "metadata": {},
   "source": [
    "### 3.2 Involving XOR gateways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e5d2f",
   "metadata": {},
   "source": [
    "#### Input B): Extracted Gateways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5b762d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [('for', 23, 'B-XOR Gateway')]\n",
      "1 [('If', 0, 'B-XOR Gateway'), ('otherwise', 11, 'B-XOR Gateway')]\n",
      "2 []\n",
      "3 []\n"
     ]
    }
   ],
   "source": [
    "for i, sentence_gateways in enumerate(o_xor_gateways):\n",
    "    print(i, sentence_gateways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32e20b",
   "metadata": {},
   "source": [
    "#### Input A): Activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "072e71ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Each morning , the files which have yet to be processed need to be checked , to make sure they are in order for the court hearing that day\n",
      "1 If some files are missing , a search is initiated , otherwise the files can be physically tracked to the intended location\n",
      "2 Once all the files are ready , these are handed to the Associate , and meantime the Judgeis Lawlist is distributed to the relevant people\n",
      "3 Afterwards , the directions hearings are conducted\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(doc_sentences_raw):\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "66f76530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [['checked']]\n",
      "1 [['initiated'], ['tracked']]\n",
      "2 [['handed'], ['distributed']]\n",
      "3 [['conducted']]\n"
     ]
    }
   ],
   "source": [
    "for i, sentence_activities in enumerate(doc_activities):\n",
    "    print(i, sentence_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33292b08",
   "metadata": {},
   "source": [
    "#### Gold Data: Flow Relations that involve AND Gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a97b3f64",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "source-head-sentence-ID: 0\n",
      "source-head-word-ID: 14\n",
      "source-entity-type: Activity\n",
      "source-entity: ['checked']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 1\n",
      "target-entity-type: Condition Specification\n",
      "target-entity: ['some', 'files', 'are', 'missing']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 1\n",
      "source-entity-type: Condition Specification\n",
      "source-entity: ['some', 'files', 'are', 'missing']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 9\n",
      "target-entity-type: Activity\n",
      "target-entity: ['initiated']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 9\n",
      "source-entity-type: Activity\n",
      "source-entity: ['initiated']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 15\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['meantime']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 11\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['otherwise']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 17\n",
      "target-entity-type: Activity\n",
      "target-entity: ['tracked']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 17\n",
      "source-entity-type: Activity\n",
      "source-entity: ['tracked']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 15\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['meantime']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 9\n",
      "source-entity-type: Activity\n",
      "source-entity: ['handed']\n",
      "target-head-sentence-ID: 3\n",
      "target-head-word-ID: 6\n",
      "target-entity-type: Activity\n",
      "target-entity: ['conducted']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 15\n",
      "source-entity-type: AND Gateway\n",
      "source-entity: ['meantime']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 9\n",
      "target-entity-type: Activity\n",
      "target-entity: ['handed']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 15\n",
      "source-entity-type: AND Gateway\n",
      "source-entity: ['meantime']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 20\n",
      "target-entity-type: Activity\n",
      "target-entity: ['distributed']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 20\n",
      "source-entity-type: Activity\n",
      "source-entity: ['distributed']\n",
      "target-head-sentence-ID: 3\n",
      "target-head-word-ID: 6\n",
      "target-entity-type: Activity\n",
      "target-entity: ['conducted']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_flow_relations))\n",
    "for i, flow_relation in enumerate(doc_flow_relations):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "31aac50b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source-head-sentence-ID: 0\n",
      "source-head-word-ID: 14\n",
      "source-entity-type: Activity\n",
      "source-entity: ['hands', 'out']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 1\n",
      "target-entity-type: Condition Specification\n",
      "target-entity: ['the', 'customer', 'decides', 'that', 'the', 'costs', 'are', 'acceptable']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 1\n",
      "source-entity-type: Condition Specification\n",
      "source-entity: ['the', 'customer', 'decides', 'that', 'the', 'costs', 'are', 'acceptable']\n",
      "target-head-sentence-ID: 3\n",
      "target-head-word-ID: 11\n",
      "target-entity-type: AND Gateway\n",
      "target-entity: ['whereas']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 14\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['otherwise']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 16\n",
      "target-entity-type: Activity\n",
      "target-entity: ['takes']\n",
      "\n",
      "source-head-sentence-ID: 4\n",
      "source-head-word-ID: 11\n",
      "source-entity-type: Activity\n",
      "source-entity: ['tested']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n",
      "source-head-sentence-ID: 5\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 1\n",
      "target-entity-type: Condition Specification\n",
      "target-entity: ['an', 'error', 'is', 'detected']\n",
      "\n",
      "source-head-sentence-ID: 5\n",
      "source-head-word-ID: 1\n",
      "source-entity-type: Condition Specification\n",
      "source-entity: ['an', 'error', 'is', 'detected']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 10\n",
      "target-entity-type: Activity\n",
      "target-entity: ['executed']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, flow_relation in enumerate(doc_flow_relations_xor): # (doc_flow_relations_xor):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "20d2617b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [('for', 23, 'B-XOR Gateway')]\n",
      "1 [('If', 0, 'B-XOR Gateway'), ('otherwise', 11, 'B-XOR Gateway')]\n",
      "2 []\n",
      "3 []\n"
     ]
    }
   ],
   "source": [
    "for i, sentence_gateways in enumerate(o_xor_gateways):\n",
    "    print(i, sentence_gateways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2801ce97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 After the Expense Report is received , a new account must be created if the employee does not already have one\n",
      "1 The report is then reviewed for automatic approval\n",
      "2 Amounts under $200 are automatically approved , whereas amounts equal to or over $200 require approval of the supervisor\n",
      "3 In case of rejection , the employee must receive a rejection notice by email\n",
      "4 Otherwise , the reimbursement goes to the employees direct deposit bank account\n",
      "5 If the request is not completed in 7 days , then the employee must receive an approval in progress email\n",
      "6 If the request is not finished within 30 days , then the process is stopped and the employee receives an email cancellation notice and must re-submit the expense report\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(doc_sentences_raw):\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "10e04028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "own extracted XOR gateways: [[('if', 13, 'B-XOR Gateway')], [], [('or', 11, 'B-XOR Gateway')], [('In', 0, 'B-XOR Gateway'), ('case', 1, 'I-XOR Gateway'), ('of', 2, 'I-XOR Gateway')], [('Otherwise', 0, 'B-XOR Gateway')], [('If', 0, 'B-XOR Gateway')], [('If', 0, 'B-XOR Gateway')]]\n",
      "------------------------------\n",
      "------------------------------\n",
      "source-head-sentence-ID: 0\n",
      "source-head-word-ID: 5\n",
      "source-entity-type: Activity\n",
      "source-entity: ['received']\n",
      "target-head-sentence-ID: 0\n",
      "target-head-word-ID: 13\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['if']\n",
      "\n",
      "source-head-sentence-ID: 0\n",
      "source-head-word-ID: 13\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['if']\n",
      "target-head-sentence-ID: 0\n",
      "target-head-word-ID: 12\n",
      "target-entity-type: Activity\n",
      "target-entity: ['created']\n",
      "\n",
      "source-head-sentence-ID: 0\n",
      "source-head-word-ID: 13\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['if']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 4\n",
      "target-entity-type: Activity\n",
      "target-entity: ['reviewed']\n",
      "\n",
      "source-head-sentence-ID: 0\n",
      "source-head-word-ID: 12\n",
      "source-entity-type: Activity\n",
      "source-entity: ['created']\n",
      "target-head-sentence-ID: 1\n",
      "target-head-word-ID: 4\n",
      "target-entity-type: Activity\n",
      "target-entity: ['reviewed']\n",
      "\n",
      "source-head-sentence-ID: 1\n",
      "source-head-word-ID: 4\n",
      "source-entity-type: Activity\n",
      "source-entity: ['reviewed']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 11\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['or']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 14\n",
      "source-entity-type: Activity\n",
      "source-entity: ['require']\n",
      "target-head-sentence-ID: 3\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['In', 'case', 'of']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 11\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['or']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 5\n",
      "target-entity-type: Activity\n",
      "target-entity: ['approved']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 11\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['or']\n",
      "target-head-sentence-ID: 2\n",
      "target-head-word-ID: 14\n",
      "target-entity-type: Activity\n",
      "target-entity: ['require']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 5\n",
      "source-entity-type: Activity\n",
      "source-entity: ['approved']\n",
      "target-head-sentence-ID: 3\n",
      "target-head-word-ID: 8\n",
      "target-entity-type: Activity\n",
      "target-entity: ['receive']\n",
      "\n",
      "source-head-sentence-ID: 2\n",
      "source-head-word-ID: 14\n",
      "source-entity-type: Activity\n",
      "source-entity: ['require']\n",
      "target-head-sentence-ID: 3\n",
      "target-head-word-ID: 8\n",
      "target-entity-type: Activity\n",
      "target-entity: ['receive']\n",
      "\n",
      "source-head-sentence-ID: 3\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['In', 'case', 'of']\n",
      "target-head-sentence-ID: 3\n",
      "target-head-word-ID: 8\n",
      "target-entity-type: Activity\n",
      "target-entity: ['receive']\n",
      "\n",
      "source-head-sentence-ID: 3\n",
      "source-head-word-ID: 8\n",
      "source-entity-type: Activity\n",
      "source-entity: ['receive']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 14\n",
      "target-entity-type: Activity\n",
      "target-entity: ['receive']\n",
      "\n",
      "source-head-sentence-ID: 4\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['Otherwise']\n",
      "target-head-sentence-ID: 4\n",
      "target-head-word-ID: 4\n",
      "target-entity-type: Activity\n",
      "target-entity: ['goes']\n",
      "\n",
      "source-head-sentence-ID: 4\n",
      "source-head-word-ID: 4\n",
      "source-entity-type: Activity\n",
      "source-entity: ['goes']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 14\n",
      "target-entity-type: Activity\n",
      "target-entity: ['receive']\n",
      "\n",
      "source-head-sentence-ID: 4\n",
      "source-head-word-ID: 4\n",
      "source-entity-type: Activity\n",
      "source-entity: ['goes']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n",
      "source-head-sentence-ID: 5\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 5\n",
      "target-head-word-ID: 14\n",
      "target-entity-type: Activity\n",
      "target-entity: ['receive']\n",
      "\n",
      "source-head-sentence-ID: 5\n",
      "source-head-word-ID: 14\n",
      "source-entity-type: Activity\n",
      "source-entity: ['receive']\n",
      "target-head-sentence-ID: 6\n",
      "target-head-word-ID: 25\n",
      "target-entity-type: Activity\n",
      "target-entity: ['re-submit']\n",
      "\n",
      "source-head-sentence-ID: 6\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 6\n",
      "target-head-word-ID: 18\n",
      "target-entity-type: Activity\n",
      "target-entity: ['receives']\n",
      "\n",
      "source-head-sentence-ID: 6\n",
      "source-head-word-ID: 18\n",
      "source-entity-type: Activity\n",
      "source-entity: ['receives']\n",
      "target-head-sentence-ID: 6\n",
      "target-head-word-ID: 25\n",
      "target-entity-type: Activity\n",
      "target-entity: ['re-submit']\n",
      "\n",
      "------------------------------\n",
      "source-head-sentence-ID: 3\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['In', 'case', 'of']\n",
      "target-head-sentence-ID: 4\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['Otherwise']\n",
      "\n",
      "source-head-sentence-ID: 5\n",
      "source-head-word-ID: 0\n",
      "source-entity-type: XOR Gateway\n",
      "source-entity: ['If']\n",
      "target-head-sentence-ID: 6\n",
      "target-head-word-ID: 0\n",
      "target-entity-type: XOR Gateway\n",
      "target-entity: ['If']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contradictory_gateways = [(['if'], ['otherwise']), (['if'], ['else']), (['if'], ['if']), (['in', 'case', 'of'], ['otherwise'])]\n",
    "\n",
    "def _extract_exclusive_flows(doc_activity_tokens, extracted_gateways):\n",
    "    sequence_flows = []\n",
    "    same_gateway_relations = []\n",
    "    \n",
    "    # helper method only for this method\n",
    "    def preprocess_gateways(extracted_gateways):\n",
    "        \"\"\"\n",
    "        flatten gateways but keep sentence index; merge multiple gateway tokens into one gateway\n",
    "        :param extracted_gateways: gateways in PET format\n",
    "        :return: flattened gateway list filled with (sentence_idx, start_token_idx, ['Word', 'List'], ['word', 'list'])\n",
    "        \"\"\"\n",
    "        gateways = []\n",
    "        for sentence_idx, sentence_gateways in enumerate(extracted_gateways):\n",
    "            sentence_gateways_already_included = []\n",
    "            for i, gateway in enumerate(sentence_gateways):\n",
    "                if gateway not in sentence_gateways_already_included:\n",
    "                    gateway_tokens = [gateway[0]]\n",
    "                    start_token_idx = gateway[1]\n",
    "                    # append further tokens of same gateway ('I-' marked)\n",
    "                    I_index = i+1\n",
    "                    while I_index < len(sentence_gateways) and sentence_gateways[I_index][2].startswith('I-'):\n",
    "                        gateway_tokens.append(sentence_gateways[I_index][0])\n",
    "                        sentence_gateways_already_included.append(sentence_gateways[I_index])\n",
    "                        I_index += 1\n",
    "                    gateway_tokens_lower = [t.lower() for t in gateway_tokens]\n",
    "                    gateways.append((sentence_idx, start_token_idx, gateway_tokens, gateway_tokens_lower))    \n",
    "        return gateways\n",
    "    gateways = preprocess_gateways(extracted_gateways)\n",
    "    gateways_involved = []  # list for gateways already involved into sequence flows\n",
    "    \n",
    "    # RULE 1): check for every pair of following gateways if it fits to a gateway constellation with contradictory key words\n",
    "    # gateways must be in range of X (default 1) sentences, otherwise they would be seen as seperate ones\n",
    "    # Signal word of first gateway must be at the beginning of a sentence\n",
    "    for i in range(len(gateways)-1):\n",
    "        g1, g2 = gateways[i], gateways[i+1]\n",
    "        \n",
    "        # if sentence distances is larger than threshold, reject possible pair\n",
    "        if abs(g2[0] - g1[0]) > 1:\n",
    "            continue\n",
    "        for pattern_gateway_1, pattern_gateway_2 in contradictory_gateways:\n",
    "            if g1[3] == pattern_gateway_1 and g2[3] == pattern_gateway_2 and g1[1] == 0:\n",
    "                gateways_involved.append(g1)\n",
    "                gateways_involved.append(g2)\n",
    "\n",
    "                # A) find related activities\n",
    "                pa_g1 = get_previous_activity(g1[0], g1[1], doc_activity_tokens)\n",
    "                fa_g1 = get_following_activity(g1[0], g1[1], doc_activity_tokens)\n",
    "                fa_g2 = get_following_activity(g2[0], g2[1], doc_activity_tokens)\n",
    "                # check if following activities of g1 and g2 are the same -> if yes, the first branch is without activity\n",
    "                if fa_g1 == fa_g2:\n",
    "                    fa_g1 = 'empty branch'\n",
    "                ffa_g2 = get_following_activity(g2[0], g2[1], doc_activity_tokens, skip_first=True)\n",
    "\n",
    "                # B) get dictionary representations\n",
    "                g1_source = get_flow_relation_representation(g1[0], g1[1], XOR_GATEWAY, g1[2], source=True)\n",
    "                g1_target = get_flow_relation_representation(g1[0], g1[1], XOR_GATEWAY, g1[2], source=False)\n",
    "                g2_source = get_flow_relation_representation(g2[0], g2[1], XOR_GATEWAY, g2[2], source=True)\n",
    "                g2_target = get_flow_relation_representation(g2[0], g2[1], XOR_GATEWAY, g2[2], source=False)\n",
    "                if pa_g1: # could be None if at document start\n",
    "                    pa_g1_source = get_flow_relation_representation(pa_g1[0], pa_g1[1], ACTIVITY, pa_g1[2], source=True)\n",
    "                if fa_g1 != 'empty branch' and fa_g1: # could be set in A) manually to None or at document end\n",
    "                    fa_g1_source = get_flow_relation_representation(fa_g1[0], fa_g1[1], ACTIVITY, fa_g1[2], source=True)\n",
    "                    fa_g1_target = get_flow_relation_representation(fa_g1[0], fa_g1[1], ACTIVITY, fa_g1[2], source=False)\n",
    "                if fa_g2: # could be None if at document end\n",
    "                    fa_g2_source = get_flow_relation_representation(fa_g2[0], fa_g2[1], ACTIVITY, fa_g2[2], source=True)\n",
    "                    fa_g2_target = get_flow_relation_representation(fa_g2[0], fa_g2[1], ACTIVITY, fa_g2[2], source=False)\n",
    "                if ffa_g2: # could be None if at document end\n",
    "                    ffa_g2_target = get_flow_relation_representation(ffa_g2[0], ffa_g2[1], ACTIVITY, ffa_g2[2], source=False)\n",
    "\n",
    "\n",
    "                # C.1) connect elements to sequence flows\n",
    "                # a) previous activity to first gateway -> split point (if not None because of document start)\n",
    "                if pa_g1:\n",
    "                    sequence_flows.append(merge_source_target_dicts(pa_g1_source, g1_target))\n",
    "                # b) gateway 1 to following activity and following activity to activity after gateway (second following of g2)\n",
    "                # if None because of empty branch then directly there\n",
    "                if fa_g1: # could be None if at document end\n",
    "                    sequence_flows.append(merge_source_target_dicts(g1_source, fa_g1_target))\n",
    "                    if ffa_g2: # could be None if at document end\n",
    "                        sequence_flows.append(merge_source_target_dicts(fa_g1_source, ffa_g2_target))\n",
    "                elif fa_g1 != 'empty branch' and ffa_g2: # could be None if at document end\n",
    "                    sequence_flows.append(merge_source_target_dicts(g1_source, ffa_g2_target))\n",
    "                # c) gateway 2 to following activity and following activity to activity after gateway (second following of g2)\n",
    "                if fa_g2: # could be None if at document end\n",
    "                    sequence_flows.append(merge_source_target_dicts(g2_source, fa_g2_target))\n",
    "                if ffa_g2: # could be None if at document end\n",
    "                    sequence_flows.append(merge_source_target_dicts(fa_g2_source, ffa_g2_target))\n",
    "\n",
    "                # C.2) same gateway flows\n",
    "                same_gateway_relations.append(merge_source_target_dicts(g1_source, g2_target))\n",
    "    \n",
    "    # RULE 2): exclusive actions of common pattern \"... <activity> ... or ... <activity> ...\"\n",
    "    for g in gateways:\n",
    "        if g not in gateways_involved and g[3] == ['or']:\n",
    "            # A) find related activities\n",
    "            pa = get_previous_activity(g[0], g[1], doc_activity_tokens)\n",
    "            ppa = get_previous_activity(g[0], g[1], doc_activity_tokens, skip_first=True)\n",
    "            fa = get_following_activity(g[0], g[1], doc_activity_tokens)\n",
    "            ffa = get_following_activity(g[0], g[1], doc_activity_tokens, skip_first=True)\n",
    "            \n",
    "            if pa and fa:  # check if existence because of document end/start\n",
    "                if pa[0] == g[0] and fa[0] == g[0]:  # check if in same sentence\n",
    "                    \n",
    "                    # B) get dict representations\n",
    "                    g_source = get_flow_relation_representation(g[0], g[1], XOR_GATEWAY, g[2], source=True)\n",
    "                    g_target = get_flow_relation_representation(g[0], g[1], XOR_GATEWAY, g[2], source=False)\n",
    "                    if pa: # could be None if at document start\n",
    "                        pa_source = get_flow_relation_representation(pa[0], pa[1], ACTIVITY, pa[2], source=True)\n",
    "                        pa_target = get_flow_relation_representation(pa[0], pa[1], ACTIVITY, pa[2], source=False)\n",
    "                    if fa: # could be None if at document end\n",
    "                        fa_source = get_flow_relation_representation(fa[0], fa[1], ACTIVITY, fa[2], source=True)\n",
    "                        fa_target = get_flow_relation_representation(fa[0], fa[1], ACTIVITY, fa[2], source=False)\n",
    "                    if ppa: # could be None if at document start\n",
    "                        ppa_source = get_flow_relation_representation(ppa[0], ppa[1], ACTIVITY, ppa[2], source=True)\n",
    "                    if ffa: # could be None if at document end\n",
    "                        ffa_target = get_flow_relation_representation(ffa[0], ffa[1], ACTIVITY, ffa[2], source=False)\n",
    "                    \n",
    "                    if pa is None or fa is None:\n",
    "                        continue  # if not two surrounding activities are given, do not wire anything (maybe drop gateway again)\n",
    "                        \n",
    "                    # C) connect elements to sequence flows\n",
    "                    # a) second previous activity to gateway -> split point (if not None because of document start)\n",
    "                    if ppa:\n",
    "                        sequence_flows.append(merge_source_target_dicts(ppa_source, g_target))\n",
    "                    # b) gateway to following activity and previous activity -> exclusive branches\n",
    "                    sequence_flows.append(merge_source_target_dicts(g_source, pa_target))\n",
    "                    sequence_flows.append(merge_source_target_dicts(g_source, fa_target))\n",
    "                    # c) exclusive activities to second following activity of gateway -> merge point\n",
    "                    if ffa:  # if not None because of document end\n",
    "                        sequence_flows.append(merge_source_target_dicts(pa_source, ffa_target))\n",
    "                        sequence_flows.append(merge_source_target_dicts(fa_source, ffa_target))\n",
    "                    \n",
    "                    gateways_involved.append(g)\n",
    "                    \n",
    "    # RULE 3): single-branch gateways: the gateway is related to an activity in the same sentence (order is arbitrary)\n",
    "    # Assumptiosn: multi-branch gateways are already recognized by rule 1 before; only one activity for the gateway\n",
    "    for g in gateways:\n",
    "        if g not in gateways_involved and g[3] != ['or']:\n",
    "            # A) find related activities\n",
    "            pa = get_previous_activity(g[0], g[1], doc_activity_tokens)\n",
    "            ppa = get_previous_activity(g[0], g[1], doc_activity_tokens, skip_first=True)\n",
    "            fa = get_following_activity(g[0], g[1], doc_activity_tokens)\n",
    "            ffa = get_following_activity(g[0], g[1], doc_activity_tokens, skip_first=True)\n",
    "            \n",
    "            # B) check if activity is before or after the gateway (assumption: both is not included)\n",
    "            if fa[0] == g[0]:\n",
    "                case = 'activity after gateway'\n",
    "            elif pa[0] == g[0]:\n",
    "                case = 'activity before gateway'\n",
    "            else:\n",
    "                continue  # if no activity in same sentence, do not wire anything (maybe drop gateway again)\n",
    "            gateways_involved.append(g)\n",
    "            \n",
    "            # C) get dict representations\n",
    "            g_source = get_flow_relation_representation(g[0], g[1], XOR_GATEWAY, g[2], source=True)\n",
    "            g_target = get_flow_relation_representation(g[0], g[1], XOR_GATEWAY, g[2], source=False)\n",
    "            if pa: # could be None if at document start\n",
    "                pa_source = get_flow_relation_representation(pa[0], pa[1], ACTIVITY, pa[2], source=True)\n",
    "                pa_target = get_flow_relation_representation(pa[0], pa[1], ACTIVITY, pa[2], source=False)\n",
    "            if fa: # could be None if at document end\n",
    "                fa_source = get_flow_relation_representation(fa[0], fa[1], ACTIVITY, fa[2], source=True)\n",
    "                fa_target = get_flow_relation_representation(fa[0], fa[1], ACTIVITY, fa[2], source=False)\n",
    "            if ppa: # could be None if at document start\n",
    "                ppa_source = get_flow_relation_representation(ppa[0], ppa[1], ACTIVITY, ppa[2], source=True)\n",
    "            if ffa: # could be None if at document end\n",
    "                ffa_target = get_flow_relation_representation(ffa[0], ffa[1], ACTIVITY, ffa[2], source=False)\n",
    "            \n",
    "            # D) connect elements to sequence flows\n",
    "            if case == 'activity after gateway':\n",
    "                # 1) previous activity to gateway -> split point\n",
    "                if pa:  # could be None if at document start\n",
    "                    sequence_flows.append(merge_source_target_dicts(pa_source, g_target))\n",
    "                # 2) gateway to following activity -> exclusive branch\n",
    "                sequence_flows.append(merge_source_target_dicts(g_source, fa_target))\n",
    "                # 3) exclusive activity and gateway to second following activity of gateway -> merge point\n",
    "                if ffa: # could be None if at document end\n",
    "                    sequence_flows.append(merge_source_target_dicts(g_source, ffa_target))\n",
    "                    sequence_flows.append(merge_source_target_dicts(fa_target, ffa_target))\n",
    "                \n",
    "            elif case == 'activity before gateway':\n",
    "                # 1) second previous activity to gateway -> split point\n",
    "                if ppa: # could be None if at document start\n",
    "                    sequence_flows.append(merge_source_target_dicts(ppa_source, g_target))\n",
    "                # 2) gateway to previous activity -> exclusive branch\n",
    "                sequence_flows.append(merge_source_target_dicts(g_source, pa_target))\n",
    "                # 3) exclusive activity and gateway to following activity of gateway -> merge point\n",
    "                if fa: # could be None if at document end\n",
    "                    sequence_flows.append(merge_source_target_dicts(g_source, fa_target))\n",
    "                    sequence_flows.append(merge_source_target_dicts(pa_source, fa_target))\n",
    "                    \n",
    "    sequence_flows.sort(key=lambda flow: flow['source-head-sentence-ID'])\n",
    "    return sequence_flows, same_gateway_relations\n",
    "\n",
    "\n",
    "\n",
    "print(\"own extracted XOR gateways:\", o_xor_gateways)\n",
    "print(\"-\"*30)\n",
    "o_flow_relations_xor, o_same_gateway_xor = _extract_exclusive_flows(doc_activity_tokens, o_xor_gateways)\n",
    "print(\"-\"*30)\n",
    "for i, flow_relation in enumerate(o_flow_relations_xor):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()\n",
    "    \n",
    "print(\"-\"*30)\n",
    "    \n",
    "for i, flow_relation in enumerate(o_same_gateway_xor):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309dabd",
   "metadata": {},
   "source": [
    "## 3.3 Involving Remaining Gold Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5753c608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[['receives']]\n",
      "[['reject'], ['accept']]\n",
      "[]\n",
      "[['informed']]\n",
      "[['processes'], ['checks']]\n",
      "[['reserved']]\n",
      "[['back-ordered']]\n",
      "[]\n",
      "[['prepares']]\n",
      "[['assembles']]\n",
      "[['ships']]\n",
      "[['receives'], ['reject'], ['accept'], ['informed'], ['processes'], ['checks'], ['reserved'], ['back-ordered'], ['prepares'], ['assembles'], ['ships']]\n"
     ]
    }
   ],
   "source": [
    "# doc_activities = token_dataset.GetDocumentActivities(doc_name)\n",
    "for sentence_activities in doc_activities:\n",
    "    print(sentence_activities)\n",
    "\n",
    "# flatten activities to a list of document activities\n",
    "activities_flattened = [activitiy for sentence_activities in doc_activities for activitiy in sentence_activities]\n",
    "print(activities_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9398e435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | source-entity: ['receives']\n",
      "0 | target-entity: ['reject']\n",
      "\n",
      "1 | source-entity: ['reject']\n",
      "1 | target-entity: ['accept']\n",
      "\n",
      "2 | source-entity: ['accept']\n",
      "2 | target-entity: ['informed']\n",
      "\n",
      "3 | source-entity: ['informed']\n",
      "3 | target-entity: ['processes']\n",
      "\n",
      "4 | source-entity: ['processes']\n",
      "4 | target-entity: ['checks']\n",
      "\n",
      "5 | source-entity: ['checks']\n",
      "5 | target-entity: ['reserved']\n",
      "\n",
      "6 | source-entity: ['reserved']\n",
      "6 | target-entity: ['back-ordered']\n",
      "\n",
      "7 | source-entity: ['back-ordered']\n",
      "7 | target-entity: ['prepares']\n",
      "\n",
      "8 | source-entity: ['prepares']\n",
      "8 | target-entity: ['assembles']\n",
      "\n",
      "9 | source-entity: ['assembles']\n",
      "9 | target-entity: ['ships']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_activity_flows(doc_activity_tokens):\n",
    "    activities_flattened = [(i, activitiy) for i, sentence_activities in enumerate(doc_activity_tokens) \n",
    "                            for activitiy in sentence_activities]\n",
    "    flow_relations = []\n",
    "    for i in range(len(activities_flattened) - 1):\n",
    "        s_idx_1, a1 = activities_flattened[i]\n",
    "        s_idx_2, a2 = activities_flattened[i+1]\n",
    "        if True:\n",
    "            flow_relations.append({labels.SOURCE_ENTITY: a1[0], labels.TARGET_ENTITY: a2[0]})\n",
    "        else:\n",
    "            a1 = get_flow_relation_representation(s_idx_1, a1[1], labels.ACTIVITY, a1[0], source=True)\n",
    "            a2 = get_flow_relation_representation(s_idx_2, a2[1], labels.ACTIVITY, a2[0], source=False)\n",
    "            flow_relations.append(merge_source_target_dicts(a1, a2))\n",
    "    return flow_relations\n",
    "\n",
    "\n",
    "gold_activity_flows = create_activity_flows(doc_activity_tokens)\n",
    "for i, flow_relation in enumerate(gold_activity_flows):\n",
    "    for key, value in flow_relation.items():\n",
    "        print(f\"{i} | {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1df07",
   "metadata": {},
   "source": [
    "## 4 Evaluate Extraction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
